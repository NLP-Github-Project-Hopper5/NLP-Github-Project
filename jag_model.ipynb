{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Failed to start the Kernel 'base (Python 3.8.12)'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Kernel has not been started"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import nltk.sentiment\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from time import strftime\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import acquire\n",
    "import acquire_jg\n",
    "import prepare_jag\n",
    "\n",
    "\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire and prep\n",
    "df = pd.read_json('data.json')\n",
    "df = prepare_jag.prep_article_data(df, 'original', extra_words=['&#9;', \"'\", '1', '0', 'use', 'file', 'build', 'test', 'code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make word and bigram lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make word lists by targets from lemmatized words\n",
    "swift_words = ' '.join(df[df.target == 'swift'].lemmatized).split()\n",
    "python_words = ' '.join(df[df.target == 'python'].lemmatized).split()\n",
    "c_words = ' '.join(df[df.target == 'c'].lemmatized).split()\n",
    "other_words = ' '.join(df[df.target == 'other'].lemmatized).split()\n",
    "all_words = ' '.join(df.lemmatized).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Swift words: '+str(len(swift_words)))\n",
    "print('Python words: '+str(len(python_words)))\n",
    "print('C words: '+str(len(c_words)))\n",
    "print('Other words: '+str(len(other_words)))\n",
    "print('All words: '+str(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at word frequency per languge\n",
    "swift_freq = pd.Series(swift_words).value_counts()\n",
    "python_freq = pd.Series(python_words).value_counts()\n",
    "c_freq = pd.Series(c_words).value_counts()\n",
    "other_freq = pd.Series(other_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 20 most common words\n",
    "word_counts = (pd.concat([all_freq, swift_freq, python_freq, c_freq, other_freq], axis = 1, sort = True)\n",
    "                .set_axis(['all', 'swift', 'python', 'c', 'other'], axis = 1, inplace = False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))\n",
    "word_counts.sort_values(by = 'all', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bigrams \n",
    "swift_bigrams = (pd.Series(nltk.ngrams(swift_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "python_bigrams = (pd.Series(nltk.ngrams(python_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "c_bigrams = (pd.Series(nltk.ngrams(c_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "other_bigrams = (pd.Series(nltk.ngrams(other_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common language is swift, so our baseline model would predict that all the repositories are coded in swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish baseline \n",
    "# baseline\n",
    "df[df['language'] == 'Swift'].language.value_counts()/sum(df.language.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to split words in to train / test / split\n",
    "- For x - start with vectorzed,  lematized words \n",
    "- y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X Y\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df.lemmatized)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create evaluation dataframe\n",
    "train_eval = pd.DataFrame(dict(actual=y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to use lazy classifier, but getting install warnings from XG boost. Had problems installing it before with the M1 chip. I am going to create a new environment and see if I cant get it to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
