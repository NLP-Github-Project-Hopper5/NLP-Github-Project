[
 {
  "repo": "apple/llvm-project",
  "language": null,
  "readme_contents": "# Apple's fork of llvm-project\n\nThis is Apple's fork of llvm-project.  For more information on Apple's\nbranching scheme, please see\n[apple-docs/AppleBranchingScheme.md](https://github.com/apple/llvm-project/tree/apple/main/apple-docs/AppleBranchingScheme.md).\n\nThe LLVM project's main README follows.\n\n# The LLVM Compiler Infrastructure\n\nThis directory and its sub-directories contain source code for LLVM,\na toolkit for the construction of highly optimized compilers,\noptimizers, and run-time environments.\n\nThe README briefly describes how to get started with building LLVM.\nFor more information on how to contribute to the LLVM project, please\ntake a look at the\n[Contributing to LLVM](https://llvm.org/docs/Contributing.html) guide.\n\n## Getting Started with the LLVM System\n\nTaken from https://llvm.org/docs/GettingStarted.html.\n\n### Overview\n\nWelcome to the LLVM project!\n\nThe LLVM project has multiple components. The core of the project is\nitself called \"LLVM\". This contains all of the tools, libraries, and header\nfiles needed to process intermediate representations and convert them into\nobject files.  Tools include an assembler, disassembler, bitcode analyzer, and\nbitcode optimizer.  It also contains basic regression tests.\n\nC-like languages use the [Clang](http://clang.llvm.org/) front end.  This\ncomponent compiles C, C++, Objective-C, and Objective-C++ code into LLVM bitcode\n-- and from there into object files, using LLVM.\n\nOther components include:\nthe [libc++ C++ standard library](https://libcxx.llvm.org),\nthe [LLD linker](https://lld.llvm.org), and more.\n\n### Getting the Source Code and Building LLVM\n\nThe LLVM Getting Started documentation may be out of date.  The [Clang\nGetting Started](http://clang.llvm.org/get_started.html) page might have more\naccurate information.\n\nThis is an example work-flow and configuration to get and build the LLVM source:\n\n1. Checkout LLVM (including related sub-projects like Clang):\n\n     * ``git clone https://github.com/llvm/llvm-project.git``\n\n     * Or, on windows, ``git clone --config core.autocrlf=false\n    https://github.com/llvm/llvm-project.git``\n\n2. Configure and build LLVM and Clang:\n\n     * ``cd llvm-project``\n\n     * ``cmake -S llvm -B build -G <generator> [options]``\n\n        Some common build system generators are:\n\n        * ``Ninja`` --- for generating [Ninja](https://ninja-build.org)\n          build files. Most llvm developers use Ninja.\n        * ``Unix Makefiles`` --- for generating make-compatible parallel makefiles.\n        * ``Visual Studio`` --- for generating Visual Studio projects and\n          solutions.\n        * ``Xcode`` --- for generating Xcode projects.\n\n        Some common options:\n\n        * ``-DLLVM_ENABLE_PROJECTS='...'`` --- semicolon-separated list of the LLVM\n          sub-projects you'd like to additionally build. Can include any of: clang,\n          clang-tools-extra, compiler-rt,cross-project-tests, flang, libc, libclc,\n          libcxx, libcxxabi, libunwind, lld, lldb, mlir, openmp, polly, or pstl.\n\n          For example, to build LLVM, Clang, libcxx, and libcxxabi, use\n          ``-DLLVM_ENABLE_PROJECTS=\"clang;libcxx;libcxxabi\"``.\n\n        * ``-DCMAKE_INSTALL_PREFIX=directory`` --- Specify for *directory* the full\n          path name of where you want the LLVM tools and libraries to be installed\n          (default ``/usr/local``).\n\n        * ``-DCMAKE_BUILD_TYPE=type`` --- Valid options for *type* are Debug,\n          Release, RelWithDebInfo, and MinSizeRel. Default is Debug.\n\n        * ``-DLLVM_ENABLE_ASSERTIONS=On`` --- Compile with assertion checks enabled\n          (default is Yes for Debug builds, No for all other build types).\n\n      * ``cmake --build build [-- [options] <target>]`` or your build system specified above\n        directly.\n\n        * The default target (i.e. ``ninja`` or ``make``) will build all of LLVM.\n\n        * The ``check-all`` target (i.e. ``ninja check-all``) will run the\n          regression tests to ensure everything is in working order.\n\n        * CMake will generate targets for each tool and library, and most\n          LLVM sub-projects generate their own ``check-<project>`` target.\n\n        * Running a serial build will be **slow**.  To improve speed, try running a\n          parallel build.  That's done by default in Ninja; for ``make``, use the option\n          ``-j NNN``, where ``NNN`` is the number of parallel jobs, e.g. the number of\n          CPUs you have.\n\n      * For more information see [CMake](https://llvm.org/docs/CMake.html)\n\nConsult the\n[Getting Started with LLVM](https://llvm.org/docs/GettingStarted.html#getting-started-with-llvm)\npage for detailed information on configuring and compiling LLVM. You can visit\n[Directory Layout](https://llvm.org/docs/GettingStarted.html#directory-layout)\nto learn about the layout of the source code tree.\n"
 },
 {
  "repo": "apple/swift-argument-parser",
  "language": "Swift",
  "readme_contents": "# Swift Argument Parser\n\n## Usage\n\nBegin by declaring a type that defines the information\nthat you need to collect from the command line.\nDecorate each stored property with one of `ArgumentParser`'s property wrappers,\nand then declare conformance to `ParsableCommand` and add the `@main` attribute.\nFinally, implement your command's logic in the `run()` method.\n\n```swift\nimport ArgumentParser\n\n@main\nstruct Repeat: ParsableCommand {\n    @Flag(help: \"Include a counter with each repetition.\")\n    var includeCounter = false\n\n    @Option(name: .shortAndLong, help: \"The number of times to repeat 'phrase'.\")\n    var count: Int?\n\n    @Argument(help: \"The phrase to repeat.\")\n    var phrase: String\n\n    mutating func run() throws {\n        let repeatCount = count ?? .max\n\n        for i in 1...repeatCount {\n            if includeCounter {\n                print(\"\\(i): \\(phrase)\")\n            } else {\n                print(phrase)\n            }\n        }\n    }\n}\n```\n\nThe `ArgumentParser` library parses the command-line arguments,\ninstantiates your command type, and then either executes your `run()` method\nor exits with a useful message.\n\n`ArgumentParser` uses your properties' names and type information,\nalong with the details you provide using property wrappers,\nto supply useful error messages and detailed help:\n\n```\n$ repeat hello --count 3\nhello\nhello\nhello\n$ repeat --count 3\nError: Missing expected argument 'phrase'.\nHelp:  <phrase>  The phrase to repeat.\nUsage: repeat [--count <count>] [--include-counter] <phrase>\n  See 'repeat --help' for more information.\n$ repeat --help\nUSAGE: repeat [--count <count>] [--include-counter] <phrase>\n\nARGUMENTS:\n  <phrase>                The phrase to repeat.\n\nOPTIONS:\n  --include-counter       Include a counter with each repetition.\n  -c, --count <count>     The number of times to repeat 'phrase'.\n  -h, --help              Show help for this command.\n```\n\n## Documentation\n\nFor guides, articles, and API documentation see the \n[library's documentation on the Web][docs] or in Xcode.\n\n- [ArgumentParser documentation][docs]\n- [Getting Started with ArgumentParser](https://apple.github.io/swift-argument-parser/documentation/argumentparser/gettingstarted)\n- [`ParsableCommand` documentation](https://apple.github.io/swift-argument-parser/documentation/argumentparser/parsablecommand)\n\n[docs]: https://apple.github.io/swift-argument-parser/documentation/argumentparser/\n\n#### Examples\n\nThis repository includes a few examples of using the library:\n\n- [`repeat`](Examples/repeat/main.swift) is the example shown above.\n- [`roll`](Examples/roll/main.swift) is a simple utility implemented as a straight-line script.\n- [`math`](Examples/math/main.swift) is an annotated example of using nested commands and subcommands.\n\nYou can also see examples of `ArgumentParser` adoption among Swift project tools:\n\n- [`swift-format`](https://github.com/apple/swift-format/) uses some advanced features, like custom option values and hidden flags.\n- [`swift-package-manager`](https://github.com/apple/swift-package-manager/) includes a deep command hierarchy and extensive use of option groups.\n\n## Project Status\n\nThe Swift Argument Parser package is source stable;\nversion numbers follow semantic versioning.\nSource breaking changes to public API can only land in a new major version.\n\nThe public API of version 1.0 of the `swift-argument-parser` package\nconsists of non-underscored declarations that are marked public in the `ArgumentParser` module.\nInterfaces that aren't part of the public API may continue to change in any release,\nincluding the exact wording and formatting of the autogenerated help and error messages,\nas well as the package\u2019s examples, tests, utilities, and documentation. \n\nFuture minor versions of the package may introduce changes to these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate.\nAccordingly, from time to time,\nwe expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release.\nRequiring a new Swift release will only require a minor version bump.\n\n## Adding `ArgumentParser` as a Dependency\n\nTo use the `ArgumentParser` library in a SwiftPM project, \nadd it to the dependencies for your package and your command-line executable target:\n\n```swift\nlet package = Package(\n    // name, platforms, products, etc.\n    dependencies: [\n        // other dependencies\n        .package(url: \"https://github.com/apple/swift-argument-parser\", from: \"1.0.0\"),\n    ],\n    targets: [\n        .executableTarget(name: \"<command-line-tool>\", dependencies: [\n            // other dependencies\n            .product(name: \"ArgumentParser\", package: \"swift-argument-parser\"),\n        ]),\n        // other targets\n    ]\n)\n```\n"
 },
 {
  "repo": "apple/swift-docc",
  "language": "Swift",
  "readme_contents": "# Swift-DocC\n\nSwift-DocC is a documentation compiler for Swift frameworks and packages aimed \nat making it easy to write and publish great developer documentation.\n\nFor an example of Swift-DocC in action, check out \n[developer.apple.com](https://developer.apple.com/documentation).\nMuch of Apple's developer documentation,\nfrom [Reference documentation](https://developer.apple.com/documentation/GroupActivities)\nto [Tutorials](https://developer.apple.com/tutorials/swiftui),\nis built using Swift-DocC.\n\nSwift-DocC is being actively developed. For more information about the\nSwift-DocC project, see the introductory blog post\n[here](https://swift.org/blog/swift-docc/).\n\nThe latest documentation for the Swift-DocC project is available\non [Swift.org](https://swift.org/documentation/docc).\n\nThe [Swift Forums](https://forums.swift.org/c/development/swift-docc) are\nthe best place to get help with Swift-DocC and discuss future plans.\n\n## Writing and Publishing Documentation with Swift-DocC\n\nIf you're looking to write and publish documentation with Swift-DocC, \nthe best way to get started is with Swift-DocC's\n[user documentation](https://www.swift.org/documentation/docc).\n\n## Technical Overview and Related Projects\n\nSwift-DocC builds documentation by combining _Symbol Graph_ files containing API information \nwith a `.docc` Documentation Catalog containing articles and tutorials\nto create a final archive containing the compiled documentation.\n\nMore concretely, Swift-DocC understands the following kinds of inputs:\n\n  1. _Symbol Graph_ files with the `.symbols.json` extension.\n     _Symbol Graph_ files are a machine-readable representation of a module's APIs, \n     including their documentation comments and relationship with one another.\n\n  2. A Documentation Catalog with the `.docc` extension. \n     Documentation Catalogs can include additional documentation content like the following:\n   \n     - Documentation markup files with the `.md` extension. Documentation markup files can\n       be used to extend documentation for symbols and to write free-form articles.\n \n     - Tutorial files with the `.tutorial` extension. Tutorial files are used to author\n       step-by-step instructions on how to use a framework.\n \n     - Additional documentation assets with known extensions like `.png`, `.jpg`, `.mov`,\n       and `.zip`.\n \n     - An `Info.plist` file containing metadata such as the name of the documented module. \n       This file is optional and the information it contains can be passed via the command line.\n\nSwift-DocC outputs a machine-readable archive of the compiled documentation.\nThis archive contains _render JSON_ files, which fully describe the contents\nof a documentation page and can be processed by a renderer such as\n[Swift-DocC-Render](https://github.com/apple/swift-docc-render).\n\nFor more in-depth technical information about Swift-DocC, you\ncan build and preview the project's technical documentation \nfrom the command line with\n\n```sh\nbin/preview-docs\n```\n\n> **Note:** The `preview-docs` script expects the `DOCC_HTML_DIR` environment\n> variable to be set with the path to a Swift-DocC renderer. See the\n> [Using `docc` to build and preview documentation](#using-docc-to-build-and-preview-documentation)\n> section below for details.\n\nAlternatively, you can open the package in Xcode 13 and select the \"Build Documentation\"\nbutton in the Product menu to view the documentation in Xcode's documentation window.\n\n### Related Projects\n\n  - As of Swift 5.5, the [Swift Compiler](https://github.com/apple/swift) is able to \n    emit _Symbol Graph_ files as part of the compilation process.\n    \n  - [SymbolKit](https://github.com/apple/swift-docc-symbolkit) is a Swift package containing\n    the specification and reference model for the _Symbol Graph_ File Format.\n  \n  - [Swift Markdown](https://github.com/apple/swift-markdown) is a \n    Swift package for parsing, building, editing, and analyzing \n    Markdown documents. It includes support for the Block Directive elements\n    that Swift-DocC's tutorial files rely on.\n    \n  - [Swift-DocC-Render](https://github.com/apple/swift-docc-render) \n    is a web application that understands and renders\n    Swift-DocC's _render JSON_ format.\n    \n  - [Xcode](https://developer.apple.com/xcode/) consists of a suite of\n    tools that developers use to build apps for Apple platforms.\n    Beginning with Xcode 13, Swift-DocC is integrated into Xcode\n    with support for building and viewing documentation for your framework and\n    its dependencies.\n  \n## Getting Started with `docc`\n\n`docc` is the command line interface (CLI) for Swift-DocC and provides\nsupport for converting and previewing DocC documentation.\n\n### Prerequisites\n\nDocC is a Swift package. If you're new to Swift package manager,\nthe [documentation here](https://swift.org/getting-started/#using-the-package-manager)\nprovides an explanation of how to get started and the software you'll need\ninstalled.\n\nDocC requires Swift 5.5 which is included in Xcode 13.\n\n### Build\n\n1. Checkout this repository using:\n\n    ```bash\n    git clone https://github.com/apple/swift-docc.git\n    ```\n\n2. Navigate to the root of the repository with:\n\n    ```bash\n    cd swift-docc\n    ```\n\n3. Finally, build DocC by running:\n\n    ```bash\n    swift build\n    ```\n\n### Run\n\nTo run `docc`, run the following command:\n\n  ```bash\n  swift run docc\n  ```\n  \n### Installing into Xcode\n\nYou can test a locally built version of Swift-DocC in Xcode 13 or later by setting\nthe `DOCC_EXEC` build setting to the path of your local `docc`:\n\n  1. Select the project in the Project Navigator.\n  \n  2. In the Build Settings tab, click '+' and then 'Add User-Defined Setting'. \n  \n  3. Create a build setting `DOCC_EXEC` with the value set to `/path/to/docc`. \n\nThe next time you invoke a documentation build with the \"Build Documentation\"\nbutton in Xcode's Product menu, your custom `docc` will be used for the build.\nYou can confirm that your custom `docc` is being used by opening the latest build\nlog in Xcode's report navigator and expanding the \"Compile documentation\" step.\n  \n## Using `docc` to build and preview documentation\n\nYou can use `docc` directly to build documentation for your Swift framework\nor package. The below instructions use this repository as an example but\napply to any Swift package. Just replace any reference to `SwiftDocC` below\nwith the name of your package.\n\n### 1. Generate a symbol graph file\n\nBegin by navigating to the root of your Swift package.\n\n```sh\ncd ~/Developer/swift-docc\n```\n\nThen run the following to generate _Symbol Graph_ files for your target:\n\n```sh\nmkdir -p .build/symbol-graphs && \\\n  swift build --target SwiftDocC \\\n    -Xswiftc -emit-symbol-graph \\\n    -Xswiftc -emit-symbol-graph-dir -Xswiftc .build/symbol-graphs\n```\n\nYou should now have a number of `.symbols.json` files in `.build/symbol-graphs`\nrepresenting the provided target and its dependencies. You can copy out the files representing\njust the target itself with:\n\n```sh\nmkdir .build/swift-docc-symbol-graphs \\\n  && mv .build/symbol-graphs/SwiftDocC* .build/swift-docc-symbol-graphs\n```\n\n### 2. Set the path to your renderer\n\nThe best place to get started with Swift-DocC-Render is with the\ninstructions in the [project's README](https://github.com/apple/swift-docc-render).\n\nIf you have Xcode 13 installed, you can use the version of Swift-DocC-Render\nthat comes included in Xcode (instead of building a copy locally) with:\n\n```sh\nexport DOCC_HTML_DIR=\"$(dirname $(xcrun --find docc))/../share/docc/render\"\n```\n\nAlternatively, you can clone Swift-DocC-Render and build a local\ncopy of the renderer with these instructions:\n\n> **Note:** Swift-DocC-Render \n> requires [Node.js](https://nodejs.org/en/download/) v14.\n\n```sh\ngit clone https://github.com/apple/swift-docc-render.git\ncd swift-docc-render\nnpm install\nnpm run build\n```\n\nThen point the `DOCC_HTML_DIR` environment variable\nto the generated `/dist` folder.\n\n```sh\nexport DOCC_HTML_DIR=\"/path/to/swift-docc-render/dist\"\n```\n\n### 3. Preview your documentation\n\nThe `docc preview` command performs a conversion of your documentation and\nstarts a local web server to allow for easy previewing of the built documentation.\nIt monitors the provided Documentation Catalog for changes and updates the preview\nas you're working.\n\n```sh\ndocc preview Sources/SwiftDocC/SwiftDocC.docc \\\n  --fallback-display-name SwiftDocC \\\n  --fallback-bundle-identifier org.swift.SwiftDocC \\\n  --fallback-bundle-version 1.0.0 \\\n  --additional-symbol-graph-dir .build/swift-docc-symbol-graphs\n```\n\nYou should now see the following in your terminal:\n\n```\nInput: ~/Developer/swift-docc/Sources/SwiftDocC/SwiftDocC.docc\nTemplate: ~/Developer/swift-docc-render/dist\n========================================\nStarting Local Preview Server\n   Address: http://localhost:8000/documentation/swiftdocc\n========================================\nMonitoring ~/Developer/swift-docc/Sources/SwiftDocC/SwiftDocC.docc for changes...\n```\n\nAnd if you navigate to <http://localhost:8000/documentation/swiftdocc> you'll see\nthe rendered documentation for `SwiftDocC`.\n  \n## Versioning\n\nSwift-DocC's CLI tool (`docc`) will be integrated into the Swift toolchain \nand follows the Swift compiler's versioning scheme.\n\nThe `SwiftDocC` library is versioned separately from `docc`. `SwiftDocC` is under\nactive development and source stability is not guaranteed.\n\n## Bug Reports and Feature Requests\n\n### Submitting a Bug Report\n\nSwift-DocC tracks all bug reports with [Swift JIRA](https://bugs.swift.org/).\nWhen you submit a bug report we ask that you follow the\nSwift [Bug Reporting](https://swift.org/contributing/#reporting-bugs) guidelines\nand provide as many details as possible.\n\n> **Note:** You can use the [`environment`](bin/environment) script\n> in this repository to gather helpful environment information to paste\n> into your bug report by running the following:\n> \n> ```sh\n> bin/environment\n> ```\n\nIf you can confirm that the bug occurs when using the latest commit of Swift-DocC\nfrom the `main` branch (see [Building Swift-DocC](/CONTRIBUTING.md#building-swift-docc)),\nthat will help us track down the bug faster.\n\n### Submitting a Feature Request\n\nFor feature requests, please feel free to create an issue\non [Swift JIRA](https://bugs.swift.org/) with the `New Feature` type\nor start a discussion on the [Swift Forums](https://forums.swift.org/c/development/swift-docc).\n\nDon't hesitate to submit a feature request if you see a way\nSwift-DocC can be improved to better meet your needs.\n\nAll user-facing features must be discussed\nin the [Swift Forums](https://forums.swift.org/c/development/swift-docc)\nbefore being enabled by default.\n\n## Contributing to Swift-DocC\n\nPlease see the [contributing guide](/CONTRIBUTING.md) for more information.\n\n<!-- Copyright (c) 2021 Apple Inc and the Swift Project authors. All Rights Reserved. -->\n"
 },
 {
  "repo": "apple/swift",
  "language": "C++",
  "readme_contents": "<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n\n# Swift Programming Language\n\n\n| | **Architecture** | **main** | **Package** |\n|---|:---:|:---:|:---:|\n| **macOS**        | x86_64 |[![Build Status](https://ci.swift.org/job/oss-swift-incremental-RA-macos/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-incremental-RA-macos)|[![Build Status](https://ci.swift.org/job/oss-swift-package-macos/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-macos)|\n| **Ubuntu 16.04** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-incremental-RA-linux-ubuntu-16_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-incremental-RA-linux-ubuntu-16_04)|[![Build Status](https://ci.swift.org/job/oss-swift-package-linux-ubuntu-16_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-linux-ubuntu-16_04)|\n| **Ubuntu 18.04** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-incremental-RA-linux-ubuntu-18_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-incremental-RA-linux-ubuntu-18_04)|[![Build Status](https://ci.swift.org/job/oss-swift-package-linux-ubuntu-18_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-linux-ubuntu-18_04)|\n| **Ubuntu 20.04** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-package-ubuntu-20_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-ubuntu-20_04)|[![Build Status](https://ci.swift.org/job/oss-swift-package-ubuntu-20_04/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-ubuntu-20_04)|\n| **CentOS 8** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-package-centos-8/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-centos-8)|[![Build Status](https://ci.swift.org/job/oss-swift-package-centos-8/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-centos-8)|\n| **CentOS 7** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-package-centos-7/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-centos-7)|[![Build Status](https://ci.swift.org/job/oss-swift-package-centos-7/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-centos-7)|\n| **Amazon Linux 2** | x86_64 | [![Build Status](https://ci.swift.org/job/oss-swift-package-amazon-linux-2/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-amazon-linux-2)|[![Build Status](https://ci.swift.org/job/oss-swift-package-amazon-linux-2/lastCompletedBuild/badge/icon)](https://ci.swift.org/job/oss-swift-package-amazon-linux-2)|\n\n**Swift Community-Hosted CI Platforms**\n\n| **OS** | **Architecture** | **Build** |\n|---|:---:|:---:|\n|**[Ubuntu 16.04 ](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/ppc64le_ubuntu_16_04.json)** | PPC64LE |[![Build Status](https://ci-external.swift.org/job/oss-swift-5.1-RA-linux-ubuntu-16.04-ppc64le/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-5.1-RA-linux-ubuntu-16.04-ppc64le)|\n|**[Ubuntu 18.04](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/aarch64_ubuntu_18.04_docker.json)** | AArch64 |[![Build Status](https://ci-external.swift.org/job/oss-swift-RA-ubuntu-18.04-aarch64/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-RA-ubuntu-18.04-aarch64)|\n|**[Ubuntu 20.04](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/aarch64_ubuntu_20.04_docker.json)** | AArch64 |[![Build Status](https://ci-external.swift.org/job/oss-swift-RA-ubuntu-20.04-aarch64/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-RA-ubuntu-20.04-aarch64)|\n|**[Ubuntu 20.04](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/wasm32_ubuntu_20.04.json)** | wasm32 |[![Build Status](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-20.04-webassembly/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-20.04-webassembly)|\n|**[Android](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/x86_64_ubuntu_16_04_LTS_android.json)** | ARMv7 |[![Build Status](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-16.04-android/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-16.04-android)|\n|**[Android](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/x86_64_ubuntu_16_04_LTS_android.json)** | AArch64 |[![Build Status](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-16.04-android-arm64/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-RA-linux-ubuntu-16.04-android-arm64)|\n|**[Windows 2019 (VS 2017)](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/x86_64_windows_2019.json)** | x86_64 | [![Build Status](https://ci-external.swift.org/job/oss-swift-windows-x86_64/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-windows-x86_64)|\n|**[Windows 2019 (VS 2019)](https://github.com/apple/swift-community-hosted-continuous-integration/blob/main/nodes/x86_64_windows_2019_VS2019.json)** | x86_64 | [![Build Status](https://ci-external.swift.org/job/oss-swift-windows-x86_64-vs2019/lastCompletedBuild/badge/icon)](https://ci-external.swift.org/job/oss-swift-windows-x86_64-vs2019)|\n\n## Welcome to Swift\n\nSwift is a high-performance system programming language.  It has a clean\nand modern syntax, offers seamless access to existing C and Objective-C code\nand frameworks, and is memory safe by default.\n\nAlthough inspired by Objective-C and many other languages, Swift is not itself a\nC-derived language. As a complete and independent language, Swift packages core\nfeatures like flow control, data structures, and functions, with high-level\nconstructs like objects, protocols, closures, and generics. Swift embraces\nmodules, eliminating the need for headers and the code duplication they entail.\n\nTo learn more about the programming language, visit [swift.org](https://swift.org/documentation/).\n\n- [Contributing to Swift](#contributing-to-swift)\n- [Getting Started](#getting-started)\n  - [Swift Toolchains](#swift-toolchains)\n  - [Build Failures](#build-failures)\n- [Learning More](#learning-more)\n\n## Contributing to Swift\n\nContributions to Swift are welcomed and encouraged! Please see the\n[Contributing to Swift guide](https://swift.org/contributing/).\n\nTo be a truly great community, [Swift.org](https://swift.org/) needs to welcome\ndevelopers from all walks of life, with different backgrounds, and with a wide\nrange of experience. A diverse and friendly community will have more great\nideas, more unique perspectives, and produce more great code. We will work\ndiligently to make the Swift community welcoming to everyone.\n\nTo give clarity of what is expected of our members, Swift has adopted the\ncode of conduct defined by the Contributor Covenant. This document is used\nacross many open source communities, and we think it articulates our values\nwell. For more, see the [Code of Conduct](https://swift.org/code-of-conduct/).\n\n## Getting Started\n\nIf you are interested in:\n- Contributing fixes and features to the compiler: See our\n  [How to Submit Your First Pull Request guide](/docs/HowToGuides/FirstPullRequest.md).\n- Building the compiler as a one-off: See our [Getting Started guide][].\n- Building a toolchain as a one-off: Follow the [Getting Started guide][]\n  up until the \"Building the project\" section. After that, follow the\n  instructions in the [Swift Toolchains](#swift-toolchains) section below.\n\nWe also have an [FAQ](/docs/HowToGuides/FAQ.md) that answers common questions.\n\n[Getting Started guide]: /docs/HowToGuides/GettingStarted.md\n\n### Swift Toolchains\n\n#### Building\n\nSwift toolchains are created using the script\n[build-toolchain](https://github.com/apple/swift/blob/main/utils/build-toolchain). This\nscript is used by swift.org's CI to produce snapshots and can allow for one to\nlocally reproduce such builds for development or distribution purposes. A typical \ninvocation looks like the following:\n\n```\n  $ ./swift/utils/build-toolchain $BUNDLE_PREFIX\n```\n\nwhere ``$BUNDLE_PREFIX`` is a string that will be prepended to the build \ndate to give the bundle identifier of the toolchain's ``Info.plist``. For \ninstance, if ``$BUNDLE_PREFIX`` was ``com.example``, the toolchain \nproduced will have the bundle identifier ``com.example.YYYYMMDD``. It \nwill be created in the directory you run the script with a filename \nof the form: ``swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz``.\n\nBeyond building the toolchain, ``build-toolchain`` also supports the \nfollowing (non-exhaustive) set of useful options::\n\n- ``--dry-run``: Perform a dry run build. This is off by default.\n- ``--test``: Test the toolchain after it has been compiled. This is off by default.\n- ``--distcc``: Use distcc to speed up the build by distributing the c++ part of\n  the swift build. This is off by default.\n- ``--sccache``: Use sccache to speed up subsequent builds of the compiler by\n  caching more c++ build artifacts. This is off by default.\n\nMore options may be added over time. Please pass ``--help`` to\n``build-toolchain`` to see the full set of options.\n\n#### Installing into Xcode\n\nOn macOS if one wants to install such a toolchain into Xcode:\n\n1. Untar and copy the toolchain to one of `/Library/Developer/Toolchains/` or\n   `~/Library/Developer/Toolchains/`. E.x.:\n\n```\n  $ sudo tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz -C /\n  $ tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx.tar.gz -C ~/\n```\n\nThe script also generates an archive containing debug symbols which\ncan be installed over the main archive allowing symbolication of any\ncompiler crashes.\n\n```\n  $ sudo tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx-symbols.tar.gz -C /\n  $ tar -xzf swift-LOCAL-YYYY-MM-DD-a-osx-symbols.tar.gz -C ~/\n```\n\n2. Specify the local toolchain for Xcode's use via `Xcode->Toolchains`.\n\n### Build Failures\n\nTry the suggestions in\n[Troubleshooting build issues](/docs/HowToGuides/GettingStarted.md#troubleshooting-build-issues).\n\nMake sure you are using the\n[correct release](/docs/HowToGuides/GettingStarted.md#installing-dependencies)\nof Xcode.\n\nIf you have changed Xcode versions but still encounter errors that appear to\nbe related to the Xcode version, try passing `--clean` to `build-script`.\n\nWhen a new version of Xcode is released, you can update your build without\nrecompiling the entire project by passing `--reconfigure` to `build-script`.\n\n## Learning More\n\nBe sure to look at the [documentation index](/docs/README.md) for a bird's eye\nview of the available documentation. In particular, the documents titled\n[Debugging the Swift Compiler](docs/DebuggingTheCompiler.md) and\n[Continuous Integration for Swift](docs/ContinuousIntegration.md) are very\nhelpful to understand before submitting your first PR.\n"
 },
 {
  "repo": "apple/sourcekit-lsp",
  "language": "Swift",
  "readme_contents": "# SourceKit-LSP\n\nSourceKit-LSP is an implementation of the [Language Server Protocol](https://microsoft.github.io/language-server-protocol/) (LSP) for Swift and C-based languages. It provides features like code-completion and jump-to-definition to editors that support LSP. SourceKit-LSP is built on top of [sourcekitd](https://github.com/apple/swift/tree/main/tools/SourceKit) and [clangd](https://clang.llvm.org/extra/clangd.html) for high-fidelity language support, and provides a powerful source code index as well as cross-language support. SourceKit-LSP supports projects that use the Swift Package Manager.\n\n## Getting Started\n\nThe SourceKit-LSP server is included with the Swift toolchain. Depending on how you installed Swift, you may already have SourceKit-LSP. Make sure you build your package with the same toolchain as you use sourcekit-lsp from to ensure compatibility.\n\n1. Get SourceKit-LSP with a Swift toolchain\n\n    1. If you have installed Xcode 11.4+ or the corresponding Command Line Tools package, the SourceKit-LSP server is included and can be run with `xcrun sourcekit-lsp`.\n\n    2. If you are using a [toolchain from Swift.org](https://swift.org/download/), the SourceKit-LSP server is included and can be run with `xcrun --toolchain swift sourcekit-lsp` on macOS, or using the full path to the `sourcekit-lsp` executable on Linux.\n\n    3. If your toolchain did not come with SourceKit-LSP, see [Development](Documentation/Development.md) for how to build it from source.\n\n2. Configure your editor to use SourceKit-LSP. See [Editors](Editors) for more information about editor integration.\n\n3. Build the project you are working on with `swift build` using the same toolchain as the SourceKit-LSP server. The language server depends on the build to provide module dependencies and to update the global index.\n\n## Development\n\nFor more information about developing SourceKit-LSP itself, see [Development](Documentation/Development.md).\n\n## Indexing While Building\n\nSourceKit-LSP uses a global index called [IndexStoreDB](https://github.com/apple/indexstore-db) to provide features that cross file or module boundaries, such as jump-to-definition or find-references. To efficiently create an index of your source code we use a technique called \"indexing while building\". When the project is compiled for debugging using `swift build`, the compiler (swiftc or clang) automatically produces additional raw index data that is read by our indexer. Producing this information during compilation saves work and ensures that any time the project is built the index is updated and fully accurate.\n\nIn the future we intend to also provide automatic background indexing so that we can update the index in between builds or to include code that's not always built like unit tests. In the meantime, building your project should bring our index up to date.\n\n## Status\n\nSourceKit-LSP is still in early development, so you may run into rough edges with any of the features. The following table shows the status of various features when using the latest development toolchain snapshot. See [Caveats](#caveats) for important known issues you may run into.\n\n| Feature | Status | Notes |\n|---------|:------:|-------|\n| Swift | \u2705 | |\n| C/C++/ObjC | \u2705 | Uses [clangd](https://clangd.llvm.org/) |\n| Code completion | \u2705 | |\n| Quick Help (Hover) | \u2705 | |\n| Diagnostics | \u2705 | |\n| Fix-its | \u2705 | |\n| Jump to Definition | \u2705 | |\n| Find References | \u2705 | |\n| Background Indexing | \u274c | Build project to update the index using [Indexing While Building](#indexing-while-building) |\n| Workspace Symbols | \u2705 | |\n| Global Rename | \u274c | |\n| Local Refactoring | \u2705 | |\n| Formatting | \u274c | |\n| Folding | \u2705 | |\n| Syntax Highlighting | \u2705 | Both syntactic and semantic tokens |\n| Document Symbols | \u2705 |  |\n\n\n### Caveats\n\n* SourceKit-LSP does not update its global index in the background, but instead relies on indexing-while-building to provide data. This only affects global queries like find-references and jump-to-definition.\n\t* **Workaround**: build the project to update the index\n"
 },
 {
  "repo": "apple/foundationdb",
  "language": "C++",
  "readme_contents": "<img alt=\"FoundationDB logo\" src=\"documentation/FDB_logo.png?raw=true\" width=\"400\">\n\n![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiZ1FhRlNwU0JXeHVpZkt0a0k0QlNJK3BEUkplTGVRYnk3azBoT1FOazBQbGlIeDgrYmRJZVhuSUI4RTd3RWJWcjVMT3ZPTzV0NXlCTWpPTGlPVlMzckJJPSIsIml2UGFyYW1ldGVyU3BlYyI6IlB0TWVCM0VYdU5PQWtMUFYiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master)\n\nFoundationDB is a distributed database designed to handle large volumes of structured data across clusters of commodity servers. It organizes data as an ordered key-value store and employs ACID transactions for all operations. It is especially well-suited for read/write workloads but also has excellent performance for write-intensive workloads. Users interact with the database using API language binding.\n\nTo learn more about FoundationDB, visit [foundationdb.org](https://www.foundationdb.org/)\n\n## Documentation\n\nDocumentation can be found online at <https://apple.github.io/foundationdb/>. The documentation covers details of API usage, background information on design philosophy, and extensive usage examples. Docs are built from the [source in this repo](documentation/sphinx/source).\n\n## Forums\n\n[The FoundationDB Forums](https://forums.foundationdb.org/) are the home for most of the discussion and communication about the FoundationDB project. We welcome your participation!  We want FoundationDB to be a great project to be a part of and, as part of that, have established a [Code of Conduct](CODE_OF_CONDUCT.md) to establish what constitutes permissible modes of interaction.\n\n## Contributing\n\nContributing to FoundationDB can be in contributions to the code base, sharing your experience and insights in the community on the Forums, or contributing to projects that make use of FoundationDB. Please see the [contributing guide](CONTRIBUTING.md) for more specifics.\n\n## Getting Started\n\n### Binary downloads\n\nDevelopers interested in using FoundationDB can get started by downloading and installing a binary package. Please see the [downloads page](https://www.foundationdb.org/download/) for a list of available packages.\n\n\n### Compiling from source\n\nDevelopers on an OS for which there is no binary package, or who would like\nto start hacking on the code, can get started by compiling from source.\n\nThe official docker image for building is [`foundationdb/build`](https://hub.docker.com/r/foundationdb/build) which has all dependencies installed. The Docker image definitions used by FoundationDB team members can be found in the [dedicated repository.](https://github.com/FoundationDB/fdb-build-support).\n\nTo build outside the official docker image you'll need at least these dependencies:\n\n1. Install cmake Version 3.13 or higher [CMake](https://cmake.org/)\n1. Install [Mono](http://www.mono-project.com/download/stable/)\n1. Install [Ninja](https://ninja-build.org/) (optional, but recommended)\n\nIf compiling for local development, please set `-DUSE_WERROR=ON` in\ncmake. Our CI compiles with `-Werror` on, so this way you'll find out about\ncompiler warnings that break the build earlier.\n\nOnce you have your dependencies, you can run cmake and then build:\n\n1. Check out this repository.\n1. Create a build directory (you can have the build directory anywhere you\n   like). There is currently a directory in the source tree called build, but you should not use it. See [#3098](https://github.com/apple/foundationdb/issues/3098)\n1. `cd <PATH_TO_BUILD_DIRECTORY>`\n1. `cmake -G Ninja <PATH_TO_FOUNDATIONDB_DIRECTORY>`\n1. `ninja # If this crashes it probably ran out of memory. Try ninja -j1`\n\n### Language Bindings\n\nThe language bindings that are supported by cmake will have a corresponding\n`README.md` file in the corresponding `bindings/lang` directory.\n\nGenerally, cmake will build all language bindings for which it can find all\nnecessary dependencies. After each successful cmake run, cmake will tell you\nwhich language bindings it is going to build.\n\n\n### Generating `compile_commands.json`\n\nCMake can build a compilation database for you. However, the default generated\none is not too useful as it operates on the generated files. When running make,\nthe build system will create another `compile_commands.json` file in the source\ndirectory. This can than be used for tools like\n[CCLS](https://github.com/MaskRay/ccls),\n[CQuery](https://github.com/cquery-project/cquery), etc. This way you can get\ncode-completion and code navigation in flow. It is not yet perfect (it will show\na few errors) but we are constantly working on improving the development experience.\n\nCMake will not produce a `compile_commands.json`, you must pass\n`-DCMAKE_EXPORT_COMPILE_COMMANDS=ON`.  This also enables the target\n`processed_compile_commands`, which rewrites `compile_commands.json` to\ndescribe the actor compiler source file, not the post-processed output files,\nand places the output file in the source directory.  This file should then be\npicked up automatically by any tooling.\n\nNote that if building inside of the `foundationdb/build` docker\nimage, the resulting paths will still be incorrect and require manual fixing.\nOne will wish to re-run `cmake` with `-DCMAKE_EXPORT_COMPILE_COMMANDS=OFF` to\nprevent it from reverting the manual changes.\n\n### Using IDEs\n\nCMake has built in support for a number of popular IDEs. However, because flow\nfiles are precompiled with the actor compiler, an IDE will not be very useful as\na user will only be presented with the generated code - which is not what she\nwants to edit and get IDE features for.\n\nThe good news is, that it is possible to generate project files for editing\nflow with a supported IDE. There is a CMake option called `OPEN_FOR_IDE` which\nwill generate a project which can be opened in an IDE for editing. You won't be\nable to build this project, but you will be able to edit the files and get most\nedit and navigation features your IDE supports.\n\nFor example, if you want to use XCode to make changes to FoundationDB you can\ncreate a XCode-project with the following command:\n\n```sh\ncmake -G Xcode -DOPEN_FOR_IDE=ON <FDB_SOURCE_DIRECTORY>\n```\n\nYou should create a second build-directory which you will use for building and debugging.\n\n#### FreeBSD\n\n1. Check out this repo on your server.\n1. Install compile-time dependencies from ports.\n1. (Optional) Use tmpfs & ccache for significantly faster repeat builds\n1. (Optional) Install a [JDK](https://www.freshports.org/java/openjdk8/)\n   for Java Bindings. FoundationDB currently builds with Java 8.\n1. Navigate to the directory where you checked out the foundationdb\n   repo.\n1. Build from source.\n\n    ```shell\n    sudo pkg install -r FreeBSD \\\n        shells/bash devel/cmake devel/ninja devel/ccache  \\\n        lang/mono lang/python3 \\\n        devel/boost-libs devel/libeio \\\n        security/openssl\n    mkdir .build && cd .build\n    cmake -G Ninja \\\n        -DUSE_CCACHE=on \\\n        -DDISABLE_TLS=off \\\n        -DUSE_DTRACE=off \\\n        ..\n    ninja -j 10\n    # run fast tests\n    ctest -L fast\n    # run all tests\n    ctest --output-on-failure -v\n    ```\n\n\n### Linux\n\nThere are no special requirements for Linux.  A docker image can be pulled from\n`foundationdb/build` that has all of FoundationDB's dependencies\npre-installed, and is what the CI uses to build and test PRs.\n\n```\ncmake -G Ninja <FDB_SOURCE_DIR>\nninja\ncpack -G DEB\n```\n\nFor RPM simply replace `DEB` with `RPM`.\n\n### MacOS\n\nThe build under MacOS will work the same way as on Linux. To get boost and ninja you can use [Homebrew](https://brew.sh/).\n\n```sh\ncmake -G Ninja <PATH_TO_FOUNDATIONDB_SOURCE>\n```\n\nTo generate a installable package,\n\n```sh\nninja\n$SRCDIR/packaging/osx/buildpkg.sh . $SRCDIR\n```\n\n### Windows\n\nUnder Windows, only Visual Studio with ClangCl is supported\n\n1. Install Visual Studio 2019 (IDE or Build Tools), and enable llvm support\n1. Install  [CMake 3.15](https://cmake.org/) or higher\n1. Download [Boost 1.77.0](https://boostorg.jfrog.io/artifactory/main/release/1.77.0/source/boost_1_77_0.7z)\n1. Unpack boost to C:\\boost, or use `-DBOOST_ROOT=<PATH_TO_BOOST>` with `cmake` if unpacked elsewhere\n1. Install [Python](https://www.python.org/downloads/) if is not already installed by Visual Studio\n1. (Optional) Install [OpenJDK 11](https://developers.redhat.com/products/openjdk/download) to build Java bindings\n1. (Optional) Install [OpenSSL 3.x](https://slproweb.com/products/Win32OpenSSL.html) to build with TLS support\n1. (Optional) Install [WIX Toolset](http://wixtoolset.org/) to build Windows installer\n1. `mkdir build && cd build`\n1. `cmake -G \"Visual Studio 16 2019\" -A x64 -T ClangCl <PATH_TO_FOUNDATIONDB_SOURCE>`\n1. `msbuild /p:Configuration=Release foundationdb.sln`\n1. To increase build performance, use `/p:UseMultiToolTask=true` and `/p:CL_MPCount=<NUMBER_OF_PARALLEL_JOBS>` "
 },
 {
  "repo": "apple/swift-protobuf",
  "language": "Swift",
  "readme_contents": "<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n\n# Swift Protobuf\n\n**Welcome to Swift Protobuf!**\n\n[Apple's Swift programming language](https://swift.org/) is a perfect\ncomplement to [Google's Protocol\nBuffer](https://developers.google.com/protocol-buffers/) (\"protobuf\") serialization\ntechnology.\nThey both emphasize high performance and programmer safety.\n\nThis project provides both the command-line program that adds Swift\ncode generation to Google's `protoc` and the runtime library that is\nnecessary for using the generated code.\nAfter using the protoc plugin to generate Swift code from your .proto\nfiles, you will need to add this library to your project.\n\n[![Build and Test](https://github.com/apple/swift-protobuf/workflows/Build%20and%20Test/badge.svg)](https://github.com/apple/swift-protobuf/actions?query=workflow%3A%22Build+and+Test%22)\n[![Check Upstream Protos](https://github.com/apple/swift-protobuf/workflows/Check%20Upstream%20Proto%20Files/badge.svg)](https://github.com/apple/swift-protobuf/actions?query=workflow%3A%22Check+Upstream+Proto+Files%22)\n[![Run Conformance Tests](https://github.com/apple/swift-protobuf/workflows/Run%20Conformance%20Tests/badge.svg)](https://github.com/apple/swift-protobuf/actions?query=workflow%3A%22Run+Conformance+Tests%22)\n\n# Features of SwiftProtobuf\n\nSwiftProtobuf offers many advantages over alternative serialization\nsystems:\n\n* Safety: The protobuf code-generation system avoids the\n  errors that are common with hand-built serialization code.\n* Correctness: SwiftProtobuf passes both its own extensive\n  test suite and Google's full conformance test for protobuf\n  correctness.\n* Schema-driven: Defining your data structures in a separate\n  `.proto` schema file clearly documents your communications\n  conventions.\n* Idiomatic: SwiftProtobuf takes full advantage of the Swift language.\n  In particular, all generated types provide full Swift copy-on-write\n  value semantics.\n* Efficient binary serialization: The `.serializedData()`\n  method returns a `Data` with a compact binary form of your data.\n  You can deserialize the data using the `init(serializedData:)`\n  initializer.\n* Standard JSON serialization: The `.jsonUTF8Data()` method returns a JSON\n  form of your data that can be parsed with the `init(jsonUTF8Data:)`\n  initializer.\n* Hashable, Equatable: The generated struct can be put into a\n  `Set<>` or `Dictionary<>`.\n* Performant: The binary and JSON serializers have been\n  extensively optimized.\n* Extensible: You can add your own Swift extensions to any\n  of the generated types.\n\nBest of all, you can take the same `.proto` file and generate\nJava, C++, Python, or Objective-C for use on other platforms. The\ngenerated code for those languages will use the exact same\nserialization and deserialization conventions as SwiftProtobuf, making\nit easy to exchange serialized data in binary or JSON forms, with no\nadditional effort on your part.\n\n# Documentation\n\nMore information is available in the associated documentation:\n\n * [Google's protobuf documentation](https://developers.google.com/protocol-buffers/)\n   provides general information about protocol buffers, the protoc compiler,\n   and how to use protocol buffers with C++, Java, and other languages.\n * [PLUGIN.md](Documentation/PLUGIN.md) documents the `protoc-gen-swift`\n   plugin that adds Swift support to the `protoc` program\n * [API.md](Documentation/API.md) documents how to use the generated code.\n   This is recommended reading for anyone using SwiftProtobuf in their\n   project.\n * [cocoadocs.org](http://cocoadocs.org/docsets/SwiftProtobuf/) has the generated\n   API documentation\n * [INTERNALS.md](Documentation/INTERNALS.md) documents the internal structure\n   of the generated code and the library.  This\n   should only be needed by folks interested in working on SwiftProtobuf\n   itself.\n * [STYLE_GUIDELINES.md](Documentation/STYLE_GUIDELINES.md) documents the style\n   guidelines we have adopted in our codebase if you are interested in\n   contributing\n\n# Getting Started\n\nIf you've worked with Protocol Buffers before, adding Swift support is very\nsimple: you just need to build the `protoc-gen-swift` program and copy it into\nyour PATH.\nThe `protoc` program will find and use it automatically, allowing you\nto build Swift sources for your proto files.\nYou will also, of course, need to add the SwiftProtobuf runtime library to\nyour project as explained below.\n\n## System Requirements\n\nTo use Swift with Protocol buffers, you'll need:\n\n* A Swift 4.2 or later compiler (Xcode 10.0 or later).  Support is included\nfor the Swift Package Manager; or using the included Xcode project. The Swift\nprotobuf project is being developed and tested against the latest release\nversion of Swift available from [Swift.org](https://swift.org)\n\n* Google's protoc compiler.  The Swift protoc plugin is being actively\ndeveloped and tested against the latest protobuf sources.\nThe SwiftProtobuf tests need a version of protoc which supports the\n`swift_prefix` option (introduced in protoc 3.2.0).\nIt may work with earlier versions of protoc.\nYou can get recent versions from\n[Google's github repository](https://github.com/protocolbuffers/protobuf).\n\n## Building and Installing the Code Generator Plugin\n\nTo translate `.proto` files into Swift, you will need both Google's\nprotoc compiler and the SwiftProtobuf code generator plugin.\n\nBuilding the plugin should be simple on any supported Swift platform:\n\n```\n$ git clone https://github.com/apple/swift-protobuf.git\n$ cd swift-protobuf\n```\n\nPick what released version of SwiftProtobuf you are going to use.  You can get\na list of tags with:\n\n```\n$ git tag -l\n```\n\nOnce you pick the version you will use, set your local state to match, and\nbuild the protoc plugin:\n\n```\n$ git checkout tags/[tag_name]\n$ swift build -c release\n```\n\nThis will create a binary called `protoc-gen-swift` in the `.build/release`\ndirectory.\n\nTo install, just copy this one executable into a directory that is\npart of your `PATH` environment variable.\n\nNOTE: The Swift runtime support is now included with macOS. If you are\nusing old Xcode versions or are on older system versions, you might need\nto use also use `--static-swift-stdlib` with `swift build`.\n\n### Alternatively install via Homebrew\n\nIf you prefer using [Homebrew](https://brew.sh):\n\n```\n$ brew install swift-protobuf\n```\n\nThis will install `protoc` compiler and Swift code generator plugin.\n\n## Converting .proto files into Swift\n\nTo generate Swift output for your .proto files, you run the `protoc` command as\nusual, using the `--swift_out=<directory>` option:\n\n```\n$ protoc --swift_out=. my.proto\n```\n\nThe `protoc` program will automatically look for `protoc-gen-swift` in your\n`PATH` and use it.\n\nEach `.proto` input file will get translated to a corresponding `.pb.swift`\nfile in the output directory.\n\nMore information about building and using `protoc-gen-swift` can be found\nin the [detailed Plugin documentation](Documentation/PLUGIN.md).\n\n## Adding the SwiftProtobuf library to your project...\n\nTo use the generated code, you need to include the `SwiftProtobuf` library\nmodule in your project.  How you do this will vary depending on how\nyou're building your project.  Note that in all cases, we strongly recommend\nthat you use the version of the SwiftProtobuf library that corresponds to\nthe version of `protoc-gen-swift` you used to generate the code.\n\n### ...using `swift build`\n\nAfter copying the `.pb.swift` files into your project, you will need to add the\n[SwiftProtobuf library](https://github.com/apple/swift-protobuf) to your\nproject to support the generated code.\nIf you are using the Swift Package Manager, add a dependency to your\n`Package.swift` file and import the `SwiftProtobuf` library into the desired\ntargets.  Adjust the `\"1.6.0\"` here to match the `[tag_name]` you used to build\nthe plugin above:\n\n```swift\ndependencies: [\n    .package(name: \"SwiftProtobuf\", url: \"https://github.com/apple/swift-protobuf.git\", from: \"1.6.0\"),\n],\ntargets: [\n    .target(name: \"MyTarget\", dependencies: [\"SwiftProtobuf\"]),\n]\n```\n\n### ...using Xcode\n\nIf you are using Xcode, then you should:\n\n* Add the `.pb.swift` source files generated from your protos directly to your\n  project\n* Add the appropriate `SwiftProtobuf_<platform>` target from the Xcode project\n  in this package to your project.\n\n### ...using CocoaPods\n\nIf you're using CocoaPods, add this to your `Podfile` adjusting the `:tag` to\nmatch the `[tag_name]` you used to build the plugin above:\n\n```ruby\npod 'SwiftProtobuf', '~> 1.0'\n```\n\nAnd run `pod install`.\n\nNOTE: CocoaPods 1.7 or newer is required.\n\n### ...using Carthage\n\nIf you're using Carthage, add this to your `Cartfile` but adjust the tag to match the `[tag_name]` you used to build the plugin above:\n\n```ruby\ngithub \"apple/swift-protobuf\" ~> 1.0\n```\n\nRun `carthage update` and drag `SwiftProtobuf.framework` into your Xcode.project.\n\n# Quick Start\n\nOnce you have installed the code generator, used it to\ngenerate Swift code from your `.proto` file, and\nadded the SwiftProtobuf library to your project, you can\njust use the generated types as you would any other Swift\nstruct.\n\nFor example, you might start with the following very simple\nproto file:\n```protobuf\nsyntax = \"proto3\";\n\nmessage BookInfo {\n   int64 id = 1;\n   string title = 2;\n   string author = 3;\n}\n```\n\nThen generate Swift code using:\n```\n$ protoc --swift_out=. DataModel.proto\n```\n\nThe generated code will expose a Swift property for\neach of the proto fields as well as a selection\nof serialization and deserialization capabilities:\n```swift\n// Create a BookInfo object and populate it:\nvar info = BookInfo()\ninfo.id = 1734\ninfo.title = \"Really Interesting Book\"\ninfo.author = \"Jane Smith\"\n\n// As above, but generating a read-only value:\nlet info2 = BookInfo.with {\n    $0.id = 1735\n    $0.title = \"Even More Interesting\"\n    $0.author = \"Jane Q. Smith\"\n  }\n\n// Serialize to binary protobuf format:\nlet binaryData: Data = try info.serializedData()\n\n// Deserialize a received Data object from `binaryData`\nlet decodedInfo = try BookInfo(serializedData: binaryData)\n\n// Serialize to JSON format as a Data object\nlet jsonData: Data = try info.jsonUTF8Data()\n\n// Deserialize from JSON format from `jsonData`\nlet receivedFromJSON = try BookInfo(jsonUTF8Data: jsonData)\n```\n\nYou can find more information in the detailed\n[API Documentation](Documentation/API.md).\n\n## Report any issues\n\nIf you run into problems, please send us a detailed report.\nAt a minimum, please include:\n\n* The specific operating system and version (for example, \"macOS 10.12.1\" or\n  \"Ubuntu 16.10\")\n* The version of Swift you have installed (from `swift --version`)\n* The version of the protoc compiler you are working with from\n  `protoc --version`\n* The specific version of this source code (you can use `git log -1` to get the\n  latest commit ID)\n* Any local changes you may have\n"
 },
 {
  "repo": "apple/swift-llbuild",
  "language": "C++",
  "readme_contents": "llbuild\n=======\n\n*A low-level build system.*\n\n**llbuild** is a set of libraries for building build systems. Unlike most build\nsystem projects which focus on the syntax for describing the build, llbuild is\ndesigned around a reusable, flexible, and scalable general purpose *build\nengine* capable of solving many \"build system\"-like problems. The project also\nincludes additional libraries on top of that engine which provide support for\nconstructing *bespoke* build systems (like `swift build`) or for building from\nNinja manifests.\n\nllbuild currently includes:\n\n- [x] A flexible core engine capable of discovering new work on the fly.\n\n- [x] Scalability for dependency graphs reaching millions of nodes.\n\n- [x] Support for building Ninja manifests (e.g., for building LLVM, Clang, and\n  Swift).\n\n- [x] An llbuild-native build description format designed for extensibility.\n\n- [x] Library-based design intended to support embedding and reuse.\n\n\nUsage\n-----\n\nThe project currently produces three top-level products; `llbuild`, `swift-build-tool`,\nand `libllbuild` / `llbuild.framework`.\n\n### `llbuild` Command Line Tool\n\nThe `llbuild` tool provides a command line interface to various feature of the\nllbuild libraries. It has several subtools available, see `llbuild --help` for\nmore information. The most important subtool is the Ninja build support:\n\n#### Ninja Build Support\n\nYou can use `llbuild` to build Ninja-based projects using:\n\n```shell\n$ llbuild ninja build\n```\n\nThis tool supports a subset of the command line arguments supported by Ninja\nitself, to allow it to be used as a compatible replacement, even by tools like\nCMake that depend on particular Ninja command line flags during their\nconfiguration process.\n\nAs a convenience, if you invoke `llbuild` via a symlink called `ninja` then it\nwill automatically use this subtool. This supports installing llbuild as `ninja`\ninto your `PATH` and then using it as an alternative to Ninja for building\narbitrary projects (like LLVM, Clang, and Swift). This is also how we self-host\n`llbuild` (via the CMake Ninja generator).\n\nThe `llbuild ninja` subtool also provides additional commands which are\nprimarily only useful for developers interested in working on the Ninja\nsupport. These commands allow testing the lexer, parser, and manifest loading\ncomponents independently and are used as part of the test suite.\n\nIf you want to rep your use of llbuild in public, you can proudly leverage our\nconveniently provided sticker (PSD version adjacent). \ud83d\ude01\n\n<p align=\"center\">\n<a href=\"docs/im-an-llbuild-ninja-sticker.png\">\n<img src=\"docs/im-an-llbuild-ninja-sticker.png\" alt=\"I'm not always a ninja, but when I am I'm an llbuild ninja.\" width=200/>\n</a>\n</p>\n\n#### Build Trace Files\n\nInspired by Buck, `llbuild ninja` supports a `--profile PATH` option to generate\na\n[Chromium trace](https://www.chromium.org/developers/how-tos/trace-event-profiling-tool) for\nvisualizing where time is spent during a build. For example, the following graph\nis for a build of llbuild itself:\n\n![llbuild build profile](docs/llbuild-profile.png)\n\n### `swift-build-tool` Command Line Tool\n\nThe `swift-build-tool` product is the command line interface to the build system\nused by the [Swift Package Manager](https://swift.org/package-manager/). It is\nbuilt as part of the [Swift](https://swift.org) project build and incorporated\ninto the Swift language snapshots.\n\nThis tool is built on top of the [BuildSystem](docs/buildsystem.rst) library.\n\n### `libllbuild` Library\n\nThe `libllbuild` library exposes a C API for the llbuild libraries, which can be\nused directly by third-parties or to build additional language bindings. See\n[bindings](bindings/) for example Swift and Python bindings that use this\nlibrary.\n\nThis API is what is used, for example, in Xcode as the basis for the new build\nsystem introduced in Xcode 9.\n\n\nMotivation\n----------\n\nThe design of llbuild is a continuation of the LLVM philosophy of applying\nlibrary-based design to traditional developer tools.\n[Clang](http://clang.llvm.org) has followed this approach to deliver a high\nperformance compiler and assembler while also enabling new tools like\nclang-format or the libclang interfaces for code completion and\nindexing. However, the rigid command line interface between traditional build\nsystems and the compiler still limits the optimizations and features which can\nbe implemented in Clang.\n\nllbuild is designed to allow construction of more feature rich build\nenvironments which integrate external tools -- like the compiler -- using APIs\ninstead of command line interfaces. By allowing the build system and tools to\ncommunicate directly and to be co-designed, we believe we can unlock additional\noptimization opportunities and create more robust, easy-to-use build systems.\n\nFor more information, see\n[A New Architecture for Building Software](https://www.youtube.com/watch?v=b_T-eCToX1I)\nfrom the 2016 LLVM Developer's Conference.\n\n\nPhilosophy\n----------\n\nIn the abstract, build systems are used to perform a task while also being:\n\n* Incremental: Outputs should be efficiently rebuilt given a small change to the\n  inputs, by leveraging the ability to save partial outputs from a prior build.\n\n* Consistent: Equivalent inputs should always produce the same result as building from clean.\n  \n* Persistent: Results should be stored so that builds can be interrupted and\n  resumed after failure without needing to redo the full computation.\n\n* Parallel & Efficient: It must be possible to perform independent elements of\n  the computation in parallel, in order to compute the result as efficiently as\n  possible.\n\nWhen viewed in this light, it is clear that the core technology of a build\nsystem is applicable to any complex, long-running computation in which it is\ncommon for the user to only modify a small portion of the input before wanting\nthe recompute the result. For example, a movie editor application will commonly\nneed to rerender small portions of the overall movie in response to interactive\nedits in order to support preview of the final result. However, such\napplications frequently do not take full advantage of the ability to store and\npartially recompute the results because of the complexity of correctly managing\nthe dependencies between parts of the computation.\n\nPart of the goal in designing llbuild around a general purpose build engine is\nto allow its use in contexts which are not traditionally thought of as requiring\na \"build system\".\n\n\nDocumentation\n-------------\n\nTechnical documentation is available at\n[llbuild.readthedocs.io](https://llbuild.readthedocs.io).\n\n\nBug Reports\n-----------\n\nBug reports should be filed in the [Swift OSS Jira](https://bugs.swift.org) in\nthe `llbuild` component.\n\n\nOpen Projects\n-------------\n\nllbuild is a work in progress. Some of the more significant open projects which\nwe hope to tackle are:\n\n- [ ] Support for using file signatures instead of timestamps for change detection.\n\n- [ ] Support richer data types for communication between tasks.\n\n  Tasks currently only compute a single scalar value as their result. We\n  would like to support richer data types for tasks results, for example tasks\n  should be able to compute sets of results, and have the engine automatically\n  communicate the addition or removal of individual items in the set to\n  downstream consumers.\n\n- [ ] Support a more sophisticated database implementation.\n\n  The current implementation uses a SQLite3 database for storing build\n  results. This was a pragmatic choice for bring up, but it can be a performance\n  bottleneck for some applications, and we do not need the flexibility of a full\n  SQL database. We would like to evaluate the tradeoffs of designing a custom\n  solution for llbuild.\n\n- [ ] Support transparent distributed builds.\n\n  We would like llbuild to have facilities for transparently distributing a\n  build across an array of worker machines.\n\n- [ ] Support automatic auditing of build consistency.\n\n  Few build systems diagnose problems effectively. Frequently, undeclared inputs\n  or misbehaving tools can cause inconsistent build results. We would like\n  llbuild to automatically diagnose these problems, for example by periodically or\n  speculatively rebuilding items which are not expected to have changed and\n  comparing the results.\n\n- [ ] Performance tuning of core engine queues.\n\n  The core build engine does its work using a number of queues of work items,\n  and locking for the subset which support concurrent manipulation. We would\n  like to investigate moving the shared queues to using a lock-free data\n  structure and to micro-optimize the queues in general, in order to support\n  very fine-grained task subdivisions without negatively impacting performance.\n\n\nFAQ\n---\n\n*Q. Why does llbuild include some parts of LLVM?*\n\nA. As a low-level, embeddable component, we want llbuild itself to have a\nsimple build process without any significant build time dependencies. However,\nwe also wanted to take advantage of some of the data structures and support\nfacilities that have been developed for LLVM. For now, our solution is to\nincorporate some parts of LLVM's Support libraries into the repository, with the\nhope that over time LLVM will either factor out those libraries in a way that\nmakes it easier to reuse them, or that we will develop our own exclusive set of\nsupport data structures and utilities and drop use of the LLVM ones.\n\n*Q. Why does llbuild include [Ninja](https://ninja-build.org) support?*\n\nA. llbuild includes a Ninja compatibility layer which allows building projects\nwhich use Ninja manifests using the llbuild core engine. We developed this\nsupport as a proof of concept for the core engine, and as a way to bootstrap\nourselves (we develop llbuild using the CMake Ninja generator and llbuild to\nbuild itself). This support is also valuable for allowing direct benchmarking\ncomparisons of llbuild.\n\nOur implementation of Ninja support also includes a separate library for\nprogrammatically loading Ninja manifests, which may prove useful to other\nprojects wishing to use or manipulate Ninja files.\n\nWe intend to continue to maintain the Ninja support to keep compatibility with\nthe main project.\n\n\nAcknowledgements\n----------------\n\nllbuild is heavily influenced by modern build systems like\n[Shake](http://shakebuild.com), [Buck](https://buckbuild.com), and\n[Ninja](https://ninja-build.org). We would particularly like to thank Neil\nMitchell for his work describing the Shake algorithm which provided the\ninspiration for the mechanism llbuild uses to allow additional work to be\ndiscovered on the fly.\n\nFor similar projects, see [Adapton](http://adapton.org/)\nand [Salsa](https://github.com/salsa-rs/salsa).\n\n\nLicense\n-------\n\nCopyright (c) 2014 - 2020 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee http://swift.org/LICENSE.txt for license information.\n\nSee http://swift.org/CONTRIBUTORS.txt for Swift project authors.\n"
 },
 {
  "repo": "apple/swift-syntax",
  "language": "Swift",
  "readme_contents": "# SwiftSyntax\n\nSwiftSyntax is a set of Swift bindings for the\n[libSyntax](https://github.com/apple/swift/tree/main/lib/Syntax) library. It\nallows Swift tools to parse, inspect, generate, and transform Swift source\ncode.\n\nIts API is designed for performance-critical applications. It uses value types almost exclusively and aims to avoid existential conversions where possible.\n\n> Note: SwiftSyntax is still in development, and the API is not guaranteed to\n> be stable. It's subject to change without warning.\n\n## Usage\n\n### Declare SwiftPM dependency with release tag\nAdd this repository to the `Package.swift` manifest of your project:\n\n```swift\n// swift-tools-version:5.2\nimport PackageDescription\n\nlet package = Package(\n  name: \"MyTool\",\n  dependencies: [\n    .package(name: \"SwiftSyntax\", url: \"https://github.com/apple/swift-syntax.git\", .exact(\"<#Specify Release tag#>\")),\n  ],\n  targets: [\n    .target(name: \"MyTool\", dependencies: [\"SwiftSyntax\"]),\n  ]\n)\n```\n\nReplace `<#Specify Release tag#>` by the version of SwiftSyntax that you want to use (see the following table for mapping details).\n\n\n| Xcode Release | Swift Release Tag | SwiftSyntax Release Tag  |\n|:-------------------:|:-------------------:|:-------------------------:|\n| Xcode 13.0   | swift-5.5-RELEASE   | 0.50500.0 |\n| Xcode 12.5   | swift-5.4-RELEASE   | 0.50400.0 |\n| Xcode 12.0   | swift-5.3-RELEASE   | 0.50300.0 |\n| Xcode 11.4   | swift-5.2-RELEASE   | 0.50200.0 |\n\n\nThen, import `SwiftSyntax` in your Swift code.\n\n\n### Declare SwiftPM dependency with nightly build\n\n1. Download and install the latest Trunk Development [toolchain](https://swift.org/download/#snapshots).\n\n2. Define the `TOOLCHAINS` environment variable as below to have the `swift` command point inside the toolchain:\n\n```\n$ export TOOLCHAINS=swift\n```\n\n3. To make sure everything is set up correctly, check the result of `xcrun --find swift`. It should point inside the OSS toolchain.\n\n4. Add this entry to the `Package.swift` manifest of your project:\n\n```swift\n// swift-tools-version:4.2\nimport PackageDescription\n\nlet package = Package(\n  name: \"MyTool\",\n  dependencies: [\n    .package(url: \"https://github.com/apple/swift-syntax.git\", .revision(\"swift-DEVELOPMENT-SNAPSHOT-2019-02-26\")),\n  ],\n  targets: [\n    .target(name: \"MyTool\", dependencies: [\"SwiftSyntax\"]),\n  ]\n)\n```\n\nTags will be created for every nightly build in the form of `swift-DEVELOPMENT-SNAPSHOT-<DATE>`. The revision field\nshould be specified with the intended tag.\n\nDifferent from building SwiftSyntax from source, declaring SwiftSyntax as a SwiftPM dependency doesn't require\nthe Swift compiler source because we always push gyb-generated files to a tag.\n\n### Embedding SwiftSyntax in an Application\n\nSwiftSyntax depends on the `lib_InternalSwiftSyntaxParser.dylib/.so` library which provides a C interface to the underlying Swift C++ parser. When you do `swift build` SwiftSyntax links and uses the library included in the Swift toolchain. If you are building an application make sure to embed `_InternalSwiftSyntaxParser` as part of your application's libraries.\n\nYou can either copy `lib_InternalSwiftSyntaxParser.dylib/.so` directly from the toolchain or even build it yourself from the [Swift repository](https://github.com/apple/swift), as long as you are matching the same tags or branches in both the SwiftSyntax and Swift repositories. To build it for the host os (macOS/linux) use the following steps:\n\n```\ngit clone https://github.com/apple/swift.git\n./swift/utils/update-checkout --clone\n./swift/utils/build-parser-lib --release --no-assertions --build-dir /tmp/parser-lib-build\n```\n\n### Embedding in an iOS Application\n\nYou need to build `lib_InternalSwiftSyntaxParser.dylib` yourself, you cannot copy it from the toolchain. Follow the instructions above and change the invocation of `build-parser-lib` accordingly:\n\n```\n./swift/utils/build-parser-lib --release --no-assertions --build-dir /tmp/parser-lib-build-iossim --host iphonesimulator --architectures x86_64\n./swift/utils/build-parser-lib --release --no-assertions --build-dir /tmp/parser-lib-build-ios --host iphoneos --architectures arm64\n```\n\n### Some Example Users\n\n[**Swift AST Explorer**](https://swift-ast-explorer.kishikawakatsumi.com/): a Swift AST visualizer.\n\n[**swift-format**](https://github.com/apple/swift-format): formatting technology for Swift source code.\n\n[**Swift Stress Tester**](https://github.com/apple/swift-stress-tester): a test driver for sourcekitd and Swift evolution.\n\n[**SwiftSemantics**](https://github.com/SwiftDocOrg/SwiftSemantics): parses Swift code into its constituent declarations.\n\n[**Sitrep**](https://github.com/twostraws/Sitrep): A source code analyzer for Swift projects\n\n[**SwiftRewriter**](https://github.com/inamiy/SwiftRewriter): a Swift code formatter.\n\n[**SwiftPack**](https://github.com/omochi/SwiftPack): a tool for automatically embedding Swift library source.\n\n[**Periphery**](https://github.com/peripheryapp/periphery): a tool to detect unused code.\n\n[**BartyCrouch**](https://github.com/Flinesoft/BartyCrouch): a tool to incrementally update strings files to help App localization.\n\n[**Muter**](https://github.com/muter-mutation-testing/muter): Automated mutation testing for Swift\n\n[**Swift Variable Injector**](https://github.com/LucianoPAlmeida/variable-injector): a tool to replace string literals with environment variables values.\n\n[**Pecker**](https://github.com/woshiccm/Pecker): a tool to detect unused code based on [SwiftSyntax](https://github.com/apple/swift-syntax.git) and [IndexStoreDB](https://github.com/apple/indexstore-db.git).\n\n[**Piranha**](https://github.com/uber/piranha): a tool for refactoring code related to feature flags.\n\n[**STAR**](https://github.com/thumbtack/star): a tool to find how often specified Swift type(s) are used in a project.\n\n### Reporting Issues\n\nIf you should hit any issues while using SwiftSyntax, we appreciate bug reports on [bugs.swift.org](https://bugs.swift.org) in the [SwiftSyntax component](https://bugs.swift.org/issues/?jql=component%20%3D%20SwiftSyntax).\n\n## Contributing\n\n### Building SwiftSyntax from `main`\n\nSince SwiftSyntax relies on definitions in the main Swift repository to generate the layout of the syntax tree using `gyb`, a checkout of [apple/swift](https://github.com/apple/swift) is still required to build the latest development snapshot of SwiftSyntax.\n\nTo build the `main` branch of SwiftSyntax, follow the following instructions:\n\n1. Check `swift-syntax` and  `swift` out side by side:\n\n```\n- (enclosing directory)\n  - swift\n  - swift-syntax\n```\n\n2. Make sure you have a recent [Trunk Swift Toolchain](https://swift.org/download/#snapshots) installed.\n3. Define the `TOOLCHAINS` environment variable as below to have the `swift` command point inside the toolchain:\n\n```\n$ export TOOLCHAINS=swift\n```\n\n4. To make sure everything is set up correctly, check the return statement of `xcrun --find swift`. It should point inside the latest installed trunk development toolchain. If it points inside an Xcode toolchain, check that you exported the `TOOLCHAINS` environment variable correctly. If it points inside a version-specific toolchain (like Swift 5.0-dev), you'll need to remove that toolchain.\n5. Run `swift-syntax/build-script.py`.\n\nIf despite following those instructions, you get compiler errors, the Swift toolchain might be too old to contain recent changes in Swift's SwiftSyntaxParser C library. In that case, you'll have to build the compiler and SwiftSyntax together with the following command:\n\n```\n$ swift/utils/build-script --swiftsyntax --swiftpm --llbuild\n```\n\nSwift-CI will automatically run the code generation step whenever a new toolchain (development snapshot or release) is published. It should thus almost never be necessary to perform the above build yourself.\n\nAfterward, SwiftPM can also generate an Xcode project to develop SwiftSyntax by running `swift package generate-xcodeproj`.\n\nIf you also want to run tests locally, read the section below as testing has additional requirements.\n\n### Local Testing\nSwiftSyntax uses some test utilities that need to be built as part of the Swift compiler project. To build the most recent version of SwiftSyntax and test it, follow the steps in [swift/README.md](https://github.com/apple/swift/blob/main/README.md) and pass `--llbuild --swiftpm --swiftsyntax` to the build script invocation to build SwiftSyntax and all its dependencies using the current trunk (`main`) compiler.\n\nSwiftSyntax can then be tested using the build script in `apple/swift` by running\n```\nswift/utils/build-script --swiftsyntax --swiftpm --llbuild -t --skip-test-cmark --skip-test-swift --skip-test-llbuild --skip-test-swiftpm\n```\nThis command will build SwiftSyntax and all its dependencies, tell the build script to run tests, but skip all tests but the SwiftSyntax tests.\n\nNote that it is not currently supported by SwiftSyntax while building the Swift compiler using Xcode.\n\n### CI Testing\n\nRunning `@swift-ci Please test` on the main Swift repository will also test the most recent version of SwiftSyntax.\n\nTesting SwiftSyntax from its own repository is now available by commenting `@swift-ci Please test macOS platform`.\n\n## Example\n\nThis is a program that adds 1 to every integer literal in a Swift file.\n\n```swift\nimport SwiftSyntax\nimport Foundation\n\n/// AddOneToIntegerLiterals will visit each token in the Syntax tree, and\n/// (if it is an integer literal token) add 1 to the integer and return the\n/// new integer literal token.\nclass AddOneToIntegerLiterals: SyntaxRewriter {\n  override func visit(_ token: TokenSyntax) -> Syntax {\n    // Only transform integer literals.\n    guard case .integerLiteral(let text) = token.tokenKind else {\n      return Syntax(token)\n    }\n\n    // Remove underscores from the original text.\n    let integerText = String(text.filter { (\"0\"...\"9\").contains($0) })\n\n    // Parse out the integer.\n    let int = Int(integerText)!\n\n    // Create a new integer literal token with `int + 1` as its text.\n    let newIntegerLiteralToken = token.withKind(.integerLiteral(\"\\(int + 1)\"))\n    \n    // Return the new integer literal.\n    return Syntax(newIntegerLiteralToken)\n  }\n}\n\nlet file = CommandLine.arguments[1]\nlet url = URL(fileURLWithPath: file)\nlet sourceFile = try SyntaxParser.parse(url)\nlet incremented = AddOneToIntegerLiterals().visit(sourceFile)\nprint(incremented)\n```\n\nThis example turns this:\n\n```swift\nlet x = 2\nlet y = 3_000\n```\n\ninto:\n\n```swift\nlet x = 3\nlet y = 3001\n```\n\n## License\n\n Please see [LICENSE](LICENSE.txt) for more information. \n\n"
 },
 {
  "repo": "apple/swift-package-manager",
  "language": "Swift",
  "readme_contents": "# Swift Package Manager Project\n\nThe Swift Package Manager is a tool for managing distribution of source code, aimed at making it easy to share your code and reuse others\u2019 code. The tool directly addresses the challenges of compiling and linking Swift packages, managing dependencies, versioning, and supporting flexible distribution and collaboration models.\n\nWe\u2019ve designed the system to make it easy to share packages on services like GitHub, but packages are also great for private personal development, sharing code within a team, or at any other granularity.\n\nSwift Package Manager includes a build system that can build for macOS and Linux. Starting with Xcode 11, Xcode integrates with SwiftPM to provide support for including packages in iOS, macOS, watchOS, and tvOS applications.\n\nThe [SourceKit-LSP](https://github.com/apple/sourcekit-lsp) project leverages libSwiftPM and provides [Language Server Protocol](https://langserver.org/) implementation for editors that support LSP.\n\n---\n\n## Table of Contents\n* [Getting Started](#getting-started)\n* [Documentation](#documentation)\n* [System Requirements](#system-requirements)\n* [Installation](#installation)\n* [Contributing](#contributing)\n* [Reporting issues](#reporting-issues)\n* [License](#license)\n\n---\n\n## Getting Started\n\nPlease use [this guide](https://swift.org/getting-started/#using-the-package-manager) for learning package manager basics.\n\n---\n\n## Documentation\n\nFor Quick Help use the ```swift package --help ``` command.\n\nFor documentation on using Swift Package Manager, creating packages, and more, see the [documentation directory](Documentation).\n\nFor documentation on developing the Swift Package Manager itself, see the [contribution guide](CONTRIBUTING.md).\n\nFor detailed documentation on the package manifest API, see [PackageDescription API](https://docs.swift.org/package-manager/PackageDescription/index.html).\n\nFor release notes with information about changes between versions, see the [release notes](Documentation/ReleaseNotes).\n\n---\n\n## System Requirements\n\nThe package manager\u2019s system requirements are the same as [those for Swift](https://github.com/apple/swift#system-requirements) with the caveat that the package manager requires Git at runtime as well as build-time.\n\n---\n\n## Installation\n\nThe package manager is available as part the Swift toolchains available on [Swift.org](https://swift.org/download/)) including snapshots for the latest versions built from `main` branch. For installation instructions for downloaded snapshots, please see the [Getting Started](https://swift.org/getting-started/#installing-swift) section of [Swift.org](https://swift.org).\n\nThe Swift Package Manager is also included in Xcode 8.0 and all subsequent releases.\n\nYou can verify your installation by typing `swift package --version` in a terminal:\n\n```sh\n$ swift package --version\nApple Swift Package Manager - ...\n```\n\n## Contributing\n\nThere are several ways to contribute to Swift Package Manager. To learn about the policies, best practices that govern contributions to the Swift project and instructions for setting up the development environment please read the [Contributor Guide](CONTRIBUTING.md).  \n\nThe Swift package manager uses [llbuild](https://github.com/apple/swift-llbuild) as the underlying build system for compiling source files. It is also open source and part of the Swift project.\n\n---\n\n## Reporting issues\n\nIf you have any trouble with the package manager, help is available. We recommend:\n\n* The [Swift Forums](https://forums.swift.org/c/development/swiftpm/),\n* Swift's [bug tracker](http://bugs.swift.org)\n\nWhen reporting an issue please follow the bug reporting guidelines, they can be found in [contribution guide](./CONTRIBUTING.md#reporting-issues).\n\nIf you\u2019re not comfortable sharing your question with the list, contact details for the code owners can be found in [CODEOWNERS](CODEOWNERS); however, the mailing list is usually the best place to go for help.\n\n---\n\n## License\n\nCopyright 2015 - 2020 Apple Inc. and the Swift project authors. Licensed under Apache License v2.0 with Runtime Library Exception.\n\nSee [https://swift.org/LICENSE.txt](https://swift.org/LICENSE.txt) for license information.\n\nSee [https://swift.org/CONTRIBUTORS.txt](https://swift.org/CONTRIBUTORS.txt) for Swift project authors.\n"
 },
 {
  "repo": "apple/swift-source-compat-suite",
  "language": "Python",
  "readme_contents": "# Swift Source Compatibility Suite\n\nSource compatibility is a strong goal for future Swift releases. To aid in this\ngoal, a community owned source compatibility test suite serves to regression\ntest changes to the compiler against a (gradually increasing) corpus of Swift\nsource code. Projects added to this test suite are periodically built against\nthe latest development versions of Swift as part of [Swift's continuous\nintegration system](https://ci.swift.org), allowing Swift compiler developers to\nunderstand the compatibility impact their changes have on real-world Swift\nprojects.\n\n## Current List of Projects\n\nThe <a href=\"https://swift.org/source-compatibility/#current-list-of-projects\">current list of projects</a> can be viewed on Swift.org.\n\n## Adding Projects\n\nThe Swift source compatibility test suite is community driven, meaning that open\nsource Swift project owners are encouraged to submit their projects that meet\nthe acceptance criteria for inclusion in the test suite. Projects added to the\nsuite serve as general source compatibility tests and are afforded greater\nprotection against unintentional source breakage in future Swift releases.\n\n### Acceptance Criteria\n\nTo be accepted into the Swift source compatibility test suite, a project must:\n\n1. Target Linux, macOS, or iOS/tvOS/watchOS device\n2. Be an *Xcode* or *Swift Package Manager* project (Carthage and CocoaPods are currently unsupported but are being explored to be supported in the future)\n3. Support building on either Linux or macOS\n4. Be contained in a publicly accessible git repository\n5. Maintain a project branch that builds against Swift 4.2 compatibility mode\n   and passes any unit tests\n6. Have maintainers who will commit to resolve issues in a timely manner\n7. Be compatible with the latest GM/Beta versions of *Xcode* and *swiftpm*\n8. Add value not already included in the suite\n9. Be licensed with one of the following permissive licenses:\n\t* BSD\n\t* MIT\n\t* Apache License, version 2.0\n\t* Eclipse Public License\n\t* Mozilla Public License (MPL) 1.1\n\t* MPL 2.0\n\t* CDDL\n\nNote: Linux compatibility testing in continuous integration is not available\nyet, but Linux projects are being accepted now.\n\n### Adding a Project\n\nTo add a project meeting the acceptance criteria to the suite, perform the\nfollowing steps:\n\n1. Ensure the project builds successfully at a chosen commit against\n   Swift 4.2 GM\n2. Create a pull request against the [source compatibility suite\n   repository](https://github.com/apple/swift-source-compat-suite),\n   modifying **projects.json** to include a reference to the project being added\n   to the test suite.\n\nThe project index is a JSON file that contains a list of repositories containing\nXcode and/or Swift Package Manager target actions.\n\nTo add a new Swift Package Manager project, use the following template:\n\n~~~json\n{\n  \"repository\": \"Git\",\n  \"url\": \"https://github.com/example/project.git\",\n  \"path\": \"project\",\n  \"branch\": \"master\",\n  \"maintainer\": \"email@example.com\",\n  \"compatibility\": [\n    {\n      \"version\": \"4.2\",\n      \"commit\": \"195cd8cde2bb717242b3081f9c367ccd0a2f0121\"\n    }\n  ],\n  \"platforms\": [\n    \"Darwin\"\n  ],\n  \"actions\": [\n    {\n      \"action\": \"BuildSwiftPackage\",\n      \"configuration\": \"release\"\n    },\n    {\n      \"action\": \"TestSwiftPackage\"\n    }\n  ]\n}\n~~~\n\nThe `compatibility` field contains a list of version dictionaries, each\ncontaining a Swift version and a commit. Commits are checked out before\nbuilding a project in the associated Swift version compatibility mode. The\nSwift version is the earliest version of Swift known to compile the project at\nthe given commit. The goal is to have multiple commits at different points in a\nproject's history that are compatible with all supported Swift version\ncompatibility modes.\n\nThe `platforms` field specifies the platforms that can be used to build the\nproject. Linux and Darwin can currently be specified.\n\nIf tests aren't supported, remove the test action entry.\n\nTo add a new Swift Xcode workspace, use the following template:\n\n~~~json\n{\n  \"repository\": \"Git\",\n  \"url\": \"https://github.com/example/project.git\",\n  \"path\": \"project\",\n  \"branch\": \"master\",\n  \"maintainer\": \"email@example.com\",\n  \"compatibility\": [\n    {\n      \"version\": \"4.2\",\n      \"commit\": \"195cd8cde2bb717242b3081f9c367ccd0a2f0121\"\n    }\n  ],\n  \"platforms\": [\n    \"Darwin\"\n  ],\n  \"actions\": [\n    {\n      \"action\": \"BuildXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project OSX\",\n      \"destination\": \"platform=macOS\",\n      \"configuration\": \"Release\"\n    },\n    {\n      \"action\": \"BuildXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project iOS\",\n      \"destination\": \"generic/platform=iOS\",\n      \"configuration\": \"Release\"\n    },\n    {\n      \"action\": \"BuildXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project tvOS\",\n      \"destination\": \"generic/platform=tvOS\",\n      \"configuration\": \"Release\"\n    },\n    {\n      \"action\": \"BuildXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project watchOS\",\n      \"destination\": \"generic/platform=watchOS\",\n      \"configuration\": \"Release\"\n    },\n    {\n      \"action\": \"TestXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project OSX\",\n      \"destination\": \"platform=macOS\"\n    },\n    {\n      \"action\": \"TestXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project iOS\",\n      \"destination\": \"platform=iOS Simulator,name=iPhone 7\"\n    },\n    {\n      \"action\": \"TestXcodeWorkspaceScheme\",\n      \"workspace\": \"project.xcworkspace\",\n      \"scheme\": \"project tvOS\",\n      \"destination\": \"platform=tvOS Simulator,name=Apple TV 1080p\"\n    }\n  ]\n}\n~~~\n\nTo add a new Swift Xcode project, use the following template:\n\n~~~json\n{\n  \"repository\": \"Git\",\n  \"url\": \"https://github.com/example/project.git\",\n  \"path\": \"project\",\n  \"branch\": \"master\",\n  \"maintainer\": \"email@example.com\",\n  \"compatibility\": [\n    {\n      \"version\": \"4.2\",\n      \"commit\": \"195cd8cde2bb717242b3081f9c367ccd0a2f0121\"\n    }\n  ],\n  \"platforms\": [\n    \"Darwin\"\n  ],\n  \"actions\": [\n    {\n      \"action\": \"BuildXcodeProjectTarget\",\n      \"project\": \"project.xcodeproj\",\n      \"target\": \"project\",\n      \"destination\": \"generic/platform=iOS\",\n      \"configuration\": \"Release\"\n    }\n  ]\n}\n~~~\n\nAfter adding a new project to the index, ensure it builds successfully at the\npinned commits against the specified versions of Swift. In the examples,\nthe commits are specified as being compatible with Swift 4.2, which is included\nin Xcode 10.\n\n~~~bash\n# Select Xcode 10 GM\nsudo xcode-select -s /Applications/Xcode.app\n# Build project at pinned commit against selected Xcode\n./project_precommit_check project-path-field --earliest-compatible-swift-version 4.2\n~~~\n\nOn Linux, you can build against the Swift 4.2 release toolchain:\n\n~~~bash\ncurl -O https://swift.org/builds/swift-4.2-release/ubuntu1604/swift-4.2-RELEASE/swift-4.2-RELEASE-ubuntu16.04.tar.gz\ntar xzvf swift-4.2-RELEASE-ubuntu16.04.tar.gz\n./project_precommit_check project-path-field --earliest-compatible-swift-version 4.2 --swiftc swift-4.2-RELEASE-ubuntu15.10/usr/bin/swiftc\n~~~\n\n## Maintaining Projects\n\nIn the event that Swift introduces a change that breaks source compatibility\nwith a project (e.g., a compiler bug fix that fixes wrong behavior in the\ncompiler), project maintainers are expected to update their projects and submit\na new pull request with the updated commit hash within two weeks of being\nnotified. Otherwise, unmaintained projects may be removed from the project\nindex.\n\n## Pull Request Testing\n\nPull request testing against the Swift source compatibility suite can be\nexecuted by commenting with `@swift-ci Please test source compatibility` in a\nSwift pull request.\n\n## Building Projects\n\nTo build all projects against a specified Swift compiler locally, use the\n`runner.py` utility as shown below.\n\n~~~bash\n./runner.py --swift-branch main --projects projects.json --include-actions 'action.startswith(\"Build\")' --swiftc path/to/swiftc\n~~~\n\nUse the `--include-repos` flag to build a specific project.\n\n~~~bash\n./runner.py --swift-branch main --projects projects.json --include-actions 'action.startswith(\"Build\")' --include-repos 'path == \"Alamofire\"' --swiftc path/to/swiftc\n~~~\n\nBy default, build output is redirected to per-action `.log` files in the current\nworking directory. To change this behavior to output build results to standard\nout, use the `--verbose` flag.\n\n## Marking actions as expected failures\n\nWhen an action is expected to fail for an extended period of time, it's\nimportant to mark the action as an expected failure to make new failures more\nvisible.\n\nTo mark an action as an expected failure, add an `xfail` entry for the correct\nSwift version and branch to the failing actions, associating each with a link\nto a JIRA issue reporting the relevant failure. The following is an example of\nan action that's XFAIL'd when building against Swift main branch in 4.2\ncompatibility mode.\n\n~~~json\n{\n  \"repository\": \"Git\",\n  \"url\": \"https://github.com/example/project.git\",\n  \"path\": \"project\",\n  \"branch\": \"master\",\n  \"maintainer\": \"email@example.com\",\n  \"compatibility\": [\n    {\n      \"version\": \"4.2\",\n      \"commit\": \"195cd8cde2bb717242b3081f9c367ccd0a2f0121\"\n    }\n  ],\n  \"platforms\": [\n    \"Darwin\"\n  ],\n  \"actions\": [\n    {\n      \"action\": \"BuildXcodeProjectTarget\",\n      \"project\": \"project.xcodeproj\",\n      \"target\": \"project\",\n      \"destination\": \"generic/platform=iOS\",\n      \"configuration\": \"Release\",\n      \"xfail\": {\n        \"issue\": \"https://bugs.swift.org/browse/SR-9999\",\n        \"compatibility\": \"4.2\",\n        \"branch\": \"main\"\n      }\n    }\n  ]\n}\n~~~\n\nAdditional Swift branches and versions can be added to XFAIL different\nconfigurations. The currently supported fields for XFAIL entries are:\n\n- `\"compatibility\"`: the Swift version(s) it fails with, e.g. `\"4.0\"`\n- `\"branch\"`: the branch(es) of the swift compiler it fails with, e.g.\n  `\"swift-5.1-branch\"`\n- `\"platform\"`: the platform(s) it fails on, e.g. `\"Darwin\"` or `\"Linux\"`\n- `\"configuration\"`: the build configuration(s) if fails with, i.e. `\"release\"`\n  or `\"debug\"`)\n- `\"job\"`: Allows XFailing the project for only the source compatibility build \n  or the SourceKit Stress Tester. Use `\"source-compat\"` to only XFail the Source \n  Compatibility Suite CI job and `\"stress-test\"` to only stress test the \n  SourceKit Stress Tester CI job.\n\nValues can either be a single string literal or a list of alternative string\nliterals to match against. For example the below action is expected to fail on\nboth main and swift-5.1-branch in both 4.0 and 5.1 compatibility modes:\n\n~~~json\n...\n{\n  \"action\": \"BuildXcodeProjectTarget\",\n  \"project\": \"project.xcodeproj\",\n  \"target\": \"project\",\n  \"destination\": \"generic/platform=iOS\",\n  \"configuration\": \"Release\",\n  \"xfail\": {\n    \"issue\": \"https://bugs.swift.org/browse/SR-9999\",\n    \"compatibility\": [\"4.0\", \"5.1\"],\n    \"branch\": [\"main\", \"swift-5.1-branch\"]\n  }\n}\n...\n~~~\n\nIf an action is failing for different reasons in different configurations, the\nvalue of the action's `\"xfail\"` entry can also become a list rather than\na single entry. In this case the `\"issue\"` of the first item that matches will\nbe reported. In the below example any failure on Linux would be reported as\n*SR-7777*, while a failure on other platforms would be reported as *SR-8888*\nusing a toolchain built from the *master* branch and *SR-9999* using a\ntoolchain built from *swift-5.1-branch*. If the entries were in the reverse\norder, *SR-7777* would only be reported for Linux failures with toolchains built\nfrom a branch other than *main* or *swift-5.1-branch*.\n\n~~~json\n...\n{\n  \"action\": \"BuildXcodeProjectTarget\",\n  \"project\": \"project.xcodeproj\",\n  \"target\": \"project\",\n  \"destination\": \"generic/platform=iOS\",\n  \"configuration\": \"Release\",\n  \"xfail\": [\n    {\n      \"issue\": \"https://bugs.swift.org/browse/SR-7777\",\n      \"platform\": \"Linux\"\n    },\n    {\n      \"issue\": \"https://bugs.swift.org/browse/SR-8888\",\n      \"branch\": \"main\"\n    },\n    {\n      \"issue\": \"https://bugs.swift.org/browse/SR-9999\",\n      \"branch\": \"swift-5.1-branch\"\n    }\n  ]\n}\n...\n~~~\n"
 },
 {
  "repo": "apple/swift-markdown",
  "language": "Swift",
  "readme_contents": "# Swift Markdown\n\nSwift `Markdown` is a Swift package for parsing, building, editing, and analyzing Markdown documents.\n\nThe parser is powered by GitHub-flavored Markdown's [cmark-gfm](https://github.com/github/cmark-gfm) implementation, so it follows the spec closely. As the needs of the community change, the effective dialect implemented by this library may change.\n\nThe markup tree provided by this package is comprised of immutable/persistent, thread-safe, copy-on-write value types that only copy substructure that has changed. Other examples of the main strategy behind this library can be seen in Swift's [lib/Syntax](https://github.com/apple/swift/tree/master/lib/Syntax) and its Swift bindings, [SwiftSyntax](https://github.com/apple/swift-syntax).\n\n## Getting Started Using Markup\n\nIn your `Package.swift` Swift Package Manager manifest, add the following dependency to your `dependencies` argument:\n\n```swift\n.package(url: \"ssh://git@github.com/apple/swift-markdown.git\", .branch(\"main\")),\n```\n\nAdd the dependency to any targets you've declared in your manifest:\n\n```swift\n.target(name: \"MyTarget\", dependencies: [\"Markdown\"]),\n```\n\n## Parsing\n\nTo parse a document, use `Document(parsing:)`, supplying a `String` or `URL`:\n\n```swift\nimport Markdown\n\nlet source = \"This is a markup *document*.\"\nlet document = Document(parsing: source)\nprint(document.debugDescription())\n// Document\n// \u2514\u2500 Paragraph\n//    \u251c\u2500 Text \"This is a markup \"\n//    \u251c\u2500 Emphasis\n//    \u2502  \u2514\u2500 Text \"document\"\n//    \u2514\u2500 Text \".\"\n```\n\nParsing text is just one way to build a tree of `Markup` elements. You can also build them yourself declaratively.\n\n## Building Markup Trees\n\nYou can build trees using initializers for the various element types provided.\n\n```swift\nimport Markdown\n\nlet document = Document(\n    Paragraph(\n        Text(\"This is a \"),\n        Emphasis(\n            Text(\"paragraph.\"))))\n```\n\nThis would be equivalent to parsing `\"This is a *paragraph.*\"` but allows you to programmatically insert content from other data sources into individual elements.\n\n## Modifying Markup Trees with Persistence\n\nSwift Markdown uses a [persistent](https://en.wikipedia.org/wiki/Persistent_data_structure) tree for its backing storage, providing effectively immutable, copy-on-write value types that only copy the substructure necessary to create a unique root without affecting the previous version of the tree.\n\n### Modifying Elements Directly\n\nIf you just need to make a quick change, you can modify an element anywhere in a tree, and Swift Markdown will create copies of substructure that cannot be shared.\n\n```swift\nimport Markdown\n\nlet source = \"This is *emphasized.*\"\nlet document = Document(parsing: source)\nprint(document.debugDescription())\n// Document\n// \u2514\u2500 Paragraph\n//    \u251c\u2500 Text \"This is \"\n//    \u2514\u2500 Emphasis\n//       \u2514\u2500 Text \"emphasized.\"\n\nvar text = document.child(through:\n    0, // Paragraph\n    1, // Emphasis\n    0) as! Text // Text\n\ntext.string = \"really emphasized!\"\nprint(text.root.debugDescription())\n// Document\n// \u2514\u2500 Paragraph\n//    \u251c\u2500 Text \"This is \"\n//    \u2514\u2500 Emphasis\n//       \u2514\u2500 Text \"really emphasized!\"\n\n// The original document is unchanged:\n\nprint(document.debugDescription())\n// Document\n// \u2514\u2500 Paragraph\n//    \u251c\u2500 Text \"This is \"\n//    \u2514\u2500 Emphasis\n//       \u2514\u2500 Text \"emphasized.\"\n```\n\nIf you find yourself needing to systematically change many parts of a tree, or even provide a complete transformation into something else, maybe the familiar [Visitor Pattern](https://en.wikipedia.org/wiki/Visitor_pattern) is what you want.\n\n## Visitors, Walkers, and Rewriters\n\nThere is a core `MarkupVisitor` protocol that provides the basis for transforming, walking, or rewriting a markup tree.\n\n```swift\npublic protocol MarkupVisitor {\n    associatedtype Result\n}\n```\n\nUsing its `Result` type, you can transform a markup tree into anything: another markup tree, or perhaps a tree of XML or HTML elements. There are two included refinements of `MarkupVisitor` for common uses.\n\nThe first refinement, `MarkupWalker`, has an associated `Result` type of `Void`, so it's meant for summarizing or detecting aspects of a markup tree. If you wanted to append to a string as elements are visited, this might be a good tool for that.\n\n```swift\nimport Markdown\n\n/// Counts `Link`s in a `Document`.\nstruct LinkCounter: MarkupWalker {\n    var count = 0\n    mutating func visitLink(_ link: Link) {\n        if link.destination == \"https://swift.org\" {\n            count += 1\n        }\n        descendInto(link)\n    }\n}\n\nlet source = \"There are [two](https://swift.org) links to <https://swift.org> here.\"\nlet document = Document(parsing: source)\nprint(document.debugDescription())\nvar linkCounter = LinkCounter()\nlinkCounter.visit(document)\nprint(linkCounter.count)\n// 2\n```\n\nThe second refinement, `MarkupRewriter`, has an associated `Result` type of `Markup?`, so it's meant to change or even remove elements from a markup tree. You can return `nil` to delete an element, or return another element to substitute in its place.\n\n```swift\nimport Markdown\n\n/// Delete all **strong** elements in a markup tree.\nstruct StrongDeleter: MarkupRewriter {\n    mutating func visitStrong(_ strong: Strong) -> Markup? {\n        return nil\n    }\n}\n\nlet source = \"Now you see me, **now you don't**\"\nlet document = Document(parsing: source)\nvar strongDeleter = StrongDeleter()\nlet newDocument = strongDeleter.visit(document)\n\nprint(newDocument!.debugDescription())\n// Document\n// \u2514\u2500 Paragraph\n//    \u2514\u2500 Text \"Now you see me, \"\n```\n\n## Block Directives\n\nSwift Markdown includes a syntax extension for attributed block elements. See [Block Directive](Documentation/BlockDirectives.md) documentation for more information.\n\n## Getting Involved\n\n### Submitting a Bug Report\n\nSwift Markdown tracks all bug reports with [Swift JIRA](https://bugs.swift.org/).\nYou can use the \"Swift-Markdown\" component for issues and feature requests specific to Swift Markdown.\nWhen you submit a bug report we ask that you follow the\nSwift [Bug Reporting](https://swift.org/contributing/#reporting-bugs) guidelines\nand provide as many details as possible.\n\n### Submitting a Feature Request\n\nFor feature requests, please feel free to create an issue\non [Swift JIRA](https://bugs.swift.org/) with the `New Feature` type\nor start a discussion on the [Swift Forums](https://forums.swift.org/c/development/swift-docc).\n\nDon't hesitate to submit a feature request if you see a way\nSwift Markdown can be improved to better meet your needs.\n\n### Contributing to Swift Markdown\n\nPlease see the [contributing guide](https://swift.org/contributing/#contributing-code) for more information.\n\n<!-- Copyright (c) 2021 Apple Inc and the Swift Project authors. All Rights Reserved. -->\n"
 },
 {
  "repo": "apple/swift-docc-symbolkit",
  "language": "Swift",
  "readme_contents": "# SymbolKit \n\nThe specification and reference model for the *Symbol Graph* File Format.\n\nA *Symbol Graph* models a *module*, also known in various programming languages as a \"framework\", \"library\", or \"package\", as a [directed graph](https://en.wikipedia.org/wiki/Directed_graph). In this graph, the nodes are declarations, and the edges connecting nodes are relationships between declarations.\n\nTo illustrate the shape of a symbol graph, take the following Swift code as a module called `MyModule`:\n\n```swift\npublic struct MyStruct {\n  public var x: Int\n}\n```\n\nThere are two nodes in this module's graph: the structure `MyStruct` and its property, `x`:\n\n![A graph with 2 nodes: \"Struct Node\" MyStruct and \"Instance Property Node\" x](Sources/SymbolKit/SymbolKit.docc/Resources/twonodes@2x.png)\n\n`x` is related to `MyStruct`: it is a *member* of `MyStruct`. SymbolKit represents *relationships* as directed edges in the graph:\n\n![Node x has a directed edge with text \"memberof\" to Node MyStruct](Sources/SymbolKit/SymbolKit.docc/Resources/member@2x.png)\n\nThe *source* of an edge points to its *target*. You can read this edge as *`x` is a member of `MyStruct`*. Every edge is qualified by some kind of relationship; in this case, the kind is membership. There can be many kinds of relationships, even multiple relationships between the same two nodes. Here's another example, adding a Swift protocol to the mix:\n\n```swift\npublic protocol P {}\n\npublic struct MyStruct: P {\n  public var x: Int\n}\n```\n\nNow we've added a new node for the protocol `P`, and a new conformance relationship between `MyStruct` and `P`:\n\n![Node x has a directed edge with text \"memberof\" to Node MyStruct, Node MyStruct has a directed edge with text \"conformsTo\" to \"Protocol Node\" P](Sources/SymbolKit/SymbolKit.docc/Resources/conforms@2x.png)\n\nBy modeling different kinds of relationships, SymbolKit can provide rich data to power documentation, answering interesting questions, such as:\n\n- *Which types conform to this protocol?*\n- *What is the class hierarchy rooted at this class?*\n- *Which protocol provides a requirement called `count`?*\n- *Which types customize this protocol requirement?*\n\nIn addition, graph representations of data also present opportunities for visualizations in documentation, illustrating the structure or hierarchy of a module.\n\n<!-- Copyright (c) 2021 Apple Inc and the Swift Project authors. All Rights Reserved. -->\n"
 },
 {
  "repo": "apple/swift-docc-render-artifact",
  "language": null,
  "readme_contents": "# Swift-DocC-Render-Artifact\n\nThis repository holds a pre-built copy of the [Swift-DocC-Render](https://github.com/apple/swift-docc-render)\nproject auto-generated by the Swift.org CI system. Swift-DocC-Render is a web Single Page Application (SPA)\npowered by [Vue.js](https://vuejs.org/) for rendering [Swift-DocC](https://github.com/apple/swift-docc)\ndocumentation.\n\nThe files here are available to developers interested in using the latest version\nof Swift-DocC-Render with Swift-DocC.\n\n[Swift-DocC](https://github.com/apple/swift-docc) is a tool for building and previewing documentation, \naimed at making it easy to generate reference documentation from frameworks and packages, \nas well as free-form articles and tutorials authored in Markdown. Please see the \n[Swift-DocC contribution guide](https://github.com/apple/swift-docc/blob/main/CONTRIBUTING.md)\nfor information on how to use the pre-built renderer in this repository\nwith Swift-DocC.\n"
 },
 {
  "repo": "apple/swift-lmdb",
  "language": null,
  "readme_contents": "# CLMDB\n\nCLMDB is a SwiftPM package wrapper for [LMDB](http://www.lmdb.tech/doc/), a key-value database storage solution created for fast read and write processes. You can read more about this project at: <http://www.lmdb.tech/doc>.\n\nThis wrapper provides exposure of the library's C interfaces to Swift, allowing a Swift application or service to easily create, read, and manage database instances of LMDB.\n\nChanges in LMDB will be pulled into this repository as the upstream project evolves. The LMDB repository is mirrored here: <https://github.com/LMDB/lmdb>.\n\n## License\n\nThe included LMDB database project is released under [The OpenLDAP Public License](https://git.openldap.org/openldap/openldap/-/blob/mdb.master/libraries/liblmdb/LICENSE). This same license applies to the Swift code included in this repository.\n"
 },
 {
  "repo": "apple/swift-tools-support-core",
  "language": "Swift",
  "readme_contents": "swift-tools-support-core\n=========================\n\nContains common infrastructural code for both [SwiftPM](https://github.com/apple/swift-package-manager)\nand [llbuild](https://github.com/apple/swift-llbuild).\n\nDevelopment\n-------------\n\nAll changes to source files in this repository need to be done in the repository of the Swift Package Manager repository ([link](https://github.com/apple/swift-package-manager)) and then copied here using the Script in `Utilities/import` which takes the local path to the SwiftPM directory as input (or uses `../swiftpm` as default).\nAll targets with a TSC prefix in [SwiftPM](https://github.com/apple/swift-package-manager) are part of the swift-tools-support-core and will be imported by the import script. The plan is to eventually move ownership to this repository.\n\nLicense\n-------\n\nCopyright (c) 2014 - 2019 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee http://swift.org/LICENSE.txt for license information.\n"
 },
 {
  "repo": "apple/swift-driver",
  "language": "Swift",
  "readme_contents": "# Swift Compiler Driver\n\nSwift's compiler driver is a program that coordinates the compilation of Swift source code into various compiled results: executables, libraries, object files, Swift modules and interfaces, etc. It is the program one invokes from the command line to build Swift code (i.e., `swift` or `swiftc`) and is often invoked on the developer's behalf by a build system such as the [Swift Package Manager (SwiftPM)](https://github.com/apple/swift-package-manager) or Xcode's build system.\n\nThe `swift-driver` project is a new implementation of the Swift compiler driver that is intended to replace the [existing driver](https://github.com/apple/swift/tree/main/lib/Driver) with a more extensible, maintainable, and robust code base. The specific goals of this project include:\n\n* A maintainable, robust, and flexible Swift code base\n* Library-based architecture that allows better integration with build tools\n* Leverage existing Swift build technologies ([SwiftPM](https://github.com/apple/swift-package-manager), [llbuild](https://github.com/apple/swift-llbuild))\n* A platform for experimenting with more efficient build models for Swift, including compile servers and unifying build graphs across different driver invocations\n\n## Getting Started\n\n**Note:** Currently, swift-driver is only compatible with trunk development snapshots from [swift.org](https://swift.org/download/#snapshots).\n\nThe preferred way to build `swift-driver` is to use the Swift package manager:\n\n```\n$ swift build\n```\n\nTo use `swift-driver` in place of the existing Swift driver, create a symbolic link from `swift` and `swiftc` to `swift-driver`:\n\n```\nln -s /path/to/built/swift-driver $SOME_PATH/swift\nln -s /path/to/built/swift-driver $SOME_PATH/swiftc\n```\n\nSwift packages can be built with the new Swift driver by overriding `SWIFT_EXEC` to refer to the `swiftc` symbolic link created above and `SWIFT_DRIVER_SWIFT_FRONTEND_EXEC` to refer to the original `swift-frontend`, e.g.,\n\n```\nSWIFT_EXEC=$SOME_PATH/swiftc SWIFT_DRIVER_SWIFT_FRONTEND_EXEC=$TOOLCHAIN_PATH/bin/swift-frontend swift build\n```\n\nSimilarly, one can use the new Swift driver within Xcode by adding a custom build setting (usually at the project level) named `SWIFT_EXEC` that refers to `$SOME_PATH/swiftc` and adding `-driver-use-frontend-path $TOOLCHAIN_DIR/usr/bin/swiftc` to `Other Swift Flags`.\n\n## Building with CMake\n\n`swift-driver` can also be built with CMake, which is suggested for\nenvironments where the Swift Package Manager is not yet\navailable. Doing so requires several dependencies to be built first,\nall with CMake:\n\n* (Non-Apple platforms only) [swift-corelibs-foundation](https://github.com/apple/swift-corelibs-foundation)\n* [llbuild](https://github.com/apple/swift-llbuild) configure CMake with `-DLLBUILD_SUPPORT_BINDINGS=\"Swift\"` and `-DCMAKE_OSX_ARCHITECTURES=x86_64` (If building on Intel) when building\n  ```\n  cmake -B <llbuild-build-dir> -G Ninja <llbuild-source-dir> -DLLBUILD_SUPPORT_BINDINGS=\"Swift\" -DCMAKE_OSX_ARCHITECTURES=x86_64\n  ```\n* [swift-argument-parser](https://github.com/apple/swift-argument-parser)\n* [Yams](https://github.com/jpsim/Yams)\n\nOnce those dependencies have built, build `swift-driver` itself:\n```\ncmake -B <swift-driver-build-dir> -G Ninja <swift-driver-source-dir> -DTSC_DIR=<swift-tools-support-core-build-dir>/cmake/modules -DLLBuild_DIR=<llbuild-build-dir>/cmake/modules -DYams_DIR=<yamls-build-dir>/cmake/modules -DArgumentParser_DIR=<swift-argument-parser-build-dir>\ncmake --build <swift-driver-build-dir>\n```\n\n## Developing `swift-driver`\n\nThe new Swift driver is a work in progress, and there are numerous places for anyone with an interest to contribute! This section covers testing, miscellaneous development tips and tricks, and a rough development plan showing what work still needs to be done.\n\n### Driver Documentation\n\nFor a conceptual overview of the driver, see [The Swift Driver, Compilation Model, and Command-Line Experience](https://github.com/apple/swift/blob/main/docs/Driver.md). To learn more about the internals, see [Driver Design & Internals](https://github.com/apple/swift/blob/main/docs/DriverInternals.md) and [Parseable Driver Output](https://github.com/apple/swift/blob/main/docs/DriverParseableOutput.md).\n\n### Testing\n\nTest using command-line SwiftPM or Xcode.\n\n```\n$ swift test --parallel\n```\n\nIntegration tests are costly to run and are disabled by default. Enable them\nusing `SWIFT_DRIVER_ENABLE_INTEGRATION_TESTS` environment variable. In Xcode,\nyou can set this variable in the scheme's test action.\n\n```\n$ SWIFT_DRIVER_ENABLE_INTEGRATION_TESTS=1 swift test --parallel\n```\n\nSome integration tests run the lit test suites in a Swift working copy.\nTo enable these, clone Swift and its dependencies and build them with\nbuild-script, then set both `SWIFT_DRIVER_ENABLE_INTEGRATION_TESTS`\nand `SWIFT_DRIVER_LIT_DIR`, either in your Xcode scheme or\non the command line:\n\n```\n$ SWIFT_DRIVER_ENABLE_INTEGRATION_TESTS=1 \\\n  SWIFT_DRIVER_LIT_DIR=/path/to/build/Ninja-ReleaseAssert/swift-.../test-... \\\n  swift test -c release --parallel\n```\n\n#### Testing against `swift` compiler trunk\n`swift-driver` Continuous Integration runs against the most recent Trunk Development snapshot published at [swift.org/download](https://swift.org/download/).\n\nWhen developing patches that have complex interactions with the underlying `swift` compiler frontend, it may be prudent to ensure that `swift-driver` tests also pass against the current tip-of-trunk `swift`. To do so, create an empty pull request against [github.com/apple/swift](https://github.com/apple/swift) and perform cross-repository testing against your `swift-driver` pull request #, for example:\n```\nUsing:\napple/swift-driver#208\n@swift-ci smoke test\n```\n@swift-ci cross-repository testing facilities are described [here](https://github.com/apple/swift/blob/main/docs/ContinuousIntegration.md#cross-repository-testing).\n\n### Testing in Xcode with custom toolchain\nAfter the toolchain is installed, Xcode needs to be told to use it. This can mean two things, building the driver with the toolchain and telling the driver to use the toolchain when running.\n\nBuilding with the toolchain is easy, set the toolchain in Xcode: Menu Bar > Xcode > Toolchains > select your toolchain\n\nRunning the driver requires setting the TOOLCHAINS environment variable. This tells xcrun which toolchain to use (on darwin xcrun is used to find tools). This variable is the name of the toolchain and not the path (ex: `Swift Development Snapshot`). Important note: xcrun lookup is lower priority than the SWIFT_EXEC_*_EXEC family of environment variables, the tools directory, and any tools in the same directory as the driver (This includes a driver installed in a toolchain). Even though TOOLCHAINS is not highest priority it's a convenient way to run the xctest suite using a custom toolchain.\n\n#### Preparing a Linux docker for debug\n\nWhen developing on macOS without quick access to a Linux machine, using a Linux Docker is often helpful when debugging.\n\nTo get a docker up and running to the following:\n- Install Docker for Mac.\n- Get the newest swift docker image `docker pull swift`.\n- Run the following command to start a docker\n```\n$ docker run -v /path/to/swift-driver:/home/swift-driver \\\n  --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n  --security-opt apparmor=unconfined -it swift:latest bash\n```\n- Install dependencies by running\n```\n$ apt-get update\n$ apt-get install libsqlite3-dev\n$ apt-get install libncurses-dev\n```\n- You can now go to `/home/swift-driver` and run `swift test --parallel` to run your tests.\n\n\n### Rebuilding `Options.swift`\n\n`Options.swift`, which contains the complete set of options that can be parsed by the driver, is automatically generated from the [option tables in the Swift compiler](https://github.com/apple/swift/tree/main/include/swift/Option). If you need to regenerate `Options.swift`, you will need to [build the Swift compiler](https://github.com/apple/swift#building-swift) and then build `makeOptions` program with a `-I` that allows the generated `Options.inc` to\nbe found, e.g.:\n\n```\n$ swift build -Xcc -I/path/to/build/Ninja-ReleaseAssert/swift-.../include --product makeOptions\n```\n\nThen, run `makeOptions` and redirect the output to overwrite `Options.swift`:\n\n```\n$ .build/path/to/makeOptions > Sources/SwiftOptions/Options.swift\n```\n\n### Development Plan\n\nThe goal of the new Swift driver is to provide a drop-in replacement for the existing driver, which means that there is a fixed initial feature set to implement before the existing Swift driver can be deprecated and removed. The development plan below covers that feature set, as well as describing a number of tasks that can improve the Swift driver---from code cleanups, to improving testing, implementing missing features, and integrating with existing systems.\n\n* Code and documentation quality\n  * [ ] Search for `FIXME:` or `TODO:`: there are lots of little things to improve!\n  * [ ] Improve documentation of how to incorporate the driver into your own builds\n  * [ ] Add useful descriptions to any `Error` thrown within the library\n* Option parsing\n  * [ ] Look for complete \"coverage\" of the options in `Options.swift`. Is every option there checked somewhere in the driver?\n  * [ ] Find a better way to describe aliases for options. Can they be of some other type `OptionAlias` so we can't make the mistake of (e.g.) asking for an alias option when we're translating options?\n  * [ ] Diagnose unused options on the command line\n  * [ ] Typo correction for misspelled option names\n  * [ ] Find a better way than `makeOptions.cpp` to translate the command-line options from [Swift's repository](https://github.com/apple/swift/tree/main/include/swift/Option) into `Options.swift`.\n* Platform support\n  * [x] Teach the `DarwinToolchain` to also handle iOS, tvOS, watchOS\n  * [x] Fill out the `GenericUnixToolchain` toolchain to get it working\n  * [x] Implement a `WindowsToolchain`\n  * [x] Implement proper tokenization for response files\n* Compilation modes\n  * [x] Batch mode\n  * [x] Whole-module-optimization mode\n  * [x] REPL mode\n  * [x] Immediate mode\n* Features\n  * [x] Precompiled bridging headers\n  * [x] Support embedding of bitcode\n  * [x] Incremental compilation\n  * [x] Parseable output, as used by SwiftPM\n  * [x] Response files\n  * [x] Input and primary input file lists\n  * [x] Complete `OutputFileMap` implementation to handle all file types uniformly\n* Testing\n  * [ ] Build stuff with SwiftPM or Xcode or your favorite build system, using `swift-driver`. Were the results identical? What changed?\n  * [x] Shim in `swift-driver` so it can run the Swift repository's [driver test suite](https://github.com/apple/swift/tree/main/test/Driver).\n  * [ ] Investigate differences in the test results for the Swift repository's driver test suite (above) between the existing and new driver.\n  * [ ] Port interesting tests from the Swift repository's [driver test suite](https://github.com/apple/swift/tree/main/test/Driver) over to XCTest\n  * [ ] Fuzz the command-line options to try to crash the Swift driver itself\n* Integration\n  * [x] Teach the Swift compiler's [`build-script`](https://github.com/apple/swift/blob/main/utils/build-script) to build `swift-driver`.\n  * [x] Building on the above, teach the Swift compiler's [`build-toolchain`](https://github.com/apple/swift/blob/main/utils/build-toolchain) to install `swift-driver` as the primary driver so we can test full toolchains with the new driver\n"
 },
 {
  "repo": "apple/indexstore-db",
  "language": "C++",
  "readme_contents": "# IndexStoreDB\n\nIndexStoreDB is a source code indexing library. It provides a composable and efficient query API for looking up source code symbols, symbol occurrences, and relations. IndexStoreDB uses the libIndexStore library, which lives in [llvm-project](https://github.com/apple/llvm-project/tree/apple/main/clang/tools/IndexStore), for reading raw index data. Raw index data can be produced by compilers such as Clang and Swift using the `-index-store-path` option. IndexStoreDB enables efficiently querying this data by maintaining acceleration tables in a key-value database built with [LMDB](http://www.lmdb.tech/).\n\nIndexStoreDB's data model is derived from libIndexStore. For more information about libIndexStore, and producing raw indexing data, see the [Indexing While Building](https://docs.google.com/document/d/1cH2sTpgSnJZCkZtJl1aY-rzy4uGPcrI-6RrUpdATO2Q/) whitepaper.\n\n## Building IndexStoreDB\n\nIndexStoreDB is built using the [Swift Package Manager](https://github.com/apple/swift-package-manager).\n\nFor a standard debug build and test:\n\n```sh\n$ swift build\n$ swift test\n```\n\n### Building on Linux\n\nThe C++ code in the index requires `libdispatch`, but unlike Swift code, it cannot find it automatically on Linux. You can work around this by adding a search path manually.\n\n```sh\n$ swift build -Xcxx -I<path_to_swift_toolchain>/usr/lib/swift -Xcxx -I<path_to_swift_toolchain>/usr/lib/swift/Block\n```\n\n## Some Example Users\n\n[**Pecker**](https://github.com/woshiccm/Pecker): a tool to detect unused code based on [SwiftSyntax](https://github.com/apple/swift-syntax.git) and [IndexStoreDB](https://github.com/apple/indexstore-db.git).\n\n## Development\n\nFor more information about developing IndexStoreDB, see [Development](Documentation/Development.md).\n"
 },
 {
  "repo": "apple/swift-stress-tester",
  "language": "Swift",
  "readme_contents": "<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n\n# Swift Stress Tester\n\nThis project aims to provide stress testing utilities to help find reproducible crashes and other failures in tools that process Swift source code, such as the Swift compiler and SourceKit. These utilities will ideally be written in Swift and make use the SwiftSyntax and/or SwiftLang libraries to parse, generate and modify Swift source inputs.\n\n## Current tools\n\n| Tool      | Description | build-script Flag | Package Name |\n| --------- | ----------- | ----------------- | ----------------- |\n[sk&#8209;stress&#8209;test](SourceKitStressTester/README.md) | a utility for exercising a range of SourceKit functionality, such as code completion and local refactorings, at all applicable locations in a set of Swift source files. | `--skstresstester` | SourceKitStressTester |\n[swift&#8209;evolve](SwiftEvolve/README.md) | a utility to randomly modify Swift source files in ways libraries are permitted to evolve without breaking ABI compatibility. | `--swiftevolve` | SwiftEvolve |\n\n## Building\n\nThe tools in this repository can be built in several different ways:\n\n### Using Swift's build-script\n\nIf you want to build the tools to use a locally built sourcekitd and SwiftLang, use the Swift repository's build-script to build and test the stress tester by passing `--skstresstester`, its dependencies and the desired tools' flags as extra options. To build and run tests, for example, you would run:\n\n```\n$ ./swift/utils/build-script --test --skip-build-benchmark --skip-test-cmark --skip-test-swift --install-swift --llbuild --install-llbuild --skip-test-llbuild --swiftpm --install-swiftpm --skip-test-swiftpm --skstresstester --swiftevolve --release\n```\n\n### For local development\n\nFor local development, you'll first need to download and install a recent [swift.org development snapshot](https://swift.org/download/#snapshots) toolchain that matches the latest commit on main in the [SwiftSyntax](https://github.com/apple/swift-syntax). This is because the Stress Tester depends on the latest version of SwiftSyntax and SwiftSyntax integrates into the latest version of the compiler.\n\nThe toolchain is installed into `/Library/Developer/Toolchains/` if installed for all users. Note that the `$TOOLCHAIN_DIR` variables below should include `/usr` at the end of their path, eg. `TOOLCHAIN_DIR=/Library/Developer/Toolchains/swift-DEVELOPMENT-SNAPSHOT-<...>.xctoolchain/usr`.\n\n#### Via Xcode\n\nTo generate an Xcode project that's set up correctly, run `build-script-helper.py`, passing the path to the downloaded toolchain via the `--toolchain` option, the tool's package name in the `--package-dir` option, and the `generate-xcodeproj` action:\n```\n$ ./build-script-helper.py --package-dir SourceKitStressTester --toolchain $TOOLCHAIN_DIR generate-xcodeproj --no-local-deps\n```\nIf you have the [SwiftSyntax](https://github.com/apple/swift-syntax) and [SwiftPM](https://github.com/apple/swift-package-manager) repositories already checked out next to the stress tester's repository, you can omit the `--no-local-deps` option to use the existing checkouts instead of fetching the dependencies using SwiftPM.\n\nThis will generate `SourceKitStressTester/SourceKitStressTester.xcodeproj`. Open it and select the toolchain you installed from the Xcode > Toolchains menu, before building the `SourceKitStressTester-Package` scheme.\n\n#### Via command line\n\nTo build, run `build-script-helper.py`, passing the path to the downloaded toolchain via the `--toolchain` option and the tool's package name in the `--package-dir` option.\n```\n$ ./build-script-helper.py --package-dir SourceKitStressTester --toolchain $TOOLCHAIN_DIR\n```\nIf you have the [SwiftSyntax](https://github.com/apple/swift-syntax) and [SwiftPM](https://github.com/apple/swift-package-manager) repositories already checked out next to the stress tester's repository, you can omit the `--no-local-deps` option to use the existing checkouts instead of fetching the dependencies using SwiftPM.\n\nTo run the tests, repeat the above command, but additionally, pass the `test` action:\n```\n$ ./Utilities/build-script-helper.py test --package-dir SourceKitStressTester --toolchain $TOOLCHAIN_DIR\n```\n\n## Running\n\nBuilding will create either one or two executables, depending on the package you build. These will be in the package directory's `.build/debug` subdirectory if building on the command line or via the Swift repo's build-script, and under `Products/Debug` in the Xcode project's `DerivedData` directory if building there. They are also available in the `usr/bin` directory of the recent trunk and swift 5.0 development toolchains from swift.org, if you're just interested in running them, rather than building them locally.\n\nSee the individual packages' README files for information about how to run and use their executables.\n\n## License\n\nCopyright \u00a9 2014 - 2018 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee [http://swift.org/LICENSE.txt](http://swift.org/LICENSE.txt) for license information.\n"
 },
 {
  "repo": "apple/swift-xcode-playground-support",
  "language": "Swift",
  "readme_contents": "\n# Swift Xcode Playground Support\n\n**Welcome to Swift Xcode Playground Support!**\n\nThe Xcode Playground Support project enables Xcode playgrounds to\ncommunicate their results in a displayable fashion, and to control some\naspects of playground behavior. In order to use a custom build of Swift\nwith Xcode the frameworks described below must be compiled with the new\nSwift compiler and included in a toolchain. Snapshots will include this\nproject to deliver everything required. Just download, install, and\nselect the toolchain to work with the corresponding Swift features in\nXcode playgrounds.\n\n## Contents\n\nThe project contains these frameworks:\n\n* **PlaygroundSupport.** This framework defines API that may be\nexplicitly referred to by playground code to communicate with Xcode. For\nexample: this is typical for playgrounds that identify a particular\nview to display live for animation or interaction, and when playgrounds\nautomatically move between pages when defined criteria are met. A legacy\nXCPlayground version is also provided for compatibility with older\nplaygrounds.\n\n* **PlaygroundLogger.** This project is used implicitly to record values\nof interest on a line-by-line basis and communicate them to Xcode. Calls\nare automatically injected into playground code so no explicit reference\nis required.\n\n## Documentation\n\nPlaygroundSupport offers a limited set of APIs for communicating specific\nrequests to Xcode and should be self-explanatory.\n\nPlaygroundLogger is more complex and includes documentation on the API in\n[PlaygroundLogger/Documentation/LoggerAPI.md](PlaygroundLogger/Documentation/LoggerAPI.md)\nand communication format in\n[PlaygroundLogger/Documentation/LoggerFormat.md](PlaygroundLogger/Documentation/LoggerFormat.md).\n\n## Working with Xcode Playground Support\n\n### Getting Started\n\nThe standard Swift build script included in the [Swift repository](http://github.com/apple/swift)\nautomatically pulls and builds the Xcode Playground Support source.\n\n### Getting Sources Directly\n\n**Via HTTPS**  For those checking out sources as read-only, HTTPS works best:\n\n    git clone https://github.com/apple/swift-xcode-playground-support.git\n    cd swift-xcode-playground-support\n\n**Via SSH**  For those who plan on regularly making direct commits,\ncloning over SSH may provide a better experience (which requires\nuploading SSH keys to GitHub):\n\n    git clone git@github.com:apple/swift-xcode-playground-support.git\n    cd swift-xcode-playground-support\n\n### Building Individual Frameworks\n\n#### Inside Xcode\n\nPlaygroundSupport and PlaygroundLogger can be built individually building the\nrespective Xcode projects. It is critical that they're built using the Swift\ncompiler they're intended to be used with, which can be achieved by building,\ninstalling, and selecting the corresponding toolchain in the Xcode UI.\n\nBoth projects contain universal targets that build all applicable products.\nThe target in PlaygroundLogger is named `All Platforms Logger`; in\nPlaygroundSupport it is named `AllProducts`.\n\nFor testing purposes, PlaygroundLogger requires the presence of the\n`StdlibUnittest` module which is part of Swift. This can be overriden either by\nusing the `BuildAndIntegration` configuration, or via the `NOSTDLIBUNITTEST`\nconditional compilation directive.\n\n#### Command-Line Builds\n\nCommand-line builds using `xcodebuild` require setting the `SWIFT_EXEC`\nenvironment variable to a fully-qualified path for the corresponding Swift\ncompiler.\n\n## Contributing to Swift Xcode Playground Support\n\nSwift Xcode Playground Support changes may be required as the Swift language\nevolves. As with other aspects of Swift, contributions are welcomed and encouraged!\nPlease see the [Contributing to Swift guide](https://swift.org/contributing/).\n\nTo be a truly great community, [Swift.org](https://swift.org/) needs to welcome developers from all\nwalks of life, with different backgrounds, and with a wide range of experience.\nA diverse and friendly community will have more great ideas, more unique\nperspectives, and produce more great code. We will work diligently to make the\nSwift community welcoming to everyone.\n\nTo give clarity of what is expected of our members, Swift has adopted the\ncode of conduct defined by the Contributor Covenant. This document is used\nacross many open source communities, and we think it articulates our values\nwell. For more, see the [Code of Conduct](https://swift.org/community/#code-of-conduct).\n"
 },
 {
  "repo": "apple/swift-integration-tests",
  "language": "Python",
  "readme_contents": "Swift Package Tests\n===================\n\nAutomated tests for validating the generated Swift snapshots behave correctly.\n\nUsage\n-----\n\nYou are expected to check this repository out as a peer of \"llvm\" in the\nswift-project.\n\nRun the tests using:\n\n    sh ./litTest -sv --param package-path=/path/to/downloadable-package .\n\nwhere the path is the unarchived package root path.\n\nTests\n-----\n\nHere is a partial list of tests in the repository:\n\n| Test Name                | Functionality                                                    |\n|--------------------------|------------------------------------------------------------------|\n| basic                    | Check output of `swift --version`                                |\n| example-package-dealer   | Build the example package-dealer package                         |\n| repl                     | Various REPL sanity checks, notably importing Darwin and Glibc   |\n| swift-build-self-host    | Use swift build to build itself                                  |\n| swift-compiler           | Compile a basic swift file                                       |\n| test-c-library-swiftpm   | Build a package that links a 3rd party library                   |\n| test-foundation-package  | Build a package that imports Foundation                          |\n| test-import-glibc        | Compile a source file importing and using Glibc                  |\n| test-multi-compile       | Compile multiple source files into an executable                 |\n| test-multi-compile-glibc | Compile multiple source files importing Glibc into an executable |\n| test-static-lib          | Compile multiple source files into a static library              |\n| test-xctest-package      | Build a package that imports XCTest                              |\n\n"
 },
 {
  "repo": "apple/swift-cmark",
  "language": "C",
  "readme_contents": "cmark\n=====\n\n[![Build Status]](https://travis-ci.org/commonmark/cmark)\n[![Windows Build Status]](https://ci.appveyor.com/project/jgm/cmark-0ub06)\n\n`cmark` is the C reference implementation of [CommonMark], a\nrationalized version of Markdown syntax with a [spec][the spec].\n(For the JavaScript reference implementation, see\n[commonmark.js].)\n\nIt provides a shared library (`libcmark`) with functions for parsing\nCommonMark documents to an abstract syntax tree (AST), manipulating\nthe AST, and rendering the document to HTML, groff man, LaTeX,\nCommonMark, or an XML representation of the AST.  It also provides a\ncommand-line program (`cmark`) for parsing and rendering CommonMark\ndocuments.\n\nAdvantages of this library:\n\n- **Portable.**  The library and program are written in standard\n  C99 and have no external dependencies.  They have been tested with\n  MSVC, gcc, tcc, and clang.\n\n- **Fast.** cmark can render a Markdown version of *War and Peace* in\n  the blink of an eye (127 milliseconds on a ten year old laptop,\n  vs. 100-400 milliseconds for an eye blink).  In our [benchmarks],\n  cmark is 10,000 times faster than the original `Markdown.pl`, and\n  on par with the very fastest available Markdown processors.\n\n- **Accurate.** The library passes all CommonMark conformance tests.\n\n- **Standardized.** The library can be expected to parse CommonMark\n  the same way as any other conforming parser.  So, for example,\n  you can use `commonmark.js` on the client to preview content that\n  will be rendered on the server using `cmark`.\n\n- **Robust.** The library has been extensively fuzz-tested using\n  [american fuzzy lop].  The test suite includes pathological cases\n  that bring many other Markdown parsers to a crawl (for example,\n  thousands-deep nested bracketed text or block quotes).\n\n- **Flexible.** CommonMark input is parsed to an AST which can be\n  manipulated programmatically prior to rendering.\n\n- **Multiple renderers.**  Output in HTML, groff man, LaTeX, CommonMark,\n  and a custom XML format is supported. And it is easy to write new\n  renderers to support other formats.\n\n- **Free.** BSD2-licensed.\n\nIt is easy to use `libcmark` in python, lua, ruby, and other dynamic\nlanguages: see the `wrappers/` subdirectory for some simple examples.\n\nThere are also libraries that wrap `libcmark` for\n[Go](https://github.com/rhinoman/go-commonmark),\n[Haskell](https://hackage.haskell.org/package/cmark),\n[Ruby](https://github.com/gjtorikian/commonmarker),\n[Lua](https://github.com/jgm/cmark-lua),\n[Perl](https://metacpan.org/release/CommonMark),\n[Python](https://pypi.python.org/pypi/paka.cmark),\n[R](https://cran.r-project.org/package=commonmark) and\n[Scala](https://github.com/sparsetech/cmark-scala).\n\nInstalling\n----------\n\nBuilding the C program (`cmark`) and shared library (`libcmark`)\nrequires [cmake].  If you modify `scanners.re`, then you will also\nneed [re2c] \\(>= 0.14.2\\), which is used to generate `scanners.c` from\n`scanners.re`.  We have included a pre-generated `scanners.c` in\nthe repository to reduce build dependencies.\n\nIf you have GNU make, you can simply `make`, `make test`, and `make\ninstall`.  This calls [cmake] to create a `Makefile` in the `build`\ndirectory, then uses that `Makefile` to create the executable and\nlibrary.  The binaries can be found in `build/src`.  The default\ninstallation prefix is `/usr/local`.  To change the installation\nprefix, pass the `INSTALL_PREFIX` variable if you run `make` for the\nfirst time: `make INSTALL_PREFIX=path`.\n\nFor a more portable method, you can use [cmake] manually. [cmake] knows\nhow to create build environments for many build systems.  For example,\non FreeBSD:\n\n    mkdir build\n    cd build\n    cmake ..  # optionally: -DCMAKE_INSTALL_PREFIX=path\n    make      # executable will be created as build/src/cmark\n    make test\n    make install\n\nOr, to create Xcode project files on OSX:\n\n    mkdir build\n    cd build\n    cmake -G Xcode ..\n    open cmark.xcodeproj\n\nThe GNU Makefile also provides a few other targets for developers.\nTo run a benchmark:\n\n    make bench\n\nFor more detailed benchmarks:\n\n    make newbench\n\nTo run a test for memory leaks using `valgrind`:\n\n    make leakcheck\n\nTo reformat source code using `clang-format`:\n\n    make format\n\nTo run a \"fuzz test\" against ten long randomly generated inputs:\n\n    make fuzztest\n\nTo do a more systematic fuzz test with [american fuzzy lop]:\n\n    AFL_PATH=/path/to/afl_directory make afl\n\nFuzzing with [libFuzzer] is also supported but, because libFuzzer is still\nunder active development, may not work with your system-installed version of\nclang. Assuming LLVM has been built in `$HOME/src/llvm/build` the fuzzer can be\nrun with:\n\n    CC=\"$HOME/src/llvm/build/bin/clang\" LIB_FUZZER_PATH=\"$HOME/src/llvm/lib/Fuzzer/libFuzzer.a\" make libFuzzer\n\nTo make a release tarball and zip archive:\n\n    make archive\n\nInstalling (Windows)\n--------------------\n\nTo compile with MSVC and NMAKE:\n\n    nmake\n\nYou can cross-compile a Windows binary and dll on linux if you have the\n`mingw32` compiler:\n\n    make mingw\n\nThe binaries will be in `build-mingw/windows/bin`.\n\nUsage\n-----\n\nInstructions for the use of the command line program and library can\nbe found in the man pages in the `man` subdirectory.\n\nSecurity\n--------\n\nBy default, the library will scrub raw HTML and potentially\ndangerous links (`javascript:`, `vbscript:`, `data:`, `file:`).\n\nTo allow these, use the option `CMARK_OPT_UNSAFE` (or\n`--unsafe`) with the command line program. If doing so, we\nrecommend you use a HTML sanitizer specific to your needs to\nprotect against [XSS\nattacks](http://en.wikipedia.org/wiki/Cross-site_scripting).\n\nContributing\n------------\n\nThere is a [forum for discussing\nCommonMark](http://talk.commonmark.org); you should use it instead of\ngithub issues for questions and possibly open-ended discussions.\nUse the [github issue tracker](http://github.com/commonmark/CommonMark/issues)\nonly for simple, clear, actionable issues.\n\nAuthors\n-------\n\nJohn MacFarlane wrote the original library and program.\nThe block parsing algorithm was worked out together with David\nGreenspan. Vicent Marti optimized the C implementation for\nperformance, increasing its speed tenfold.  K\u0101rlis Ga\u0146\u0123is helped\nwork out a better parsing algorithm for links and emphasis,\neliminating several worst-case performance issues.\nNick Wellnhofer contributed many improvements, including\nmost of the C library's API and its test harness.\n\n[benchmarks]: benchmarks.md\n[the spec]: http://spec.commonmark.org\n[CommonMark]: http://commonmark.org\n[cmake]: http://www.cmake.org/download/\n[re2c]: http://re2c.org\n[commonmark.js]: https://github.com/commonmark/commonmark.js\n[Build Status]: https://img.shields.io/travis/commonmark/cmark/master.svg?style=flat\n[Windows Build Status]: https://ci.appveyor.com/api/projects/status/h3fd91vtd1xfmp69?svg=true\n[american fuzzy lop]: http://lcamtuf.coredump.cx/afl/\n[libFuzzer]: http://llvm.org/docs/LibFuzzer.html\n"
 },
 {
  "repo": "apple/swift-corelibs-libdispatch",
  "language": "C",
  "readme_contents": "# Grand Central Dispatch\n\nGrand Central Dispatch (GCD or libdispatch) provides comprehensive support for concurrent code execution on multicore hardware.\n\nlibdispatch is currently available on all Darwin platforms. This project aims to make a modern version of libdispatch available on all other Swift platforms. To do this, we will implement as much of the portable subset of the API as possible, using the existing open source C implementation.\n\nlibdispatch on Darwin is a combination of logic in the `xnu` kernel alongside the user-space Library. The kernel has the most information available to balance workload across the entire system. As a first step, however, we believe it is useful to bring up the basic functionality of the library using user-space pthread primitives on Linux.  Eventually, a Linux kernel module could be developed to support more informed thread scheduling.\n\n## Project Status\n\nA port of libdispatch to Linux has been completed. On Linux, since Swift 3, swift-corelibs-libdispatch has been included in all Swift releases and is used by other swift-corelibs projects.\n\nOpportunities to contribute and on-going work include:\n\n1. Develop a test suite for the Swift APIs of libdispatch.\n2. Enhance libdispatch as needed to support Swift language evolution and the needs of the other Core Libraries projects.\n\n## Build and Install\n\nFor detailed instructions on building and installing libdispatch, see [INSTALL.md](INSTALL.md)\n\n## Testing\n\nFor detailed instructions on testing libdispatch, see [TESTING.md](TESTING.md)\n"
 },
 {
  "repo": "apple/swift-corelibs-foundation",
  "language": "Swift",
  "readme_contents": "# Foundation\n\nThe Foundation framework defines a base layer of functionality that is required for almost all applications. It provides primitive classes and introduces several paradigms that define functionality not provided by either the Objective-C runtime and language or Swift standard library and language.\n\nIt is designed with these goals in mind:\n\n* Provide a small set of basic utility classes.\n* Make software development easier by introducing consistent conventions.\n* Support internationalization and localization, to make software accessible to users around the world.\n* Provide a level of OS independence, to enhance portability.\n\nThere is more information on the Foundation framework [here](https://developer.apple.com/library/mac/documentation/Cocoa/Reference/Foundation/ObjC_classic/).\n\nThis project, `swift-corelibs-foundation`, provides an implementation of the Foundation API for platforms where there is no Objective-C runtime. On macOS, iOS, and other Apple platforms, apps should use the Foundation that comes with the operating system. Our goal is for the API in this project to match the OS-provided Foundation and abstract away the exact underlying platform as much as possible.\n\n## Common API\n\nOur primary goal is to achieve implementation parity with Foundation on Apple platforms. This will help to enable the overall Swift goal of **portability**.\n\nTherefore, we are not looking to make major API changes to the library that do not correspond with changes made to Foundation on Apple platforms. However, there are some areas where API changes are unavoidable. In these cases, documentation on the method will provide additional detail of the reason for the difference.\n\nFor more information on those APIs and the overall design of Foundation, please see [our design document](Docs/Design.md).\n\n## Current Status\n\nSee our [status page](Docs/Status.md) for a detailed list of what features are currently implemented.\n\n## Using Foundation\n\nHere is a simple `main.swift` file which uses Foundation. This guide assumes you have already installed a version of the latest [Swift binary distribution](https://swift.org/download/#latest-development-snapshots).\n\n```swift\nimport Foundation\n\n// Make a URLComponents instance\nlet swifty = URLComponents(string: \"https://swift.org\")!\n\n// Print something useful about the URL\nprint(\"\\(swifty.host!)\")\n\n// Output: \"swift.org\"\n```\n\nYou will want to use the [Swift Package Manager](https://swift.org/package-manager/) to build your Swift apps.\n\n## Working on Foundation\n\nFor information on how to build Foundation, please see [Getting Started](Docs/GettingStarted.md). If you would like, please consult our [status page](Docs/Status.md) to see where you can make the biggest impact, and once you're ready to make changes of your own, check out our [information on contributing](CONTRIBUTING.md).\n\n## FAQ\n\n#### Why include Foundation on Linux?\n\nWe believe that the Swift standard library should remain small and laser-focused on providing support for language primitives. The Foundation framework has the flexibility to include higher-level concepts and to build on top of the standard library, much in the same way that it builds upon the C standard library and Objective-C runtime on Darwin platforms.\n\n#### Why include NSString, NSDictionary, NSArray, and NSSet? Aren't those already provided by the standard library?\n\nThere are several reasons why these types are useful in Swift as distinct types from the ones in the standard library:\n\n* They provide reference semantics instead of value semantics, which is a useful tool to have in the toolbox.\n* They can be subclassed to specialize behavior while maintaining the same interface for the client.\n* They exist in archives, and we wish to maintain as much forward and backward compatibility with persistence formats as is possible.\n* They are the backing for almost all Swift `Array`, `Dictionary`, and `Set` objects that you receive from frameworks implemented in Objective-C on Darwin platforms. This may be considered an implementation detail, but it leaks into client code in many ways. We want to provide them here so that your code will remain portable.\n\n#### How do we decide if something belongs in the standard library or Foundation?\n\nIn general, the dividing line should be drawn in overlapping area of what people consider the language and what people consider to be a library feature.\n\nFor example, `Optional` is a type provided by the standard library. However, the compiler understands the concept to provide support for things like optional-chaining syntax. The compiler also has syntax for creating instances of type `Array` and `Dictionary`.\n\nOn the other hand, the compiler has no built-in support for types like `URL`. `URL` also ties into more complex functionality like basic networking support. Therefore this type is more appropriate for Foundation.\n\n#### Why not make the existing Objective-C implementation of Foundation open source?\n\nFoundation on Darwin is written primarily in Objective-C, and the Objective-C runtime is not part of the Swift open source project. CoreFoundation, however, is a portable C library and does not require the Objective-C runtime. It contains much of the behavior that is exposed via the Foundation API. Therefore, it is used on all platforms including Linux.\n\n#### How do I contribute?\n\nWe welcome contributions to Foundation! Please see the [known issues](Docs/Issues.md) page if you are looking for an area where we need help. We are also standing by on the [mailing lists](https://swift.org/community/#communication) to answer questions about what is most important to do and what we will accept into the project.\n"
 },
 {
  "repo": "apple/swift-corelibs-xctest",
  "language": "Swift",
  "readme_contents": "# XCTest\n\nThe XCTest library is designed to provide a common framework for writing unit tests in Swift, for Swift packages and applications.\n\nThis version of XCTest implements the majority of unit testing APIs included in XCTest from Xcode 7 and later. Its goal is to enable your project's tests to run on all the platforms Swift supports without having to rewrite them.\n\n## Using XCTest\n\nYour tests are organized into a simple hierarchy. Each `XCTestCase` subclass has a set of `test` methods, each of which should test one part of your code.\n\nFor general information about using XCTest, see:\n\n* [Testing with Xcode](https://developer.apple.com/library/mac/documentation/DeveloperTools/Conceptual/testing_with_xcode/chapters/03-testing_basics.html)\n* [XCTest API documentation](https://developer.apple.com/documentation/xctest)\n\n### Using XCTest with Swift Package Manager\n\nThe Swift Package Manager integrates directly with XCTest to provide a streamlined experience for unit testing SwiftPM packages. If you are using XCTest within a SwiftPM package, unit test files are located within the package's `Tests` subdirectory, and you can build and run the full test suite in one step by running `swift test`.\n\nFor more information about using XCTest with SwiftPM, see its [documentation](https://github.com/apple/swift-package-manager).\n\n### Test Method Discovery\n\nUnlike the version of XCTest included with Xcode, this version does not use the Objective-C runtime to automatically discover test methods because that runtime is not available on all platforms Swift supports. This means that in certain configurations, the full set of test methods must be explicitly provided to XCTest.\n\nWhen using XCTest via SwiftPM on macOS, this is not necessary because SwiftPM uses the version of XCTest included with Xcode to run tests. But when using this version of XCTest _without_ SwiftPM, or _with_ SwiftPM on a platform other than macOS (including Linux), the full set of test methods cannot be discovered automatically, and your test target must tell XCTest about them explicitly.\n\nThe recommended way to do this is to create a static property in each of your `XCTestCase` subclasses. By convention, this property is named `allTests`, and should contain all of the tests in the class. For example:\n\n```swift\nclass TestNSURL : XCTestCase {\n    static var allTests = {\n        return [\n            (\"test_bestNumber\", test_bestNumber),\n            (\"test_URLStrings\", test_URLStrings),\n            (\"test_fileURLWithPath\", test_fileURLWithPath),\n            // Other tests go here\n        ]\n    }()\n\n    func test_bestNumber() {\n        // Write your test here. Most of the XCTAssert functions you are familiar with are available.\n        XCTAssertTrue(theBestNumber == 42, \"The number is wrong\")\n    }\n\n    // Other tests go here\n}\n```\n\nAfter creating an `allTests` property in each `XCTestCase` subclass, you must tell XCTest about those classes' tests.\n\nIf the project is a SwiftPM package which supports macOS, the easiest way to do this is to run `swift test --generate-linuxmain` from a macOS machine. This command generates files within the package's `Tests` subdirectory which contains the necessary source code for passing all test classes and methods to XCTest. These files should be committed to source control and re-generated whenever `XCTestCase` subclasses or test methods are added to or removed from your package's test suite.\n\nIf the project is a SwiftPM package but does not support macOS, you may edit the package's default  `LinuxMain.swift` file manually to add all `XCTestCase` subclasses.\n\nIf the project is not a SwiftPM package, follow the steps in the next section to create an executable which calls the `XCTMain` function manually.\n\n### Standalone Command Line Usage\n\nWhen used by itself, without SwiftPM, this version of XCTest does not use the external `xctest` CLI test runner included with Xcode to run tests. Instead, you must create your own executable which links `libXCTest.so`, and in your `main.swift`, invoke the `XCTMain` function with an array of the tests from all `XCTestCase` subclasses that you wish to run, wrapped by the `testCase` helper function. For example:\n\n```swift\nXCTMain([\n    testCase(TestNSString.allTests),\n    testCase(TestNSArray.allTests),\n    testCase(TestNSDictionary.allTests),\n])\n```\n\nThe `XCTMain` function does not return, and will cause your test executable to exit with either `0` for success or `1` for failure. Certain command line arguments can be used to modify the test runner behavior:\n\n* A particular test or test case can be selected to execute. For example:\n\n```\n$ ./FooTests Tests.FooTestCase/testFoo                            # Run a single test method\n$ ./FooTests Tests.FooTestCase                                    # Run all the tests in FooTestCase\n$ ./FooTests Tests.FooTestCase/testFoo,Tests.FooTestCase/testBar  # Run multiple test methods\n```\n* Tests can be listed, instead of executed.\n\n```\n$ ./FooTests --list-tests\nListing 4 tests in FooTests.xctest:\n\nTests.FooTestCase/testFoo\nTests.FooTestCase/testBar\nTests.BarTestCase/test123\n\n$ ./FooTests --dump-tests-json\n{\"tests\":[{\"tests\":[{\"tests\":[{\"name\":\"testFoo\"},{\"name\":\"testBar\"}],\"name\":\"Tests.FooTestCase\"},{\"tests\":[{\"name\":\"test123\"}],\"name\":\"Tests.BarTestCase\"}],\"name\":\"Tests.xctest\"}],\"name\":\"All tests\"}\n```\n\n## Contributing to XCTest\n\nTo contribute, you'll need to be able to build this project and and run its test suite. The easiest way to do so is via the Swift build script.\n\nFirst, follow [the instructions in the Swift README](https://github.com/apple/swift/blob/main/README.md) to build Swift from source. Confirm you're able to build the Swift project using `utils/build-script -R`.\n\nOnce you are able to build the Swift project, build XCTest and run its tests:\n\n```\n$ cd swift-corelibs-xctest\n$ ../swift/utils/build-script --preset corelibs-xctest\n```\n\nThis project is only guaranteed to build with the very latest commit on the Swift and swift-corelibs-foundation `main` branches. You may update to the latest commits using the Swift `utils/update-checkout` script:\n\n```\n$ ../swift/utils/update-checkout\n```\n\n### Using Xcode\n\nTo browse files in this project using Xcode, use `XCTest.xcworkspace`. You may build the project using the `SwiftXCTest` scheme. Run the `SwiftXCTestFunctionalTests` scheme to run the tests.\n\nHowever, in order to successfully build the project in Xcode, **you must use an Xcode toolchain with an extremely recent version of Swift**. The Swift website provides [Xcode toolchains to download](https://swift.org/download/#latest-development-snapshots), as well as [instructions on how to use Xcode with those toolchains](https://swift.org/download/#apple-platforms). Swift development moves fairly quickly, and so even a week-old toolchain may no longer work.\n\n> If none of the toolchains available to download are recent enough to build XCTest, you may build your own toolchain by using the [`utils/build-toolchain` script](https://github.com/apple/swift/blob/main/utils/build-toolchain) in the Swift repository.\n>\n> Keep in mind that the build script invocation in \"Contributing to XCTest\" above will always work, regardless of which Swift toolchains you have installed. The Xcode workspace exists simply for the convenience of contributors. It is not necessary to successfully build this project in Xcode in order to contribute.\n"
 },
 {
  "repo": "apple/swift-experimental-string-processing",
  "language": "Swift",
  "readme_contents": "# Declarative String Processing for Swift\n\nAn early experimental general-purpose pattern matching engine for Swift.\n\nSee [Declarative String Processing Overview][decl-string]\n\n[decl-string]: Documentation/DeclarativeStringProcessing.md\n\n## Requirements\n\n- [Swift Trunk Development Snapshot](https://www.swift.org/download/#snapshots) DEVELOPMENT-SNAPSHOT-2022-02-03 or later.\n"
 },
 {
  "repo": "apple/swift-docker",
  "language": "Dockerfile",
  "readme_contents": "# swift-docker\n\n<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n\n### Docker images for [Swift](https://swift.org).\n\n#### You can find the Docker Hub repo here: [https://hub.docker.com/_/swift/](https://hub.docker.com/_/swift/)\n\n\n### Usage\n\n##### Pull the Docker image from Docker Hub:\n\n```bash\ndocker pull swift\n```\n\n##### Create a container from the image and run it:\n\n```bash\ndocker run -it swift /bin/bash\n```\n\nIf you want to run the Swift REPL you will need to run the container with additional privileges:\n\n```bash\n# If running Docker on Linux:\ndocker run --security-opt seccomp=unconfined -it swift\n\n# If running Docker on macOS:\ndocker run --privileged -it swift\n```\n\nWe also provide a \"slim\" image. Slim images are images designed just for running an already built Swift program. Consequently, they do not contain the Swift compiler.\n\nThe normal and slim images can be combined via a multi-stage Dockerfile to produce a lighter-weight image ready for deployment. For example:\n\n```dockerfile\nFROM swift:latest as builder\nWORKDIR /root\nCOPY . .\nRUN swift build -c release\n\nFROM swift:slim\nWORKDIR /root\nCOPY --from=builder /root .\nCMD [\".build/release/docker-test\"]\n```\n\n## Contributions\n\nContributions via pull requests are welcome and encouraged :)\n\n## License\n\ndocker-swift is licensed under the [Apache License, Version 2.0](LICENSE.md).\n"
 },
 {
  "repo": "apple/swift-installer-scripts",
  "language": "C++",
  "readme_contents": "# Swift Installer Scripts\n\nThis repository contains all the supporting files required for building\ntoolchain packages for the Swift toolchain for distribution.\n\nThis repository does not contain the actual contents of the toolchain. These\nfiles are used to construct the packaged forms of the toolchain to layout the\ntoolchain properly on the destination system.\n\n## Organization\n\nBecause the repository hosts the packaging support content for multiple\nplatforms, the following structure allows all the platforms to colocate\nin the same repository without colliding with each other:\n\n~~~\nswift-installer-scripts\n  \u2514 platforms\n        \u251c Linux\n        \u2502   \u251c README\n        \u2502   \u251c DEB\n        \u2502   \u2502   \u251c README\n        \u2502   \u2502   \u251c Docs - contains information specific to .deb packages\n        \u2502   \u2502   \u251c Debian - contains information specific to the distribution\n        \u2502   \u2502   \u2502   \u251c buster - contains information specific to the version\n        \u2502   \u2502   \u2502   \u251c bullseye\n        \u2502   \u2502   \u2502   \u2514 ...\n        \u2502   \u2502   \u251c Ubuntu\n        \u2502   \u2502   \u2502   \u251c bionic\n        \u2502   \u2502   \u2502   \u251c focal\n        \u2502   \u2502   \u2502   \u251c hirsute\n        \u2502   \u2502   \u2502   \u2514 ...\n        \u2502   \u2502   \u2514 ...\n        \u2502   \u2502\n        \u2502   \u251c RPM\n        \u2502   \u2502   \u251c README\n        \u2502   \u2502   \u251c Docs - contains information specific to .rpm packages\n        \u2502   \u2502   \u251c Amazonlinux\n        \u2502   \u2502   \u2502   \u251c 2\n        \u2502   \u2502   \u2502   \u2514 ...\n        \u2502   \u2502   \u251c Centos\n        \u2502   \u2502   \u2502   \u251c 7\n        \u2502   \u2502   \u2502   \u251c 8\n        \u2502   \u2502   \u2502   \u2514 ...\n        \u2502   \u2502   \u251c Fedora\n        \u2502   \u2502   \u2502   \u251c 33\n        \u2502   \u2502   \u2502   \u251c 34\n        \u2502   \u2502   \u2502   \u251c rawhide\n        \u2502   \u2502   \u2502   \u2514 ...\n        \u2502   \u2502   \u2514 ...\n        \u2502   \u2514 ...\n        \u2514 Windows\n            \u2514 ...\n~~~\n\n## Linux Packages (RPM/Deb)\n\nCurrently Swift on Linux is distributed via tarball and Docker, and\nwe would like to start supporting RPM and Debs officially on swift.org.\nThe goal is to provide a seamless install process for Swift on Linux by\nutilizing the platform\u2019s native package manager (RPM/Deb).\n\n\n* Step 1. Develop native packages / installers for the distributions\n* Step 2. Offer the native packages / installers through swift.org\n  * Support all officially supported Linux platforms\n  * Code signed by swift.org certificate\n  * Repository hosted on swift.org\n* Step 3. Offer the native packages / installer through official repositories\nfor the various platforms\n  * Work with official repositories to accept package specs\n  * Deprecate swift.org packages / installer repository\n* Step 4. Deprecate swift.org Linux tarballs\n\n### Package Info\n\n* Package name: swiftlang\n* License: Apache 2.0\n* Maintainer: swift-infrastructure@forums.swift.org\n* URL: https://swift.org\n* Description:\n```\nSwift is a general-purpose programming language built using\na modern approach to safety, performance, and software design\npatterns.\n\nThe goal of the Swift project is to create the best available\nlanguage for uses ranging from systems programming, to mobile\nand desktop apps, scaling up to cloud services. Most\nimportantly, Swift is designed to make writing and maintaining\ncorrect programs easier for the developer.\n```\n\n### RPM Naming Convention:\n\nPackage naming convention: `swiftlang-<VERSION>-<RELEASE>.<DIST>.<ARCH>.rpm`  \nPackage structure: `/repo/<OS>/releases/<OS_VERSION>/<ARCH>/`  \nRepository configuration: `/repo/<OS>/releases/<OS_VERSION>/swiftlang.repo`  \n\n\n#### Example\n\n* **Package structure:**  \n```\n/repo/centos/releases/8/x86_64/swiftlang-5.5.0-1.el8.x86_64.rpm\n/repo/centos/releases/8/aarch64/swiftlang-5.5.0-1.el8.aarch64.rpm\n```\n\n* **Package URL:**  \nhttps://download.swift.org/repo/centos/releases/8/aarch64/swiftlang-5.5.0-1.el8.aarch64.rpm\n\n* **Repository configuration file URL:**  \nhttps://download.swift.org/repo/centos/releases/8/swiftlang.repo\n\n## Tasks\n\n### RPM Package Manager (RPM)*\n\n- [ ] [SR-15325](https://bugs.swift.org/browse/SR-15325) Create RPM spec file\n- [ ] [SR-15326](https://bugs.swift.org/browse/SR-15326) Setup CI job to build the rpm package\n- [ ] [SR-15327](https://bugs.swift.org/browse/SR-15327) Code sign rpm package with swift.org certificate\n- [ ] [SR-15328](https://bugs.swift.org/browse/SR-15328) Host the rpm package on swift.org\n- [ ] [SR-15329](https://bugs.swift.org/browse/SR-15329) Host the rpm repository on swift.org\n- [ ] [SR-15330](https://bugs.swift.org/browse/SR-15330) Verify the rpm package and repository\n- [ ] [SR-15331](https://bugs.swift.org/browse/SR-15331) Update swift.org download / install page\n- [ ] [SR-15332](https://bugs.swift.org/browse/SR-15332) Work with official repositories to accept package specs\n\n*For each platform, we will start with CentOS 8.\n\n### Debian Package (Deb)*\n\n- [ ] [SR-15334](https://bugs.swift.org/browse/SR-15334) Create Debs control file\n- [ ] [SR-15335](https://bugs.swift.org/browse/SR-15335) Setup CI to build the deb package\n- [ ] [SR-15336](https://bugs.swift.org/browse/SR-15336) Code sign package with swift.org certificate\n- [ ] [SR-15337](https://bugs.swift.org/browse/SR-15337) Host the deb package on swift.org\n- [ ] [SR-15338](https://bugs.swift.org/browse/SR-15338) Host the deb repository on swift.org\n- [ ] [SR-15339](https://bugs.swift.org/browse/SR-15339) Verify the deb package and repository\n- [ ] [SR-15340](https://bugs.swift.org/browse/SR-15340) Update swift.org download / install page\n- [ ] [SR-15341](https://bugs.swift.org/browse/SR-15341) Work with official repositories to accept package control files\n\n*For each platform, we will start with Ubuntu 20.04\n\n## Open Questions\n\n* Where should swiftlang be installed on the system?\n\t* Option 1: Diverge the install location between platform to best fit the platform requirements.\n\t\t* symlink the toolchain into /usr/ to avoid conflicting with llvm.org binaries.\n\t* Option 2: Install in /usr and rename llvm-project binaries (example: swift-lldb/lldb-swift ...)\n\t* [GitHub discussion](https://github.com/apple/swift-installer-scripts/pull/37#discussion_r726707320)\n* Should we support multiple swiftlang versions on the system?\n* Multiple packages:\n\t* swiftlang\n\t* swiftlang-runtime\n\n## Contributing\n\nBefore contributing, please read [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## LICENSE\n\nSee [LICENSE](LICENSE.txt) for license information.\n\n## Code of Conduct\n\nSee [Swift.org Code of Conduct](https://swift.org/code-of-conduct/) for Code of Conduct information.\n"
 },
 {
  "repo": "apple/swift-llbuild2",
  "language": "Swift",
  "readme_contents": "# llbuild2\n\n**llbuild2** is an experimental, Swift native, fully async, NIO futures-based\nlow level build system. Started as the **cevobuild** experiment in [**llbuild**](https://github.com/apple/swift-llbuild),\nthis repository aims to continue that exploration.\n\nCheck the [docs](Docs/index.md) for more information about llbuild2.\n\n# Development\n\nDevelopment documentation is available [here](Docs/Development.md).\n\n## License\n\nCopyright (c) 2020 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee http://swift.org/LICENSE.txt for license information.\n\nSee http://swift.org/CONTRIBUTORS.txt for Swift project authors.\n"
 },
 {
  "repo": "apple/swift-docc-render",
  "language": "JavaScript",
  "readme_contents": "# Swift-DocC-Render\n\nSwift-DocC-Render is a web Single Page Application (SPA) powered by [Vue.js](https://vuejs.org/) for creating rich code documentation websites. Pages and content are generated using render JSON data from DocC. It comes with a well suited design for documentation websites.\nSPAs are web apps that render dynamically at runtime entirely in the browser, using JavaScript.\n\n[Swift-DocC](https://github.com/apple/swift-docc) is a tool for building and previewing documentation, aimed at making it easy to generate reference documentation from frameworks and packages, as well as free-form articles and tutorials authored in Markdown. It produces a folder in the Documentation Archive format (extension `.doccarchive`), which contains a machine-readable output of the documentation as JSON data that Swift-DocC-Render uses to produce rendered documentation.\n\n## Getting Started\n\n> Note: requires [Node.js](https://nodejs.org/en/download/) v14\n> and [npm](https://www.npmjs.com/package/npm) v6.14\n\n1. **Download this repository and go to its folder**\n\n    ```shell\n    git clone https://github.com/apple/swift-docc-render.git\n    cd swift-docc-render\n    ```\n\n2. **Install dependencies**\n\n    ```shell\n    npm install\n    ```\n\n3. **Run a local server with hot reload at [localhost:8080](http://localhost:8080/)**\n\n    You may want to set a proxy to handle data requests while developing locally by setting the VUE_APP_DEV_SERVER_PROXY env variable to a documentation archive (.doccarchive or .docc-build) on your disk or served in a HTTP endpoint:\n\n    ```shell\n    VUE_APP_DEV_SERVER_PROXY=[path to documentation archive] npm run serve\n    ```\n\n    As an alternative you can just create a `.env.development.local` file on the root of the project to add the `VUE_APP_DEV_SERVER_PROXY` env variable so you don't have to set it in the `npm run serve` script each time.\n\n## Using Swift-DocC-Render to render documentation\n\nFollow [these steps](https://github.com/apple/swift-docc#using-docc-to-build-and-preview-documentation) to generate a documentation archive, set the path to your renderer and render locally your documentation using Swift-DocC-Render.\n\n## Rendering and building docs\n\nYou need to have [DocC](https://swift.org/documentation/docc) installed, in order to preview and build documentation. Read [Getting Started with Swift](https://www.swift.org/getting-started/) to learn more.\n\nTo preview the docs, run `npm run docs:preview`. This will spawn a preview server on http://localhost:8000/documentation/doccrender.\n\nTo build the docs, run `npm run docs:build`.\n\n## Bug Reports and Feature Requests\n\n### Submitting a Bug Report\n\nSwift-DocC-Render tracks all bug reports with [Swift JIRA](https://bugs.swift.org/).\nWhen you submit a bug report we ask that you follow the\nSwift [Bug Reporting](https://swift.org/contributing/#reporting-bugs) guidelines\nand provide as many details as possible.\n\nIf you can confirm that the bug occurs when using the latest commit of Swift-DocC\nfrom the `main` branch (see [Building Swift-DocC-Render](/CONTRIBUTING.md#build-and-run-swift-docc-render)),\nthat will help us track down the bug faster.\n\n### Submitting a Feature Request\n\nFor feature requests, please feel free to create an issue\non [Swift JIRA](https://bugs.swift.org/) with the `New Feature` type\nor start a discussion on the [Swift Forums](https://forums.swift.org/c/development/swift-docc).\n\nDon't hesitate to submit a feature request if you see a way\nSwift-DocC-Render can be improved to better meet your needs.\n\nAll user-facing features must be discussed\nin the [Swift Forums](https://forums.swift.org/c/development/swift-docc)\nbefore being enabled by default.\n\n## Contributing to DocC\n\nPlease see the [contributing guide](/CONTRIBUTING.md) for more information.\n\n<!-- Copyright (c) 2021 Apple Inc and the Swift Project authors. All Rights Reserved. -->\n"
 },
 {
  "repo": "apple/swift-evolution",
  "language": "Markdown",
  "readme_contents": "# Swift Programming Language Evolution\n\n**Before you initiate a pull request**, please read the [process](process.md) document.\nIdeas should be thoroughly discussed on the [swift-evolution forums](https://swift.org/community/#swift-evolution) first.\n\nThis repository tracks the ongoing evolution of Swift. It contains:\n\n* The [status page](https://apple.github.io/swift-evolution/), tracking proposals to change Swift.\n* The [process](process.md) document that governs the evolution of Swift.\n* [Commonly Rejected Changes](commonly_proposed.md), proposals that have been denied in the past.\n\n## Goals and Release Notes\n\n* [On the road to Swift 6](https://forums.swift.org/t/on-the-road-to-swift-6/32862)\n* [CHANGELOG](https://github.com/apple/swift/blob/main/CHANGELOG.md)\n\n| Version   | Announced                                                                | Released                                                 |\n| :-------- | :----------------------------------------------------------------------- | :------------------------------------------------------- |\n| Swift 5.6 | [2021-11-10](https://forums.swift.org/t/swift-5-6-release-process/53412) |\n| Swift 5.5 | [2021-03-12](https://forums.swift.org/t/swift-5-5-release-process/45644) | [2021-09-20](https://swift.org/blog/swift-5-5-released/) |\n| Swift 5.4 | [2020-11-11](https://forums.swift.org/t/swift-5-4-release-process/41936) | [2021-04-26](https://swift.org/blog/swift-5-4-released/) |\n| Swift 5.3 | [2020-03-25](https://swift.org/blog/5-3-release-process/)                | [2020-09-16](https://swift.org/blog/swift-5-3-released/) |\n| Swift 5.2 | [2019-09-24](https://swift.org/blog/5-2-release-process/)                | [2020-03-24](https://swift.org/blog/swift-5-2-released/) |\n| Swift 5.1 | [2019-02-18](https://swift.org/blog/5-1-release-process/)                | [2019-09-20](https://swift.org/blog/swift-5-1-released/) |\n| Swift 5.0 | [2018-09-25](https://swift.org/blog/5-0-release-process/)                | [2019-03-25](https://swift.org/blog/swift-5-released/)   |\n| Swift 4.2 | [2018-02-28](https://swift.org/blog/4-2-release-process/)                | [2018-09-17](https://swift.org/blog/swift-4-2-released/) |\n| Swift 4.1 | [2017-10-17](https://swift.org/blog/swift-4-1-release-process/)          | [2018-03-29](https://swift.org/blog/swift-4-1-released/) |\n| Swift 4.0 | [2017-02-16](https://swift.org/blog/swift-4-0-release-process/)          | [2017-09-19](https://swift.org/blog/swift-4-0-released/) |\n| Swift 3.1 | [2016-12-09](https://swift.org/blog/swift-3-1-release-process/)          | [2017-03-27](https://swift.org/blog/swift-3-1-released/) |\n| Swift 3.0 | [2016-05-06](https://swift.org/blog/swift-3-0-release-process/)          | [2016-09-13](https://swift.org/blog/swift-3-0-released/) |\n| Swift 2.2 | [2016-01-05](https://swift.org/blog/swift-2-2-release-process/)          | [2016-03-21](https://swift.org/blog/swift-2-2-released/) |\n"
 },
 {
  "repo": "apple/servicetalk",
  "language": "Java",
  "readme_contents": "= ServiceTalk\n\nimage:https://img.shields.io/maven-central/v/io.servicetalk/servicetalk-annotations?color=blue[Maven Central]\nimage:https://github.com/apple/servicetalk/actions/workflows/ci-snapshot.yml/badge.svg[Snapshot Publisher]\nimage:https://img.shields.io/nexus/s/io.servicetalk/servicetalk-annotations?color=blue&server=https%3A%2F%2Foss.sonatype.org[Sonatype Snapshot]\n\nServiceTalk is a JVM network application framework with APIs tailored to specific protocols (e.g. HTTP/1.x,\nHTTP/2.x, etc...) and supports multiple programming paradigms.\n\nIt is built on link:https://netty.io[Netty] and is designed to provide most of the performance/scalability benefits of\nNetty for common networking protocols used in service to service communication. ServiceTalk provides server support and\n\"smart client\" like features such as client-side load balancing and service discovery integration.\n\nSee the link:https://docs.servicetalk.io/[ServiceTalk docs] for more information.\n\n== Getting Started\n\nServiceTalk releases are available on link:https://repo1.maven.org/maven2/io/servicetalk/[Maven Central].\n\nRefer to the link:https://docs.servicetalk.io/[ServiceTalk docs] for various examples that will get you started with the\ndifferent features of ServiceTalk.\n\nNOTE: Builds of the development version are available in\nlink:https://oss.sonatype.org/content/repositories/snapshots/io/servicetalk/[Sonatype's snapshots Maven repository].\n\n== Supported JVM\nThe minimum supported JDK version is 1.8.\n\n== Compatibility\nServiceTalk follows link:https://semver.org/#semantic-versioning-200[SemVer 2.0.0]. API/ABI breaking changes will\nrequire package renaming for that module to avoid runtime classpath conflicts.\n\nNOTE: `0.x.y` releases are not stable and are permitted to break API/ABI.\n\n== Contributor Setup\n\nIMPORTANT: If you're intending to contribute to ServiceTalk,\n           make sure to first read the xref:CONTRIBUTING.adoc[contribution guidelines].\n\nServiceTalk uses link:https://gradle.org[Gradle] as its build tool and only requires JDK 8 or higher to be\npre-installed. ServiceTalk ships with the Gradle Wrapper, which means that there is no need to install Gradle on your\nmachine beforehand.\n\n=== File Encoding\n\nServiceTalk's source code is UTF-8 encoded: make sure your filesystem supports it before attempting to build\nthe project. Setting the `JAVA_TOOL_OPTIONS` env var to `-Dfile.encoding=UTF-8` should help building the project in\nnon-UTF-8 environments. Editors and IDEs must also support UTF-8 in order to successfully edit ServiceTalk's source\ncode.\n\n=== Build Commands\n\nYou should be able to run the following command to build ServiceTalk and verify that all\ntests and code quality checks pass:\n\n[source,shell]\n----\n$ ./gradlew build\n----\n\nThe supported IDE is link:https://www.jetbrains.com/idea[IntelliJ IDEA].\nIn order to generate IntelliJ IDEA project files for ServiceTalk,\nyou can run the following command:\n\n[source,shell]\n----\n$ ./gradlew idea\n----\n\nWhen done, running one of following commands would open ServiceTalk in IntelliJ:\n\n.Generic\n[source,shell]\n----\n$ idea .\n----\n\n.macOS\n[source,shell]\n----\n$ open servicetalk.ipr\n----\n\n== Project Communication\nWe encourage your participation asking questions and helping improve the ServiceTalk project.\nlink:https://github.com/apple/servicetalk/issues[Github issues] and\nlink:https://github.com/apple/servicetalk/pulls[pull requests] are the primary mechanisms of\nparticipation and communication for ServiceTalk.\n"
 },
 {
  "repo": "apple/swift-nio-ssh",
  "language": "Swift",
  "readme_contents": "# SwiftNIO SSH\n\nThis project contains SSH support using [SwiftNIO](https://github.com/apple/swift-nio).\n\n## What is SwiftNIO SSH?\n\nSwiftNIO SSH is a programmatic implementation of SSH: that is, it is a collection of APIs that allow programmers to implement SSH-speaking endpoints. Critically, this means it is more like libssh2 than openssh. SwiftNIO SSH does not ship production-ready SSH clients and servers, but instead provides the building blocks for building this kind of client and server.\n\nThere are a number of reasons to provide a programmatic SSH implementation. One is that SSH has a unique relationship to user interactivity. Technical users are highly accustomed to interacting with SSH interactively, either to run commands on remote machines or to run interactive shells. Having the ability to programmatically respond to these requests enables interesting alternative modes of interaction. As prior examples, we can point to Twisted's Manhole, which uses [a programmatic SSH implementation called `conch`](https://twistedmatrix.com/trac/wiki/TwistedConch) to provide an interactive Python interpreter within a running Python server, or [ssh-chat](https://github.com/shazow/ssh-chat), a SSH server that provides a chat room instead of regular SSH shell functionality. Innovative uses can also be imagined for TCP forwarding.\n\nAnother good reason to provide programmatic SSH is that it is not uncommon for services to need to interact with other services in a way that involves running commands. While `Process` solves this for the local use-case, sometimes the commands that need to be invoked are remote. While `Process` could launch an `ssh` client as a sub-process in order to run this invocation, it can be substantially more straightforward to simply invoke SSH directly. This is [`libssh2`](https://www.libssh2.org)'s target use-case. SwiftNIO SSH provides the equivalent of the networking and cryptographic layer of libssh2, allowing motivated users to drive SSH sessions directly from within Swift services.\n\nSwiftNIO SSH requires Swift 5.2 and newer. Older versions (0.2.x and above) support Swift 5.1.\n\n## What does SwiftNIO SSH support?\n\nSwiftNIO SSH supports SSHv2 with the following feature set:\n\n- All session channel features, including shell and exec channel requests\n- Direct and reverse TCP port forwarding\n- Modern cryptographic primitives only: Ed25519 and ECDSA over the major NIST curves (P256, P384, P521) for asymmetric cryptography, AES-GCM for symmetric cryptography, x25519 for key exchange\n- Password and public key user authentication\n- Supports all platforms supported by SwiftNIO and Swift Crypto\n\n## How do I use SwiftNIO SSH?\n\nSwiftNIO SSH provides a SwiftNIO `ChannelHandler`, `NIOSSHHandler`. This handler implements the bulk of the SSH protocol directly. Users are not expected to generate SSH messages directly: instead, they interact with the `NIOSSHHandler` through child channels and delegates.\n\nSSH is a multiplexed protocol: each SSH connection is subdivided into multiple bidirectional communication channels called, appropriately enough, channels. SwiftNIO SSH reflects this construction by using a \"child channel\" abstraction. When a peer creates a new SSH channel, SwiftNIO SSH will create a new NIO `Channel` that is used to represent all traffic on that SSH channel. Within this child `Channel` all events are strictly ordered with respect to one another: however, events in different `Channel`s may be interleaved freely by the implementation.\n\nAn active SSH connection therefore looks like this:\n\n```\n\u250c \u2500 NIO Channel \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2510\n\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n      \u2502                                \u2502\n\u2502     \u2502                                \u2502    \u2502\n      \u2502                                \u2502\n\u2502     \u2502                                \u2502    \u2502\n      \u2502         NIOSSHHandler          \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \u2502                                \u2502    \u2502                  \u2502\n      \u2502                                \u2502                       \u2502\n\u2502     \u2502                                \u2502    \u2502                  \u2502\n      \u2502                                \u2502                       \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                  \u2502\n                                                               \u2502\n\u2514 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2518                  \u2502\n                                                               \u2502\n                                                               \u2502\n                                                               \u2502\n                                                               \u2502\n                                                               \u25bc\n                     \u250c\u2500\u2500 SSH Child Channel \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502                                                                                  \u2502\n                     \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u251c\u2500\u2500\u2500\u2510\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u251c\u2500\u2500\u2500\u2510\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502          User Handler          \u2502      \u2502          User Handler          \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2502                                \u2502      \u2502                                \u2502     \u2502   \u2502   \u2502\n                     \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502   \u2502   \u2502\n                     \u2502                                                                                  \u2502   \u2502   \u2502\n                     \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502\n                         \u2502                                                                                  \u2502   \u2502\n                         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                             \u2502                                                                                  \u2502\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAn SSH channel is invoked with a channel type. NIOSSH supports three: `session`, `directTCPIP`, and `forwardedTCPIP`. The most common channel type is `session`: `session` is used to represent the invocation of a program, whether a specific named program or a shell. The other two channel types are related to TCP port forwarding, and will be discussed later.\n\nAn SSH channel operates on a single data type: `SSHChannelData`. This structure encapsulates the fact that SSH supports both regular and \"extended\" channel data. The regular channel data (`SSHChannelData.DataType.channel`) is used for the vast majority of core data. In `session` channels the `.channel` data type is used for standard input and standard output: the `.stdErr` data type is used for standard error (naturally). In TCP forwarding channels, the `.channel` data type is the only kind used, and represents the forwarded data.\n\n### Channel Events\n\nA `session` channel represents an invocation of a command. Exactly how the channel operates is communicated in a number of inbound user events. The following events are important:\n\n- `SSHChannelRequestEvent.PseudoTerminalRequest`: Requests the allocation of a pseudo-terminal.\n- `SSHChannelRequestEvent.EnvironmentRequest`: Requests a single environment variable for the command invocation. Always sent before the command itself.\n- `SSHChannelRequestEvent.ShellRequest`: Requests that the command to be invoked is the authenticated user's shell.\n- `SSHChannelRequestEvent.ExecRequest`: Requests the invocation of a specific command.\n- `SSHChannelRequestEvent.ExitStatus`: Used to signal that the remote command has exited, and communicates the exit code.\n- `SSHChannelRequestEvent.ExitSignal`: Used to indicate that the remote command was terminated in response to a signal, and what that signal was.\n- `SSHChannelRequestEvent.SignalRequest`:  Used to send a signal to the remote command.\n- `SSHChannelRequestEvent.LocalFlowControlRequest`: Used to indicate whether the client is capable of performing Ctrl-Q/Ctrl-S flow control itself.\n- `SSHChannelRequestEvent.WindowChangeRequest`: Used to communicate a change in the size of the terminal window on the client to the allocated peudo-terminal.\n- `SSHChannelRequestEvent.SubsystemRequest`: Used to request invocation of a specific subsystem. The meaning of this is specific to individual use-cases.\n\nThese events are unused in port forwarding messages. SSH implementations that support `.session` type channels need to be prepared to handle most or all of these in various ways.\n\nEach of these events also has a `wantReply` field. This indicates whether the request need a reply to indicate success or failure. If it does, the following two events are used:\n\n- `ChannelSuccessEvent`, to communicate success.\n- `ChannelFailureEvent`, to communicate failure.\n\n### Half Closure\n\nThe SSH network protocol pervasively uses half-closure in the child channels. NIO `Channel`s typically have half-closure support disabled by default, and SwiftNIO SSH respects this default in its child channels as well. However, if you leave this setting at its default value the SSH child channels will behave extremely unexpectedly. For this reason, it is strongly recommended that all child channels have half closure support enabled:\n\n```swift\nchannel.setOption(ChannelOptions.allowRemoteHalfClosure, true)\n```\n\nThis then uses standard NIO half-closure support. The remote peer sending EOF will be communicated with an inbound user event, `ChannelEvent.inputClosed`. To send EOF yourself, call `close(mode: .output)`.\n\n### User Authentication\n\nUser authentication is a vital part of SSH. To manage it, SwiftNIO SSH uses a pair of delegate protocols: `NIOSSHClientUserAuthenticationDelegate` and `NIOSSHServerUserAuthenticationDelegate`. Clients and servers should provide implementations of these delegate protocols to manage user authentication.\n\nThe client protocol is straightforward: SwiftNIO SSH will invoke the method `nextAuthenticationType(availableMethods:nextChallengePromise:)` on the delegate. The `availableMethods` will be an instance of `NIOSSHAvailableUserAuthenticationMethods` communicating which authentication methods the server has suggested will be acceptable. The delegate can then complete `nextChallengePromise` with either a new authentication request, or with `nil` to indicate that the client has run out of things to try.\n\nThe server protocol is more complex. The delegate must provide a `supportedAuthenticationMethods` property that communicates which authentication methods are supported by the delegate. Then, each time the client sends a user auth request, the `requestReceived(request:responsePromise:)` method will be invoked. This may be invoked multiple times in parallel, as clients are allowed to issue auth requests in parallel. The `responsePromise` should be succeeded with the result of the authentication. There are three results: `.success` and `.failure` are straightforward, but in principle the server can require multiple challenges using `.partialSuccess(remainingMethods:)`.\n\n### Direct Port Forwarding\n\nDirect port forwarding is port forwarding from client to server. In this mode traditionally the client will listen on a local port, and will forward inbound connections to the server. It will ask that the server forward these connections as outbound connections to a specific host and port.\n\nThese channels can be directly opened by clients by using the `.directTCPIP` channel type.\n\n### Remote Port Forwarding and Global Requests\n\nRemote port forwarding is a less-common situation where the client asks the server to listen on a specific address and port, and to forward all inbound connections to the client. As the client needs to request this behaviour, it does so using global requests.\n\nGlobal requests are initiated using `NIOSSHHandler.sendGlobalRequest`, and are received and handled by way of a `GlobalRequestDelegate`. There are two global requests supported today:\n\n- `GlobalRequest.TCPForwardingRequest.listen(host:port:)`: a request for the server to listen on a given host and port.\n- `GlobalRequest.TCPForwardingRequest.cancel(host:port:)`: a request to cancel the listening on the given host and port.\n\nServers may be notified of and respond to these requests using a `GlobalRequestDelegate`. The method to implement here is `tcpForwardingRequest(_:handler:promise:)`. This delegate method will be invoked any time a global request is received. The response to the request is passed into `promise`.\n\nForwarded channels are then sent from server to client using the `.forwardedTCPIP` channel type.\n"
 },
 {
  "repo": "apple/password-manager-resources",
  "language": "JavaScript",
  "readme_contents": "# Password Manager Resources\n\n## Welcome!\n\nThe _Password Manager Resources_ project exists so creators of password managers can collaborate on resources to make password management better for users. Resources currently consist of data, or \"quirks\", as well as code.\n\n\"Quirk\" is a term from web browser development that refers to a website-specific, hard-coded behavior to work around an issue with a website that can't be fixed in a principled, universal way. In this project, it has the same meaning. Although ideally, the industry will work to eliminate the need for all of the quirks in this project, there's value in customizing behaviors to ensure better user experience. The current quirks are:\n\n* [**Password Rules**](#password-rules): Rules to generate compatible passwords with websites' particular requirements.\n* [**Shared Credentials**](#shared-credentials): Groups of websites known to use the same credential backend, which can be used to enhance suggested credentials to sign in to websites.\n* [**Change Password URLs**](#change-password-urls): To drive the adoption of strong passwords, it's useful to be able to take users directly to websites' change password pages.\n* [**Websites Where 2FA Code is Appended to Password**](#websites-where-2fa-code-is-appended-to-password): Some websites use a two-factor authentication scheme where the user must append a generated code to their password when signing in.\n\nHaving password managers collaborate on these resources has three high-level benefits:\n\n1. By sharing resources, all password managers can improve their quality with less work than it'd take for any individual password manager to achieve the same effect.\n1. By publicly documenting website-specific behaviors, password managers can offer an incentive for websites to use standards or emerging standards to improve their compatibility with password managers; it's no fun to be called out on a list!\n1. By improving the quality of password managers, we improve user trust in them as a concept, which benefits everyone.\n\nWe encourage you to incorporate the data from this project into your password manager, but kindly ask that you please contribute any quirks you have back to the project so that all users of participating password managers can benefit from your discoveries and testing.\n\n## The Resources, In Detail\n\n### Password Rules\n\nMany password managers generate strong, unique passwords for people so that they aren't tempted to create their passwords by hand, which leads to easily guessed and reused passwords. Every time a password manager generates a password that isn't compatible with a website, a person not only has a bad experience but a reason to be tempted to create their password. Compiling password rule quirks helps fewer people run into issues like these while also documenting that a service's password policy is too restrictive for people using password managers, which may incentivize the services to change.\n\nThe file [`quirks/password-rules.json`](quirks/password-rules.json) contains a JSON object mapping domains to known good password rules for generating compatible passwords for use on that website. The [Password Rules language](https://developer.apple.com/password-rules/) is a human- and machine-readable way to concisely write and read the rules to generate a compatible password on a website. [`quirks/password-rules.json`](quirks/password-rules.json) is the quirks version of the [`passwordRules` attribute](https://github.com/whatwg/html/issues/3518), which is currently an open WHATWG proposal and supported in Safari. The same language is part of [native iOS application development API](https://developer.apple.com/documentation/security/password_autofill/customizing_password_autofill_rules). If a website changes its password requirements to be general enough to not warrant quirks, or if it adopts the `passwordRules` attribute to accurately communicate its requirements to password managers and web browsers, it should be removed from this list.\n\nWhen a domain is listed in [`quirks/password-rules.json`](quirks/password-rules.json), it means that that domain and all of its subdomains use the rule. For example, a rule for `example.com` will match URLs on `example.com` as well as `*.example.com`. A rule for `a.example.com` will match URLs on `a.example.com` as well as `*.a.example.com`, but will not match other subdomains of `example.com` such as `b.example.com`.\n\nA rule that should only be applied to the exact domain stated as a key should have the `exact-domain-match-only` key set to a value of `true`. The absence of the `exact-domain-match-only` key means that it is false.\n\n### Password Rules Language Parser\n\nAn implementation of a parser for the Password Rules language that's written in JavaScript can be found in [`tools/PasswordRulesParser.js`](tools/PasswordRulesParser.js). It can be used as a reference implementation, interpreted in build systems to convert `data/password-rules.json` to an application-specific format, or interpreted at application runtime wherever it's possible to execute JavaScript (e.g. using the JavaScriptCore framework on Apple platforms).\n\nA [third-party parser implementation](https://github.com/1Password/password-rules-parser) that's written in Rust is also available.\n\n### Shared Credentials\n\nThe files [`quirks/shared-credentials.json`](quirks/shared-credentials.json) and [`quirks/shared-credentials-historical.json`](quirks/shared-credentials-historical.json) express relationships between groups of websites that share credentials. The `-historical` file describes such relationships that were valid in the past but either are not valid today or we don't have a high degree of confidence are valid today.\n\nInformation in [`quirks/shared-credentials.json`](quirks/shared-credentials.json) can be used by password managers to offer contextually relevant accounts to users on `first.website`, even if credentials were previously saved for `second.website`. This list should not be used as part of any user experience that releases user credentials to a website without the user's explicit review and consent. In general, saved credentials should only be suggested to users with site-bound scoping. This list is appropriate for allowing a credential saved for website A to appear on website B if the website the credential was saved for is clearly stated.\n\nThere are existing proposals to allow different domains to declare an affiliation with each other, which could be a way for websites to solve this problem themselves, given browser and password manager adoption of such a proposal. Until and perhaps beyond then, it is useful to have these groupings of websites to make password filling suggestions more useful.\n\nInformation in [`quirks/shared-credentials-historical.json`](quirks/shared-credentials-historical.json) can be used by password managers to suppress password reuse warnings across websites, given that website A and website B once were known to share credentials in the past.\n\nThe [Contributing](CONTRIBUTING.md) document goes into detail on the format of these files.\n\n### Change Password URLs\n\nThe file [`quirks/change-password-URLs.json`](quirks/change-password-URLs.json) contains a JSON object mapping domains to URLs where users can change their password. This is the quirks version of the [Well Known URL for Changing Passwords](https://github.com/w3c/webappsec-change-password-url). If a website adopts the Change Password URL, it should be removed from this list.\n\n### Websites Where 2FA Code is Appended to Password\n\nThe file [`quirks/websites-that-append-2fa-to-password.json`](quirks/websites-that-append-2fa-to-password.json) contains a JSON array of domains which use a two-factor authentication scheme where the user must append a generated code to their password when signing in. This list of websites could be used to prevent auto-submission of signin forms, allowing the user to append the 2FA code without frustration. It can also be used to suppress prompting to update a saved password when the submitted password is prefixed by the already-stored password.\n\n## Contributing\n\nPlease review [how to contribute](CONTRIBUTING.md) if you would like to submit a pull request.\n\n## Asking Questions and Discussing Ideas\n\nIf you have any questions you'd like to ask publicly, or ideas you'd like to discuss, please [raise a GitHub issue](https://github.com/apple/password-manager-resources/issues) or send a message in the project's [Slack instance](https://pw-manager-resources.slack.com). Anyone participating in the project is welcome to join the Slack instance by [emailing the project's maintainers at Apple](mailto:password-manager-resources-maintainers@apple.com) and asking for an invitation. Please include your GitHub user name when you do this.\n\n## Project Maintenance\n\nProject maintenance involves, but is not limited to, adding clarity to incoming [issues](https://github.com/apple/password-manager-resources/issues) and reviewing pull requests. Project maintainers can approve and merge pull requests. Reviewing a pull request involves judging that a proposed contribution follows the project's guidelines, as described by the [guide to contributing](CONTRIBUTING.md). If you are interested in becoming a project maintainer, please [email the project maintainers at Apple](mailto:password-manager-resources-maintainers@apple.com) with the following information:\n\n* Your name\n* Your GitHub user name\n* Any organizations you're affiliated with that are related to password management, including professionally\n* Links to examples of pull requests submitted, review feedback given, and comments on issues that demonstrate why you'd be a good project maintainer\n\nIdeally, you'd provide somewhere between five and eight examples. The purpose of this note is to remind the Apple maintainers of who you are; ideally, before sending this message, we already know you from your great contributions!\n\nProject maintainers are expected to always follow the project's [Code of Conduct](CODE_OF_CONDUCT.md), and help to model it for others.\n\n## Project Governance\n\nAlthough we expect this to happen very infrequently, Apple reserves the right to make changes, including changes to data format and scope, to the project at any time.\n"
 },
 {
  "repo": "apple/swift-nio",
  "language": "Swift",
  "readme_contents": "[![sswg:graduated|104x20](https://img.shields.io/badge/sswg-graduated-green.svg)](https://github.com/swift-server/sswg/blob/main/process/incubation.md#graduated-level)\n\n# SwiftNIO\n\nSwiftNIO is a cross-platform asynchronous event-driven network application framework\nfor rapid development of maintainable high performance protocol servers & clients.\n\nIt's like [Netty](https://netty.io), but written for Swift.\n\n### Repository organization\n\nThe SwiftNIO project is split across multiple repositories:\n\nRepository | NIO 2 (Swift 5.2+)\n--- | ---\n[https://github.com/apple/swift-nio][repo-nio] <br> SwiftNIO core | `from: \"2.0.0\"`\n[https://github.com/apple/swift-nio-ssl][repo-nio-ssl] <br> TLS (SSL) support | `from: \"2.0.0\"`\n[https://github.com/apple/swift-nio-http2][repo-nio-http2]<br> HTTP/2 support | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-extras][repo-nio-extras] <br>useful additions around SwiftNIO | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-transport-services][repo-nio-transport-services] <br> first-class support for macOS, iOS, tvOS, and watchOS | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-ssh][repo-nio-ssh] <br> SSH support | `.upToNextMinor(from: \"0.2.0\")`\n\nNIO 2.29.0 and older support Swift 5.0+.\n\nWithin this repository we have a number of products that provide different functionality. This package contains the following products:\n\n- `NIO`. This is an umbrella module exporting `NIOCore`, `NIOEmbedded` and `NIOPosix`.\n- `NIOCore`. This provides the core abstractions and types for using SwiftNIO (see [\"Conceptual Overview\"](#conceptual-overview) for more details). Most NIO extension projects that provide things like new [`EventLoop`s][el] and [`Channel`s][c] or new protocol implementations should only need to depend on `NIOCore`.\n- `NIOPosix`. This provides the primary [`EventLoopGroup`], [`EventLoop`][el], and [`Channel`s][c] for use on POSIX-based systems. This is our high performance core I/O layer. In general, this should only be imported by projects that plan to do some actual I/O, such as high-level protocol implementations or applications.\n- `NIOEmbedded`. This provides [`EmbeddedChannel`][ec] and [`EmbeddedEventLoop`][eel], implementations of the `NIOCore` abstractions that provide fine-grained control over their execution. These are most often used for testing, but can also be used to drive protocol implementations in a way that is decoupled from networking altogether.\n- `NIOConcurrencyHelpers`. This provides a few low-level concurrency primitives that are used by NIO implementations, such as locks and atomics.\n- `NIOFoundationCompat`. This extends a number of NIO types for better interoperation with Foundation data types. If you are working with Foundation data types such as `Data`, you should import this.\n- `NIOTLS`. This provides a few common abstraction types for working with multiple TLS implementations. Note that this doesn't provide TLS itself: please investigate [swift-nio-ssl][repo-nio-ssl] and [swift-nio-transport-services][repo-nio-transport-services] for concrete implementations.\n- `NIOHTTP1`. This provides a low-level HTTP/1.1 protocol implementation.\n- `NIOWebSocket`. This provides a low-level WebSocket protocol implementation.\n- `NIOTestUtils`. This provides a number of helpers for testing projects that use SwiftNIO.\n\n### Protocol Implementations\n\nBelow you can find a list of a few protocol implementations that are done with SwiftNIO. This is a non-exhaustive list of protocols that are either part of the SwiftNIO project or are accepted into the [SSWG](https://swift.org/server)'s incubation process. All of the libraries listed below do all of their I/O in a non-blocking fashion using SwiftNIO.\n\n#### Low-level protocol implementations\n\nLow-level protocol implementations are often a collection of [`ChannelHandler`][ch]s that implement a protocol but still require the user to have a good understanding of SwiftNIO. Often, low-level protocol implementations will then be wrapped in high-level libraries with a nicer, more user-friendly API.\n\nProtocol | Client | Server | Repository | Module | Comment\n--- |  --- | --- | --- | --- | ---\nHTTP/1 | \u2705| \u2705 | [apple/swift-nio](https://github.com/apple/swift-nio) | [`NIOHTTP1`](https://apple.github.io/swift-nio/docs/current/NIOHTTP1/index.html) | official NIO project\nHTTP/2 | \u2705| \u2705 | [apple/swift-nio-http2](https://github.com/apple/swift-nio-http2) | [`NIOHTTP2`](https://apple.github.io/swift-nio-http2/docs/current/NIOHTTP2/index.html) | official NIO project\nWebSocket | \u2705| \u2705 | [apple/swift-nio](https://github.com/apple/swift-nio) | [`NIOWebSocket`](https://apple.github.io/swift-nio/docs/current/NIOWebSocket/index.html) | official NIO project\nTLS | \u2705 | \u2705 | [apple/swift-nio-ssl](https://github.com/apple/swift-nio-ssl) | [`NIOSSL`](https://apple.github.io/swift-nio-ssl/docs/current/NIOSSL/index.html) | official NIO project\nSSH | \u2705 | \u2705 | [apple/swift-nio-ssh][repo-nio-ssh] | _n/a_ | official NIO project\n\n\n#### High-level implementations\n\nHigh-level implementations are usually libraries that come with an API that doesn't expose SwiftNIO's [`ChannelPipeline`][cp] and can therefore be used with very little (or no) SwiftNIO-specific knowledge. The implementations listed below do still do all of their I/O in SwiftNIO and integrate really well with the SwiftNIO ecosystem.\n\nProtocol | Client | Server | Repository | Module | Comment\n--- |  --- | --- | --- | --- | ---\nHTTP | \u2705| \u274c | [swift-server/async-http-client](https://github.com/swift-server/async-http-client) | `AsyncHTTPClient` | SSWG community project\ngRPC | \u2705| \u2705 | [grpc/grpc-swift](https://github.com/grpc/grpc-swift) | `GRPC` | also offers a low-level API; SSWG community project\nAPNS | \u2705 | \u274c | [kylebrowning/APNSwift](https://github.com/kylebrowning/APNSwift) | `APNSwift` | SSWG community project\nPostgreSQL | \u2705 | \u274c | [vapor/postgres-nio](https://github.com/vapor/postgres-nio) | `PostgresNIO` | SSWG community project\nRedis | \u2705 | \u274c | [mordil/swift-redi-stack](https://gitlab.com/Mordil/swift-redi-stack) | `RediStack` | SSWG community project\n\n### Supported Versions\n\n### SwiftNIO 2\nThis is the current version of SwiftNIO and will be supported for the foreseeable future.\n\nThe latest released SwiftNIO 2 version\u00a0supports Swift 5.2+. NIO 2.29.0 and older support Swift 5.0+.\n\n### SwiftNIO 1\nSwiftNIO 1 is considered end of life - it is strongly recommended that you move to a newer version.  The Core NIO team does not actively work on this version.  No new features will be added to this version but PRs which fix bugs or security vulnerabilities will be accepted until the end of May 2022.\n\nIf you have a SwiftNIO 1 application or library that you would like to migrate to SwiftNIO 2, please check out the [migration guide](docs/migration-guide-NIO1-to-NIO2.md) we prepared for you.\n\nThe latest released SwiftNIO 1 version\u00a0supports Swift 4.0, 4.1, 4.2, and 5.0.\n\n### Supported Platforms\n\nSwiftNIO aims to support all of the platforms where Swift is supported. Currently, it is developed and tested on macOS and Linux, and is known to support the following operating system versions:\n\n* Ubuntu 18.04+\n* macOS 10.9+, iOS 7+; (macOS 10.14+, iOS 12+, tvOS 12+ or watchOS 6+ with [swift-nio-transport-services][repo-nio-transport-services])\n\n### Compatibility\n\nSwiftNIO follows [SemVer 2.0.0](https://semver.org/#semantic-versioning-200) with a separate document declaring [SwiftNIO's Public API](docs/public-api.md).\n\nWhat this means for you is that you should depend on SwiftNIO with a version range that covers everything from the minimum SwiftNIO version you require up to the next major version.\nIn SwiftPM that can be easily done specifying for example `from: \"2.0.0\"` meaning that you support SwiftNIO in every version starting from 2.0.0 up to (excluding) 3.0.0.\nSemVer and SwiftNIO's Public API guarantees should result in a working program without having to worry about testing every single version for compatibility.\n\n\n\n## Conceptual Overview\n\nSwiftNIO is fundamentally a low-level tool for building high-performance networking applications in Swift. It particularly targets those use-cases where using a \"thread-per-connection\" model of concurrency is inefficient or untenable. This is a common limitation when building servers that use a large number of relatively low-utilization connections, such as HTTP servers.\n\nTo achieve its goals SwiftNIO extensively uses \"non-blocking I/O\": hence the name! Non-blocking I/O differs from the more common blocking I/O model because the application does not wait for data to be sent to or received from the network: instead, SwiftNIO asks for the kernel to notify it when I/O operations can be performed without waiting.\n\nSwiftNIO does not aim to provide high-level solutions like, for example, web frameworks do. Instead, SwiftNIO is focused on providing the low-level building blocks for these higher-level applications. When it comes to building a web application, most users will not want to use SwiftNIO directly: instead, they'll want to use one of the many great web frameworks available in the Swift ecosystem. Those web frameworks, however, may choose to use SwiftNIO under the covers to provide their networking support.\n\nThe following sections will describe the low-level tools that SwiftNIO provides, and provide a quick overview of how to work with them. If you feel comfortable with these concepts, then you can skip right ahead to the other sections of this README.\n\n### Basic Architecture\n\nThe basic building blocks of SwiftNIO are the following 8 types of objects:\n\n- [`EventLoopGroup`][elg], a protocol, provided by `NIOCore`.\n- [`EventLoop`][el], a protocol, provided by `NIOCore`.\n- [`Channel`][c], a protocol, provided by `NIOCore`.\n- [`ChannelHandler`][ch], a protocol, provided by `NIOCore`.\n- `Bootstrap`, several related structures, provided by `NIOCore`.\n- [`ByteBuffer`][bb], a struct, provided by `NIOCore`.\n- [`EventLoopFuture`][elf], a generic class, provided by `NIOCore`.\n- [`EventLoopPromise`][elp], a generic struct, provided by `NIOCore`.\n\nAll SwiftNIO applications are ultimately constructed of these various components.\n\n#### EventLoops and EventLoopGroups\n\nThe basic I/O primitive of SwiftNIO is the event loop. The event loop is an object that waits for events (usually I/O related events, such as \"data received\") to happen and then fires some kind of callback when they do. In almost all SwiftNIO applications there will be relatively few event loops: usually only one or two per CPU core the application wants to use. Generally speaking event loops run for the entire lifetime of your application, spinning in an endless loop dispatching events.\n\nEvent loops are gathered together into event loop *groups*. These groups provide a mechanism to distribute work around the event loops. For example, when listening for inbound connections the listening socket will be registered on one event loop. However, we don't want all connections that are accepted on that listening socket to be registered with the same event loop, as that would potentially overload one event loop while leaving the others empty. For that reason, the event loop group provides the ability to spread load across multiple event loops.\n\nIn SwiftNIO today there is one [`EventLoopGroup`][elg] implementation, and two [`EventLoop`][el] implementations. For production applications there is the [`MultiThreadedEventLoopGroup`][mtelg], an [`EventLoopGroup`][elg] that creates a number of threads (using the POSIX [`pthreads`][pthreads] library) and places one `SelectableEventLoop` on each one. The `SelectableEventLoop` is an event loop that uses a selector (either [`kqueue`][kqueue] or [`epoll`][epoll] depending on the target system) to manage I/O events from file descriptors and to dispatch work. These [`EventLoop`s][el] and [`EventLoopGroup`s][elg] are provided by the `NIOPosix` module. Additionally, there is the [`EmbeddedEventLoop`][eel], which is a dummy event loop that is used primarily for testing purposes, provided by the `NIOEmbedded` module.\n\n[`EventLoop`][el]s have a number of important properties. Most vitally, they are the way all work gets done in SwiftNIO applications. In order to ensure thread-safety, any work that wants to be done on almost any of the other objects in SwiftNIO must be dispatched via an [`EventLoop`][el]. [`EventLoop`][el] objects own almost all the other objects in a SwiftNIO application, and understanding their execution model is critical for building high-performance SwiftNIO applications.\n\n#### Channels, Channel Handlers, Channel Pipelines, and Channel Contexts\n\nWhile [`EventLoop`][el]s are critical to the way SwiftNIO works, most users will not interact with them substantially beyond asking them to create [`EventLoopPromise`][elp]s and to schedule work. The parts of a SwiftNIO application most users will spend the most time interacting with are [`Channel`][c]s and [`ChannelHandler`][ch]s.\n\nAlmost every file descriptor that a user interacts with in a SwiftNIO program is associated with a single [`Channel`][c]. The [`Channel`][c] owns this file descriptor, and is responsible for managing its lifetime. It is also responsible for processing inbound and outbound events on that file descriptor: whenever the event loop has an event that corresponds to a file descriptor, it will notify the [`Channel`][c] that owns that file descriptor.\n\n[`Channel`][c]s by themselves, however, are not useful. After all, it is a rare application that doesn't want to do anything with the data it sends or receives on a socket! So the other important part of the [`Channel`][c] is the [`ChannelPipeline`][cp].\n\nA [`ChannelPipeline`][cp] is a sequence of objects, called [`ChannelHandler`][ch]s, that process events on a [`Channel`][c]. The [`ChannelHandler`][ch]s process these events one after another, in order, mutating and transforming events as they go. This can be thought of as a data processing pipeline; hence the name [`ChannelPipeline`][cp].\n\nAll [`ChannelHandler`][ch]s are either Inbound or Outbound handlers, or both. Inbound handlers process \"inbound\" events: events like reading data from a socket, reading socket close, or other kinds of events initiated by remote peers. Outbound handlers process \"outbound\" events, such as writes, connection attempts, and local socket closes.\n\nEach handler processes the events in order. For example, read events are passed from the front of the pipeline to the back, one handler at a time, while write events are passed from the back of the pipeline to the front. Each handler may, at any time, generate either inbound or outbound events that will be sent to the next handler in whichever direction is appropriate. This allows handlers to split up reads, coalesce writes, delay connection attempts, and generally perform arbitrary transformations of events.\n\nIn general, [`ChannelHandler`][ch]s are designed to be highly re-usable components. This means they tend to be designed to be as small as possible, performing one specific data transformation. This allows handlers to be composed together in novel and flexible ways, which helps with code reuse and encapsulation.\n\n[`ChannelHandler`][ch]s are able to keep track of where they are in a [`ChannelPipeline`][cp] by using a [`ChannelHandlerContext`][chc]. These objects contain references to the previous and next channel handler in the pipeline, ensuring that it is always possible for a [`ChannelHandler`][ch] to emit events while it remains in a pipeline.\n\nSwiftNIO ships with many [`ChannelHandler`][ch]s built in that provide useful functionality, such as HTTP parsing. In addition, high-performance applications will want to provide as much of their logic as possible in [`ChannelHandler`][ch]s, as it helps avoid problems with context switching.\n\nAdditionally, SwiftNIO ships with a few [`Channel`][c] implementations. In particular, it ships with `ServerSocketChannel`, a [`Channel`][c] for sockets that accept inbound connections; `SocketChannel`, a [`Channel`][c] for TCP connections; and `DatagramChannel`, a [`Channel`][c] for UDP sockets. All of these are provided by the `NIOPosix` module. It also provides[`EmbeddedChannel`][ec], a [`Channel`][c] primarily used for testing, provided by the `NIOEmbedded` module.\n\n##### A Note on Blocking\n\nOne of the important notes about [`ChannelPipeline`][cp]s is that they are thread-safe. This is very important for writing SwiftNIO applications, as it allows you to write much simpler [`ChannelHandler`][ch]s in the knowledge that they will not require synchronization.\n\nHowever, this is achieved by dispatching all code on the [`ChannelPipeline`][cp] on the same thread as the [`EventLoop`][el]. This means that, as a general rule, [`ChannelHandler`][ch]s **must not** call blocking code without dispatching it to a background thread. If a [`ChannelHandler`][ch] blocks for any reason, all [`Channel`][c]s attached to the parent [`EventLoop`][el] will be unable to progress until the blocking call completes.\n\nThis is a common concern while writing SwiftNIO applications. If it is useful to write code in a blocking style, it is highly recommended that you dispatch work to a different thread when you're done with it in your pipeline.\n\n#### Bootstrap\n\nWhile it is possible to configure and register [`Channel`][c]s with [`EventLoop`][el]s directly, it is generally more useful to have a higher-level abstraction to handle this work.\n\nFor this reason, SwiftNIO ships a number of `Bootstrap` objects whose purpose is to streamline the creation of channels. Some `Bootstrap` objects also provide other functionality, such as support for Happy Eyeballs for making TCP connection attempts.\n\nCurrently SwiftNIO ships with three `Bootstrap` objects in the `NIOPosix` module: [`ServerBootstrap`](https://apple.github.io/swift-nio/docs/current/NIOPosix/Classes/ServerBootstrap.html), for bootstrapping listening channels; [`ClientBootstrap`](https://apple.github.io/swift-nio/docs/current/NIOPosix/Classes/ClientBootstrap.html), for bootstrapping client TCP channels; and [`DatagramBootstrap`](https://apple.github.io/swift-nio/docs/current/NIOPosix/Classes/DatagramBootstrap.html) for bootstrapping UDP channels.\n\n#### ByteBuffer\n\nThe majority of the work in a SwiftNIO application involves shuffling buffers of bytes around. At the very least, data is sent and received to and from the network in the form of buffers of bytes. For this reason it's very important to have a high-performance data structure that is optimized for the kind of work SwiftNIO applications perform.\n\nFor this reason, SwiftNIO provides [`ByteBuffer`][bb], a fast copy-on-write byte buffer that forms a key building block of most SwiftNIO applications. This type is provided by the `NIOCore` module.\n\n[`ByteBuffer`][bb] provides a number of useful features, and in addition provides a number of hooks to use it in an \"unsafe\" mode. This turns off bounds checking for improved performance, at the cost of potentially opening your application up to memory correctness problems.\n\nIn general, it is highly recommended that you use the [`ByteBuffer`][bb] in its safe mode at all times.\n\nFor more details on the API of [`ByteBuffer`][bb], please see our API documentation, linked below.\n\n#### Promises and Futures\n\nOne major difference between writing concurrent code and writing synchronous code is that not all actions will complete immediately. For example, when you write data on a channel, it is possible that the event loop will not be able to immediately flush that write out to the network. For this reason, SwiftNIO provides [`EventLoopPromise<T>`][elp] and [`EventLoopFuture<T>`][elf] to manage operations that complete *asynchronously*. These types are provided by the `NIOCore` module.\n\nAn [`EventLoopFuture<T>`][elf] is essentially a container for the return value of a function that will be populated *at some time in the future*. Each [`EventLoopFuture<T>`][elf] has a corresponding [`EventLoopPromise<T>`][elp], which is the object that the result will be put into. When the promise is succeeded, the future will be fulfilled.\n\nIf you had to poll the future to detect when it completed that would be quite inefficient, so [`EventLoopFuture<T>`][elf] is designed to have managed callbacks. Essentially, you can hang callbacks off the future that will be executed when a result is available. The [`EventLoopFuture<T>`][elf] will even carefully arrange the scheduling to ensure that these callbacks always execute on the event loop that initially created the promise, which helps ensure that you don't need too much synchronization around [`EventLoopFuture<T>`][elf] callbacks.\n\nAnother important topic for consideration is the difference between how the promise passed to `close` works as opposed to `closeFuture` on a [`Channel`][c]. For example, the promise passed into `close` will succeed after the [`Channel`][c] is closed down but before the [`ChannelPipeline`][cp] is completely cleared out. This will allow you to take action on the [`ChannelPipeline`][cp] before it is completely cleared out, if needed. If it is desired to wait for the [`Channel`][c] to close down and the [`ChannelPipeline`][cp] to be cleared out without any further action, then the better option would be to wait for the `closeFuture` to succeed.\n\nThere are several functions for applying callbacks to [`EventLoopFuture<T>`][elf], depending on how and when you want them to execute. Details of these functions is left to the API documentation.\n\n### Design Philosophy\n\nSwiftNIO is designed to be a powerful tool for building networked applications and frameworks, but it is not intended to be the perfect solution for all levels of abstraction. SwiftNIO is tightly focused on providing the basic I/O primitives and protocol implementations at low levels of abstraction, leaving more expressive but slower abstractions to the wider community to build. The intention is that SwiftNIO will be a building block for server-side applications, not necessarily the framework those applications will use directly.\n\nApplications that need extremely high performance from their networking stack may choose to use SwiftNIO directly in order to reduce the overhead of their abstractions. These applications should be able to maintain extremely high performance with relatively little maintenance cost. SwiftNIO also focuses on providing useful abstractions for this use-case, such that extremely high performance network servers can be built directly.\n\nThe core SwiftNIO repository will contain a few extremely important protocol implementations, such as HTTP, directly in tree. However, we believe that most protocol implementations should be decoupled from the release cycle of the underlying networking stack, as the release cadence is likely to be very different (either much faster or much slower). For this reason, we actively encourage the community to develop and maintain their protocol implementations out-of-tree. Indeed, some first-party SwiftNIO protocol implementations, including our TLS and HTTP/2 bindings, are developed out-of-tree!\n\n## Documentation\n\n - [API documentation](https://apple.github.io/swift-nio/docs/current/NIO/index.html)\n\n## Example Usage\n\nThere are currently several example projects that demonstrate how to use SwiftNIO.\n\n- **chat client** https://github.com/apple/swift-nio/tree/main/Sources/NIOChatClient\n- **chat server** https://github.com/apple/swift-nio/tree/main/Sources/NIOChatServer\n- **echo client** https://github.com/apple/swift-nio/tree/main/Sources/NIOEchoClient\n- **echo server** https://github.com/apple/swift-nio/tree/main/Sources/NIOEchoServer\n- **UDP echo client** https://github.com/apple/swift-nio/tree/main/Sources/NIOUDPEchoClient\n- **UDP echo server** https://github.com/apple/swift-nio/tree/main/Sources/NIOUDPEchoServer\n- **HTTP client** https://github.com/apple/swift-nio/tree/main/Sources/NIOHTTP1Client\n- **HTTP server** https://github.com/apple/swift-nio/tree/main/Sources/NIOHTTP1Server\n- **WebSocket client** https://github.com/apple/swift-nio/tree/main/Sources/NIOWebSocketClient\n- **WebSocket server** https://github.com/apple/swift-nio/tree/main/Sources/NIOWebSocketServer\n\nTo build & run them, run following command, replace TARGET_NAME with the folder name under `./Sources`\n\n```bash\nswift run TARGET_NAME\n```\n\nFor example, to run NIOHTTP1Server, run following command:\n\n```bash\nswift run NIOHTTP1Server\n```\n\n## Getting Started\n\nSwiftNIO primarily uses [SwiftPM](https://swift.org/package-manager/) as its build tool, so we recommend using that as well. If you want to depend on SwiftNIO in your own project, it's as simple as adding a `dependencies` clause to your `Package.swift`:\n\n```swift\ndependencies: [\n    .package(url: \"https://github.com/apple/swift-nio.git\", from: \"2.0.0\")\n]\n```\n\nand then adding the appropriate SwiftNIO module(s) to your target dependencies.\nThe syntax for adding target dependencies differs slightly between Swift\nversions. For example, if you want to depend on the `NIOCore`, `NIOPosix` and \n`NIOHTTP1` modules, specify the following dependencies:\n\n#### Swift 5.2 and newer (`swift-tools-version:5.2`)\n\n    dependencies: [.product(name: \"NIOCore\", package: \"swift-nio\"),\n                   .product(name: \"NIOPosix\", package: \"swift-nio\"),\n                   .product(name: \"NIOHTTP1\", package: \"swift-nio\")]\n\n### Using Xcode Package support\n\nIf your project is set up as an Xcode project and you're using Xcode 11+, you can add SwiftNIO as a dependency to your\nXcode project by clicking File -> Swift Packages -> Add Package Dependency. In the upcoming dialog, please enter\n`https://github.com/apple/swift-nio.git` and click Next twice. Finally, select the targets you are planning to use (for\nexample `NIOCore`, `NIOHTTP1`, and `NIOFoundationCompat`) and click finish. Now will be able to `import NIOCore` (as well as all\nthe other targets you have selected) in your project.\n\nTo work on SwiftNIO itself, or to investigate some of the demonstration applications, you can clone the repository directly and use SwiftPM to help build it. For example, you can run the following commands to compile and run the example echo server:\n\n```bash\nswift build\nswift test\nswift run NIOEchoServer\n```\n\nTo verify that it is working, you can use another shell to attempt to connect to it:\n\n```bash\necho \"Hello SwiftNIO\" | nc localhost 9999\n```\n\nIf all goes well, you'll see the message echoed back to you.\n\nTo work on SwiftNIO in Xcode 11+, you can just open the `Package.swift`\nfile in Xcode and use Xcode's support for SwiftPM Packages.\n\nIf you want to develop SwiftNIO with Xcode 10, you have to generate an Xcode project:\n\n```bash\nswift package generate-xcodeproj\n```\n\n### An alternative: using `docker-compose`\n\nAlternatively, you may want to develop or test with `docker-compose`.\n\nFirst make sure you have [Docker](https://www.docker.com/community-edition) installed, next run the following commands:\n\n- `docker-compose -f docker/docker-compose.yaml run test`\n\n  Will create a base image with Swift runtime and other build and test dependencies, compile SwiftNIO and run the unit and integration tests\n\n- `docker-compose -f docker/docker-compose.yaml up echo`\n\n  Will create a base image, compile SwiftNIO, and run a sample `NIOEchoServer` on\n  `localhost:9999`. Test it by `echo Hello SwiftNIO | nc localhost 9999`.\n\n- `docker-compose -f docker/docker-compose.yaml up http`\n\n  Will create a base image, compile SwiftNIO, and run a sample `NIOHTTP1Server` on\n  `localhost:8888`. Test it by `curl http://localhost:8888`\n\n- `docker-compose -f docker/docker-compose.yaml -f docker/docker-compose.2004.54.yaml run test`\n\n  Will create a base image using Ubuntu 20.04 and swift 5.4, compile SwiftNIO and run the unit and integration tests.  Files exist for other ubuntu and swift versions in the docker directory.\n\n\n## Developing SwiftNIO\n\n*Note*: This section is only relevant if you would like to develop SwiftNIO yourself. You can ignore the information here if you just want to use SwiftNIO as a SwiftPM package.\n\nFor the most part, SwiftNIO development is as straightforward as any other SwiftPM project. With that said, we do have a few processes that are worth understanding before you contribute. For details, please see `CONTRIBUTING.md` in this repository.\n\n### Prerequisites\n\nSwiftNIO's `main` branch is the development branch for the next releases of SwiftNIO 2, it's Swift 5-only.\n\nTo be able to compile and run SwiftNIO and the integration tests, you need to\nhave a few prerequisites installed on your system.\n\n#### macOS\n\n- Xcode 11.4 or newer, Xcode 12 recommended.\n\n### Linux\n\n- Swift 5.2, 5.3, or 5.4 from [swift.org/download](https://swift.org/download/#releases). We always recommend to use the latest released version.\n- netcat (for integration tests only)\n- lsof (for integration tests only)\n- shasum (for integration tests only)\n\n#### Ubuntu 18.04\n\n```\n# install swift tarball from https://swift.org/downloads\napt-get install -y git curl libatomic1 libxml2 netcat-openbsd lsof perl\n```\n\n\n### Fedora 28+\n\n```\ndnf install swift-lang /usr/bin/nc /usr/bin/lsof /usr/bin/shasum\n```\n\n[ch]: https://apple.github.io/swift-nio/docs/current/NIOCore/Protocols/ChannelHandler.html\n[c]: https://apple.github.io/swift-nio/docs/current/NIOCore/Protocols/Channel.html\n[chc]: https://apple.github.io/swift-nio/docs/current/NIOCore/Classes/ChannelHandlerContext.html\n[ec]: https://apple.github.io/swift-nio/docs/current/NIOCore/Classes/EmbeddedChannel.html\n[el]: https://apple.github.io/swift-nio/docs/current/NIOCore/Protocols/EventLoop.html\n[eel]: https://apple.github.io/swift-nio/docs/current/NIOCore/Classes/EmbeddedEventLoop.html\n[elg]: https://apple.github.io/swift-nio/docs/current/NIOCore/Protocols/EventLoopGroup.html\n[bb]: https://apple.github.io/swift-nio/docs/current/NIOCore/Structs/ByteBuffer.html\n[elf]: https://apple.github.io/swift-nio/docs/current/NIOCore/Classes/EventLoopFuture.html\n[elp]: https://apple.github.io/swift-nio/docs/current/NIOCore/Structs/EventLoopPromise.html\n[cp]: https://apple.github.io/swift-nio/docs/current/NIOCore/Classes/ChannelPipeline.html\n[mtelg]: https://apple.github.io/swift-nio/docs/current/NIOPosix/Classes/MultiThreadedEventLoopGroup.html\n[pthreads]: https://en.wikipedia.org/wiki/POSIX_Threads\n[kqueue]: https://en.wikipedia.org/wiki/Kqueue\n[epoll]: https://en.wikipedia.org/wiki/Epoll\n[repo-nio]: https://github.com/apple/swift-nio\n[repo-nio-extras]: https://github.com/apple/swift-nio-extras\n[repo-nio-http2]: https://github.com/apple/swift-nio-http2\n[repo-nio-ssl]: https://github.com/apple/swift-nio-ssl\n[repo-nio-transport-services]: https://github.com/apple/swift-nio-transport-services\n[repo-nio-ssh]: https://github.com/apple/swift-nio-ssh\n\n### Speeding up testing\nIt's possible to run the test suite in parallel, it can save significant time if you have a larger multi-core machine, just add `--parallel` when running the tests. This can speed up the run time of the test suite by 30x or more.\n\n```\nswift test --parallel\n```\n"
 },
 {
  "repo": "apple/ml-hypersim",
  "language": "Python",
  "readme_contents": "![The Hypersim Dataset](docs/teaser_web.jpg \"The Hypersim Dataset\")\n\n# The Hypersim Dataset\n\nFor many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.\n\nThe Hypersim Dataset is licensed under the [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/).\n\n&nbsp;\n## Citation\n\nIf you find the Hypersim Dataset or the Hypersim Toolkit useful in your research, please cite the following [paper](https://arxiv.org/abs/2011.02523):\n\n```\n@inproceedings{roberts:2021,\n    author    = {Mike Roberts AND Jason Ramapuram AND Anurag Ranjan AND Atulit Kumar AND\n                 Miguel Angel Bautista AND Nathan Paczan AND Russ Webb AND Joshua M. Susskind},\n    title     = {{Hypersim}: {A} Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding},\n    booktitle = {International Conference on Computer Vision (ICCV) 2021},\n    year      = {2021}\n}\n```\n\n&nbsp;\n## Downloading the Hypersim Dataset\n\nTo obtain our image dataset, you can run the following download script. On Windows, you'll need to modify the script so it doesn't depend on the `curl` and `unzip` command-line utilities.\n\n```\npython code/python/tools/dataset_download_images.py --downloads_dir /Volumes/portable_hard_drive/downloads --decompress_dir /Volumes/portable_hard_drive/evermotion_dataset/scenes\n```\n\nNote that our dataset is roughly 1.9TB. We have partitioned the dataset into a few hundred separate ZIP files, where each ZIP file is between 1GB and 20GB. Our [download script](code/python/tools/dataset_download_images.py) contains the URLs for each ZIP file. [Thomas Germer](https://github.com/99991) has generously contributed an [alternative download script](contrib/99991) that can be used to download subsets of files from within each ZIP archive.\n\nNote also that we manually excluded images containing people and prominent logos from our public release, and therefore our public release contains 74,619 images, rather than 77,400 images. We list all the images we manually excluded in `ml-hypersim/evermotion_dataset/analysis/metadata_images.csv`.\n\nTo obtain the ground truth triangle meshes for each scene, you must purchase the asset files [here](https://www.turbosquid.com/Search/3D-Models?include_artist=evermotion).\n\n&nbsp;\n## Working with the Hypersim Dataset\n\nThe Hypersim Dataset consists of a collection of synthetic scenes. Each scene has a name of the form `ai_VVV_NNN` where `VVV` is the volume number, and `NNN` is the scene number within the volume. For each scene, there are one or more camera trajectories named {`cam_00`, `cam_01`, ...}. Each camera trajectory has one or more images named {`frame.0000`, `frame.0001`, ...}. Each scene is stored in its own ZIP file according to the following data layout:\n\n```\nai_VVV_NNN\n\u251c\u2500\u2500 _detail\n\u2502   \u251c\u2500\u2500 metadata_cameras.csv                     # list of all the camera trajectories for this scene\n\u2502   \u251c\u2500\u2500 metadata_node_strings.csv                # all human-readable strings in the definition of each V-Ray node\n\u2502   \u251c\u2500\u2500 metadata_nodes.csv                       # establishes a correspondence between the object names in an exported OBJ file, and the V-Ray node IDs that are stored in our render_entity_id images\n\u2502   \u251c\u2500\u2500 metadata_scene.csv                       # includes the scale factor to convert asset units into meters\n\u2502   \u251c\u2500\u2500 cam_XX                                   # camera trajectory information\n\u2502   \u2502   \u251c\u2500\u2500 camera_keyframe_orientations.hdf5    # camera orientations\n\u2502   \u2502   \u2514\u2500\u2500 camera_keyframe_positions.hdf5       # camera positions (in asset coordinates)\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 mesh                                                                            # mesh information\n\u2502       \u251c\u2500\u2500 mesh_objects_si.hdf5                                                        # NYU40 semantic label for each object ID (available in our public code repository)\n\u2502       \u251c\u2500\u2500 mesh_objects_sii.hdf5                                                       # semantic instance ID for each object ID (available in our public code repository)\n\u2502       \u251c\u2500\u2500 metadata_objects.csv                                                        # object name for each object ID (available in our public code repository)\n\u2502       \u251c\u2500\u2500 metadata_scene_annotation_tool.log                                          # log of the time spent annotating each scene (available in our public code repository)\n\u2502       \u251c\u2500\u2500 metadata_semantic_instance_bounding_box_object_aligned_2d_extents.hdf5      # length (in asset units) of each dimension of the 3D bounding for each semantic instance ID\n\u2502       \u251c\u2500\u2500 metadata_semantic_instance_bounding_box_object_aligned_2d_orientations.hdf5 # orientation of the 3D bounding box for each semantic instance ID\n\u2502       \u2514\u2500\u2500 metadata_semantic_instance_bounding_box_object_aligned_2d_positions.hdf5    # position (in asset coordinates) of the 3D bounding box for each semantic instance ID\n\u2514\u2500\u2500 images\n    \u251c\u2500\u2500 scene_cam_XX_final_hdf5                  # lossless HDR image data that requires accurate shading\n    \u2502   \u251c\u2500\u2500 frame.IIII.color.hdf5                # color image before any tone mapping has been applied\n    \u2502   \u251c\u2500\u2500 frame.IIII.diffuse_illumination.hdf5 # diffuse illumination\n    \u2502   \u251c\u2500\u2500 frame.IIII.diffuse_reflectance.hdf5  # diffuse reflectance (many authors refer to this modality as \"albedo\")\n    \u2502   \u251c\u2500\u2500 frame.IIII.residual.hdf5             # non-diffuse residual\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 scene_cam_XX_final_preview               # preview images\n    |   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 scene_cam_XX_geometry_hdf5               # lossless HDR image data that does not require accurate shading\n    \u2502   \u251c\u2500\u2500 frame.IIII.depth_meters.hdf5         # Euclidean distances (in meters) to the optical center of the camera\n    \u2502   \u251c\u2500\u2500 frame.IIII.position.hdf5             # world-space positions (in asset coordinates)\n    \u2502   \u251c\u2500\u2500 frame.IIII.normal_cam.hdf5           # surface normals in camera-space (ignores bump mapping)\n    \u2502   \u251c\u2500\u2500 frame.IIII.normal_world.hdf5         # surface normals in world-space (ignores bump mapping)\n    \u2502   \u251c\u2500\u2500 frame.IIII.normal_bump_cam.hdf5      # surface normals in camera-space (takes bump mapping into account)\n    \u2502   \u251c\u2500\u2500 frame.IIII.normal_bump_world.hdf5    # surface normals in world-space (takes bump mapping into account)\n    \u2502   \u251c\u2500\u2500 frame.IIII.render_entity_id.hdf5     # fine-grained segmentation where each V-Ray node has a unique ID\n    \u2502   \u251c\u2500\u2500 frame.IIII.semantic.hdf5             # NYU40 semantic labels\n    \u2502   \u251c\u2500\u2500 frame.IIII.semantic_instance.hdf5    # semantic instance IDs\n    \u2502   \u251c\u2500\u2500 frame.IIII.tex_coord.hdf5            # texture coordinates\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 scene_cam_XX_geometry_preview            # preview images\n    |   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n```\n\n### Dataset split\n\nWe include a standard train/val/test split in `ml-hypersim/evermotion_dataset/analysis/metadata_images_split_scene_v1.csv`. We refer to this split as the _v1_ split of our dataset. We generated this split by randomly partitioning our data at the granularity of scenes, rather than images or camera trajectories, to minimize the probability of very similar images ending up in different partitions. In order to maximize reproducibilty, we only include publicly released images in our split.\n\nIn the Hypersim dataset, there is a small amount of asset reuse across scenes. This asset reuse is difficult to detect by analyzing metadata in the original scene assets, but it is evident when manually browsing through our rendered images. We do not attempt to address this issue when generating our split. Therefore, individual objects in our training images are occasionally also present in our validation and test images.\n\n### Coordinate conventions\n\nWe store positions in _asset coordinates_ (and lengths in _asset units_) unless explicitly noted otherwise. By _asset coordinates_, we mean the world-space coordinate system defined by the artist when they originally created the assets. In general, asset units are not the same as meters. To convert a distance in asset units to a distance in meters, use the `meters_per_asset_unit` scale factor defined in `ai_VVV_NNN/_detail/metadata_scene.csv`.\n\nWe store orientations as 3x3 rotation matrices that map points to world-space from object-space, assuming that points are stored as [x,y,z] column vectors. Our convention for storing camera orientations is that the camera's positive x-axis points right, the positive y-axis points up, and the positive z-axis points away from where the camera is looking.\n\n### Lossless high-dynamic range images\n\nImages for each camera trajectory are stored as lossless high-dynamic range HDF5 files in `ai_VVV_NNN/images/scene_cam_XX_final_hdf5` and `ai_VVV_NNN/images/scene_cam_XX_geometry_hdf5`.\n\nOur `depth_meters` images contain Euclidean distances (in meters) to the optical center of the camera (perhaps a better name for these images would be `distance_from_camera_meters`). In other words, these images do not contain planar depth values, i.e., negative z-coordinates in camera-space. [Simon Niklaus](https://github.com/sniklaus) has generously contributed a [self-contained code snippet](https://github.com/apple/ml-hypersim/issues/9#issuecomment-754935697) for converting our `depth_meters` images into planar depth images. Because our `depth_meters` images contain distances in meters, but our camera positions are stored in asset coordinates, you need to convert our `depth_meters` images into asset units before performing calculations that involve camera positions (or any other positions that are stored in asset coordinates, e.g., bounding box positions).\n\nOur `position` images contain world-space positions specified in asset coordinates.\n\nOur `color`, `diffuse_illumination`, `diffuse_reflectance`, and `residual` images adhere with very low error to the following equation:\n\n```\ncolor == (diffuse_reflectance * diffuse_illumination) + residual\n```\n\nNote that our `color`, `diffuse_illumination`, `diffuse_reflectance`, and `residual` images do not have any tone mapping applied to them. In order to use these images for downstream learning tasks, we recommend applying your own tone mapping operator to the images. We implement a simple tone mapping operator in `ml-hypersim/code/python/tools/scene_generate_images_tonemap.py`.\n\n### Lossy preview images\n\nWe include lossy preview images in `ai_VVV_NNN/images/scene_cam_XX_final_preview` and `ai_VVV_NNN/images/scene_cam_XX_geometry_preview`. We do not recommend using these images for downstream learning tasks, but they are useful for debugging and manually browsing through the data.\n\n### Camera trajectories\n\nEach camera trajectory is stored as a dense list of camera poses in `ai_VVV_NNN/_detail/cam_XX` in the following files.\n\n`camera_keyframe_orientations.hdf5` contains an Nx3x3 array of camera orientations, where N is the number of frames in the trajectory, and each orientation is represented as a 3x3 rotation matrix that maps points to world-space from camera-space, assuming that points are stored as [x,y,z] column vectors. The convention in the Hypersim Dataset is that the camera's positive x-axis points right, the positive y-axis points up, and the positive z-axis points away from where the camera is looking.\n\n`camera_keyframe_positions.hdf5` contains an Nx3 array of camera positions, where N is the number of frames in the trajectory, and each position is stored in [x,y,z] order. These positions are specified in asset coordinates.\n\n### Camera intrinsics\n\nEach scene uses slightly different camera intrinsics for rendering. This behavior arises because some scenes use non-standard tilt-shift photography parameters in their scene definition files. In [`ml-hypersim/contrib/mikeroberts3000`](contrib/mikeroberts3000), we provide a modified perspective projection matrix for each scene that can be used as a drop-in replacement for the usual OpenGL perspective projection matrix, as well as example code for projecting world-space points into Hypersim images. We recommend browsing through this example code to better understand our camera pose conventions.\n\n### 3D bounding boxes\n\nWe include a tight 9-DOF bounding box for each semantic instance in `ai_VVV_NNN/_detail/mesh`. We represent each bounding box as a position, a rotation matrix, and the length of each bounding box dimension. We store this information in the following files.\n\n`metadata_semantic_instance_bounding_box_object_aligned_2d_extents.hdf5` contains an Nx3 array of lengths, where N is the number of semantic instances, and each row represents the length of the each bounding box dimension stored in [x,y,z] order. These lengths are specified in asset units.\n\n`metadata_semantic_instance_bounding_box_object_aligned_2d_orientations.hdf5` contains an Nx3x3 array of orientations, where N is the number of semantic instances, and each orientation is represented as a 3x3 rotation matrix that maps points to world-space from object-space, assuming that points are stored as [x,y,z] column vectors.\n\n`metadata_semantic_instance_bounding_box_object_aligned_2d_positions.hdf5` contains an Nx3 array of bounding box center positions, where N is the number of semantic instances, and each position is stored in [x,y,z] order. These positions are specified in asset coordinates.\n\nWe compute each bounding box's rotation matrix according to the following algorithm. We always set the positive z-axis of our rotation matrix to point up, i.e., to align with the world-space gravity vector. We then compute a 2D minimum-area bounding box in the world-space xy-plane. Once we have computed our minimum-area bounding box, we have 4 possible choices for the positive x-axis of our rotation matrix. To make this choice, we consider the vector from the bounding box's geometric center to the center-of-mass of the points used to compute the bounding box. We choose the direction (among our 4 possible choices) that most closely aligns with this vector as the positive x-axis of our rotation matrix. Finally, we set the positive y-axis to be our positive x-axis rotated by +90 degrees in the world-space xy-plane (i.e., so our rotation matrix will have a determinant of 1). This algorithm encourages that similar objects with semantically similar orientations will be assigned rotation matrices that are similar (i.e., the difference of their rotation matrices will have a small matrix norm).\n\nOur code can be used to compute other types of bounding boxes (e.g., axis-aligned in world-space, minimum-volume), but we don't include these other types of bounding boxes in our public release.\n\nWe recommend browsing through `ml-hypersim/code/python/tools/scene_generate_images_bounding_box.py` to better understand our bounding box conventions. In this file, we generate an image that has per-instance 3D bounding boxes overlaid on top of a previously rendered image. This process involves loading a previously rendered image, loading the appropriate bounding boxes for that image, and projecting the world-space corners of each bounding box into the image.\n\n### Mesh annotations\n\nWe include our mesh annotations in `ml-hypersim/evermotion_dataset/scenes/ai_VVV_NNN/_detail/mesh`. The exported OBJ file for each scene (which can be obtained by purchasing the original scene assets) partitions each scene into a flat list of low-level \"objects\". We manually group these low-level objects into semantically meaningful instances, and assign an NYU40 semantic label to each instance, using our custom scene annotation tool. We store our mesh annotation information in the following files.\n\n`mesh_objects_si.hdf5` contains an array of length N, where N is the number of low-level objects in the exported OBJ file, and `mesh_objects_si[i]` is the NYU40 semantic label for the low-level object with `object_id == i`.\n\n`mesh_objects_sii.hdf5` contains an array of length N, where N is the number of low-level objects in the exported OBJ file, and `mesh_objects_sii[i]` is the semantic instance ID for the low-level object with `object_id == i`.\n\n`metadata_objects.csv` contains N text entries, where N is the number of low-level objects in the exported OBJ file, and `metadata_objects[i]` is the object name for the low-level object with `object_id == i`. This file establishes a correspondence between the object names in the exported OBJ file, and the object IDs used as indices in `mesh_objects_si.hdf5` and `mesh_objects_sii.hdf5`.\n\n`metadata_scene_annotation_tool.log` contains a log of the time spent annotating each scene.\n\n### Rendering costs\n\nWe include the cost of rendering each image in our dataset in `ml-hypersim/evermotion_dataset/analysis/metadata_rendering_tasks.csv`. We include this rendering metadata so the marginal value and marginal cost of each image can be analyzed jointly in downstream applications.\n\nIn our pipeline, we divide rendering into 3 passes per image. Each rendering pass for each image in each camera trajectory corresponds to a particular rendering \"task\", and the costs in `metadata_rendering_tasks.csv` are specified per task. To compute the total cost of rendering the image `frame.IIII` in the camera trajectory `cam_XX` in the scene `ai_VVV_NNN`, we add up the `vray_cost_dollars` and `cloud_cost_dollars` columns for the rows where `job_name is in {ai_VVV_NNN@scene_cam_XX_geometry, ai_VVV_NNN@scene_cam_XX_pre, ai_VVV_NNN@scene_cam_XX_final} and task_id == IIII`.\n\n&nbsp;\n# The Hypersim Toolkit\n\nThe Hypersim Toolkit is a set of tools for generating photorealistic synthetic datasets from V-Ray scenes. By building on top of V-Ray, the datasets generated using the Hypersim Toolkit can leverage advanced rendering effects (e.g., rolling shutter, motion and defocus blur, chromatic aberration), as well as abundant high-quality 3D content from online marketplaces.\n\nThe Hypersim Toolkit consists of tools that operate at two distinct levels of abstraction. The _Hypersim Low-Level Toolkit_ is concerned with manipulating individual V-Ray scene files. The _Hypersim High-Level Toolkit_ is concerned with manipulating collections of scenes. You can use the Hypersim Low-Level Toolkit to output richly annotated ground truth labels, programmatically specify camera trajectories and custom lens distortion models, and programmatically insert geometry into a scene. You can use the Hypersim High-Level Toolkit to generate collision-free camera trajectories that are biased towards the salient parts of a scene, and interactively apply semantic labels to scenes.\n\n&nbsp;\n## Disclaimer\n\nThis software depends on several open-source projects. Some of the dependent projects have portions that are licensed under the GPL, but this software does not depend on those GPL-licensed portions. The GPL-licensed portions may be omitted from builds of those dependent projects.\n\nV-Ray Standalone and the V-Ray AppSDK are available under their own terms [here](http://www.chaosgroup.com). The authors of this software are not responsible for the contents of third-party websites.\n\n&nbsp;\n## Installing the prerequisite applications, tools, and libraries\n\n### Quick start for Anaconda Python\n\nIf you're using Anaconda, you can install all of the required Python libraries using our `requirements.txt` file.\n\n```\nconda create --name hypersim-env --file requirements.txt\nconda activate hypersim-env\n```\n\nOptional Python libraries (see below) can be installed separately. For example,\n\n```\npip install mayavi\nconda install -c conda-forge opencv\nconda install -c anaconda pillow\n```\n\n### Hypersim Low-Level Toolkit\n\n- Install Python (we recommend Anaconda Python 3.7)\n- Install the following Python libraries: h5py, matplotlib, pandas, scikit-learn\n  - http://www.h5py.org\n  - http://matplotlib.org\n  - http://pandas.pydata.org\n  - http://scikit-learn.org\n- Install V-Ray Standalone and the V-Ray AppSDK (version Next Standalone, update 2.1 for x64 or later; see below)\n- Configure the Hypersim Python tools for your system (see below)\n\n### Hypersim High-Level Toolkit\n\n- Complete all the prerequisite steps for the Hypersim Low-Level Toolkit (see above).\n- Install the following Python libraries: joblib, scipy\n  - http://joblib.readthedocs.io\n  - http://www.scipy.org\n- Install the following C++ libraries: args, Armadillo, Embree, HDF5, Octomap, OpenEXR\n  - http://github.com/Taywee/args\n  - http://arma.sourceforge.net\n  - http://www.hdfgroup.org/solutions/hdf5\n  - http://www.embree.org\n  - http://octomap.github.io\n  - http://www.openexr.com\n- Build the Hypersim C++ tools (see below)\n\n### Optional components\n\nThe following components are optional, so you only need to install these prerequisites if you intend to use a particular component.\n\n- Using the Hypersim debug visualization tools\n  - Install the following Python libraries: mayavi\n    - http://docs.enthought.com/mayavi/mayavi\n- Specifying custom non-parametric lens distortion models for rendering\n  - Install the OpenCV Python bindings with OpenEXR support enabled\n    - http://opencv.org\n- Using the Hypersim Scene Annotation Tool\n  - Install the following C++ libraries: Font-Awesome, IconFontCppHeaders, libigl\n    - http://github.com/FortAwesome/Font-Awesome\n    - http://github.com/juliettef/IconFontCppHeaders\n    - http://libigl.github.io\n- Computing bounding boxes around objects\n  - Install the following C++ libraries: ApproxMVBB\n    - http://github.com/gabyx/ApproxMVBB\n- Generating images of bounding boxes overlaid on top of rendered images\n  - Install the following Python libraries: pillow\n    - http://pillow.readthedocs.io\n\n### Configuring the Hypersim Python tools for your system\n\nYou need to rename `ml-hypersim/code/python/_system_config.py.example -> _system_config.py`, and modify the paths contained in this file for your system.\n\n### Installing V-Ray Standalone and the V-Ray AppSDK\n\nMake sure the `bin` directory from V-Ray Standalone is in your `PATH` environment variable. Also make sure that the `bin` directory from the V-Ray AppSDK is in your `DYLD_LIBRARY_PATH` environment variable. For example, I add the following to my `~/.bash_profile` file.\n\n```\nexport PATH=$PATH:/Applications/ChaosGroup/V-Ray/Standalone_for_mavericks_x64/bin\nexport DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/Applications/ChaosGroup/V-Ray/AppSDK/bin\n```\n\nManually copy `vray.so` from the AppSDK directory so it is visible to your Python distribution.\n\nManually copy the following files and subdirectories from the AppSDK `bin` directory to the `ml-hypersim/code/python/tools` directory. For example,\n\n```\ncp /Applications/ChaosGroup/V-Ray/AppSDK/bin/libcgauth.dylib          /Users/mike/code/github/ml-hypersim/code/python/tools\ncp /Applications/ChaosGroup/V-Ray/AppSDK/bin/libvray.dylib            /Users/mike/code/github/ml-hypersim/code/python/tools\ncp /Applications/ChaosGroup/V-Ray/AppSDK/bin/libvrayopenimageio.dylib /Users/mike/code/github/ml-hypersim/code/python/tools\ncp /Applications/ChaosGroup/V-Ray/AppSDK/bin/libvrayosl.dylib         /Users/mike/code/github/ml-hypersim/code/python/tools\ncp /Applications/ChaosGroup/V-Ray/AppSDK/bin/libVRaySDKLibrary.dylib  /Users/mike/code/github/ml-hypersim/code/python/tools\ncp -a /Applications/ChaosGroup/V-Ray/AppSDK/bin/plugins               /Users/mike/code/github/ml-hypersim/code/python/tools\n```\n\nYou can verify that the V-Ray AppSDK is installed correctly by executing the following command-line tool.\n\n```\npython code/python/tools/check_vray_appsdk_install.py\n```\n\nIf the V-Ray AppSDK is installed correctly, this tool will print out the following message.\n\n```\n[HYPERSIM: CHECK_VRAY_APPSDK_INSTALL] The V-Ray AppSDK is configured correctly on your system.\n```\n\n### Building the Hypersim C++ tools\n\nYou need to rename `ml-hypersim/code/cpp/system_config.inc.example -> system_config.inc`, and modify the paths contained in this file for your system. Then you need to build the Hypersim C++ tools. The easiest way to do this is to use the top-level makefile in `ml-hypersim/code/cpp/tools`.\n\n```\ncd code/cpp/tools\nmake\n```\n\nIf you intend to use the Hypersim Scene Annotation Tool, you need to build it separately.\n\n```\ncd code/cpp/tools/scene_annotation_tool\nmake\n```\n\nIf you intend to compute bounding boxes around objects, you need to build the following tool separately.\n\n```\ncd code/cpp/tools/generate_oriented_bounding_boxes\nmake\n```\n\n&nbsp;\n## Using the Hypersim Toolkit\n\nThe Hypersim Low-Level Toolkit consists of the following Python command-line tools.\n\n- `ml-hypersim/code/python/tools/generate_*.py`\n- `ml-hypersim/code/python/tools/modify_vrscene_*.py`\n\nThe Hypersim High-Level Toolkit consists of the following Python command-line tools.\n\n- `ml-hypersim/code/python/tools/dataset_*.py`\n- `ml-hypersim/code/python/tools/scene_*.py`\n- `ml-hypersim/code/python/tools/visualize_*.py`\n\nThe Hypersim High-Level Toolkit also includes the Hypersim Scene Annotation Tool executable, which is located in the `ml-hypersim/code/cpp/bin` directory, and can be launched from the command-line as follows.\n\n```\ncd code/cpp/bin\n./scene_annotation_tool\n```\n\nThe following tutorial examples demonstrate the functionality in the Hypersim Toolkit.\n\n- [`00_empty_scene`](examples/00_empty_scene) In this tutorial example, we use the Hypersim Low-Level Toolkit to add a camera trajectory and a collection of textured quads to a V-Ray scene.\n\n- [`01_marketplace_dataset`](examples/01_marketplace_dataset) In this tutorial example, we use the Hypersim High-Level Toolkit to export and manipulate a scene downloaded from a content marketplace. We generate a collection of richly annotated ground truth images based on a random walk camera trajectory through the scene.\n\n&nbsp;\n## Generating the full Hypersim Dataset\n\nWe recommend completing the [`00_empty_scene`](examples/00_empty_scene) and [`01_marketplace_dataset`](examples/01_marketplace_dataset) tutorial examples before attempting to generate the full Hypersim Dataset.\n\n### Downloading scenes\n\nIn order to generate the full Hypersim Dataset, we use Evermotion Archinteriors Volumes 1-55 excluding 20,25,40,49. All the Evermotion Archinteriors volumes are available for purchase [here](https://www.turbosquid.com/Search/3D-Models?include_artist=evermotion).\n\nYou need to create a `downloads` directory, and manually download the Evermotion Archinteriors RAR and 7z archives into it. Almost all the archives have clear filenames that include the volume number and scene number, and do not need to be renamed to avoid confusion. The exception to this rule is Evermotion Archinteriors Volume 11, whose archives are named {`01.rar`, `02.rar`, ...}. You need to manually rename these archives to {`AI11_01.rar`, `AI11_02.rar`, ...} in order to match the dataset configuration file (`_dataset_config.py`) we provide.\n\n### Running our pipeline on multiple operating systems\n\nSome of our pipeline steps require Windows, and others require macOS or Linux. It is therefore desriable to specify an output directory for the various steps of our pipeline that is visible to both operating systems. Ideally, you would specify an output directory on a fast network drive with lots of storage space. However, our pipeline generates a lot of intermediate data, and disk I/O can become a significant bottleneck, even on relatively fast network drives. We therefore recommend the quick-and-dirty solution of generating the Hypersim Dataset on portable hard drives that you can read and write from Windows and macOS (or Linux).\n\nYou need to make sure that the absolute path to the dataset on Windows is consistent (i.e., always has the same drive letter) when executing the Windows-only steps of our pipeline. We recommend making a note of the absolute Windows path to the dataset, because you will need to supply it whenever a subsequent pipeline step requires the `dataset_dir_when_rendering` argument.\n\nIf you are generating data on portable hard drives, we recommend running our pipeline in batches of 10 volumes at a time (i.e., roughly 100 scenes at a time), and storing each batch on its own 4TB drive. If you attempt to run our pipeline in batches that are too large, the pipeline will eventually generate too much intermediate data, and you will run out of storage space. In our experience, the most straightforward way to run our pipeline in batches is to include the optional `scene_names` argument when executing each step of the pipeline.\n\nThe `scene_names` argument works in the following way. We give each scene in our dataset a unique name, `ai_VVV_NNN`, where `VVV` is the volume number, and `NNN` is the scene number within the volume (e.g., the name `ai_001_002` refers to Volume 1 Scene 2). Each step of our pipeline can process a particular scene (or scenes) by specifying the `scene_names` argument, which accepts wildcard expressions. For example, `ai_001_001` specifies Volume 1 Scene 1, `ai_001_*` specifies all scenes from Volume 1, `ai_00*` specifies all scenes from Volumes 1-9, `ai_01*` specifies all scenes from Volumes 10-19, and so on. We include the argument `--scene_names ai_00*` in our instructions below.\n\n### Handling scenes and camera trajectories that have been manually excluded\n\nWhen preparing the Hypersim Dataset, we chose to manually exclude some scenes and automatically generated camera trajectories. Most of the scenes we excluded are simply commented out in our `_dataset_config.py` file, and therefore our pipeline never processes these scenes. However, for some scenes, we needed to run some of our pipeline in order to decide to exclude them. These scenes are un-commmented in our `dataset_config.py` file, and therefore our pipeline will process these scenes by default. There is no harm in running our pipeline for these scenes, but it is possible to save a bit of time and money by not rendering images for these manually excluded scenes and camera trajectories.\n\nThe camera trajectories we manually excluded from our dataset are listed in `ml-hypersim/evermotion_dataset/analysis/metadata_camera_trajectories.csv`. If the `Scene type` column is listed as `OUTSIDE VIEWING AREA (BAD INITIALIZATION)` or `OUTSIDE VIEWING AREA (BAD TRAJECTORY)`, then we consider that trajectory to be manually excluded from our dataset. If all the camera trajectories for a scene have been manually excluded, then we consider the scene to be manually excluded. We recommend excluding these scenes and camera trajectories in downstream learning applications for consistency with other publications, and to obtain the cleanest possible training data.\n\n### Using our mesh annotations\n\nOur mesh annotations for each scene are checked in at `ml-hypersim/evermotion_dataset/scenes/ai_VVV_NNN/_detail/mesh`, where `VVV` is the volume number and `NNN` is the scene number within the volume. So, you can use our automatic pipeline to generate instance-level semantic segmentation images without needing to manually annotate any scenes.\n\n### Running the full pipeline\n\nTo process the first batch of scenes (Volumes 1-9) in the Hypersim Dataset, we execute the following pipeline steps. We process subsequent batches by executing these steps repeatedly, substituting the `scene_names` argument as described above.  See the [`01_marketplace_dataset`](examples/01_marketplace_dataset) tutorial example for more details on each of these pipeline steps.\n\n_You must substitute your own `dataset_dir_when_rendering` when executing these pipeline steps, and it must be an absolute path. You must also substitute your own `dataset_dir` and `downloads_dir`, but these arguments do not need to be absolute paths. You must wait until each rendering pass is complete, and all data has finished downloading from the cloud, before proceeding to the next pipeline step._\n\n```\n# pre-processing\n\n# unpack scene data\npython code/python/tools/dataset_initialize_scenes.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --downloads_dir downloads --dataset_dir_to_copy evermotion_dataset --scene_names \"ai_00*\"\n\n# export scene data from native asset file into vrscene file (not provided)\n\n# correct bad default export options\npython code/python/tools/dataset_modify_vrscenes_normalize.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --platform_when_rendering windows --dataset_dir_when_rendering Z:\\\\evermotion_dataset --scene_names \"ai_00*\"\n\n# generate a fast binary triangle mesh representation\npython code/python/tools/dataset_generate_meshes.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n```\n\n```\n# generate an occupancy map (must be run on macOS or Linux)\npython code/python/tools/dataset_generate_octomaps.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n```\n\n```\n# generate camera trajectories (must be run on macOS or Linux)\npython code/python/tools/dataset_generate_camera_trajectories.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n```\n\n```\n# modify vrscene to render camera trajectories with appropriate ground truth layers\npython code/python/tools/dataset_modify_vrscenes_for_hypersim_rendering.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --platform_when_rendering windows --dataset_dir_when_rendering Z:\\\\evermotion_dataset --scene_names \"ai_00*\"\n```\n\n```\n# cloud rendering\n\n# output rendering job description files for geometry pass\npython code/python/tools/dataset_submit_rendering_jobs.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --render_pass geometry --scene_names \"ai_00*\"\n\n# render geometry pass in the cloud (not provided)\n\n# output rendering job description files for pre pass\npython code/python/tools/dataset_submit_rendering_jobs.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --render_pass pre --scene_names \"ai_00*\"\n\n# render pre pass in the cloud (not provided)\n\n# merge per-image lighting data into per-scene lighting data\npython code/python/tools/dataset_generate_merged_gi_cache_files.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n\n# output rendering job description files for final pass\npython code/python/tools/dataset_submit_rendering_jobs.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --render_pass final --scene_names \"ai_00*\"\n\n# render final pass in the cloud (not provided)\n```\n\n```\n# post-processing\n\n# generate tone-mapped images for visualization\npython code/python/tools/dataset_generate_images_tonemap.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n\n# generate semantic segmentation images\npython code/python/tools/dataset_generate_images_semantic_segmentation.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --scene_names \"ai_00*\"\n\n# generate 3D bounding boxes (must be run on macOS or Linux)\npython code/python/tools/dataset_generate_bounding_boxes.py --dataset_dir /Volumes/portable_hard_drive/evermotion_dataset --bounding_box_type object_aligned_2d --scene_names \"ai_00*\"\n```\n"
 },
 {
  "repo": "apple/swift-collections",
  "language": "Swift",
  "readme_contents": "# Swift Collections\n\n**Swift Collections** is an open-source package of data structure implementations for the Swift programming language.\n\nRead more about the package, and the intent behind it, in the [announcement on swift.org][announcement].\n\n[announcement]: https://swift.org/blog/swift-collections\n\n## Contents\n\nThe package currently provides the following implementations:\n\n- [`Deque<Element>`][Deque], a double-ended queue backed by a ring buffer. Deques are range-replaceable, mutable, random-access collections.\n\n- [`OrderedSet<Element>`][OrderedSet], a variant of the standard `Set` where the order of items is well-defined and items can be arbitrarily reordered. Uses a `ContiguousArray` as its backing store, augmented by a separate hash table of bit packed offsets into it.\n\n- [`OrderedDictionary<Key, Value>`][OrderedDictionary], an ordered variant of the standard `Dictionary`, providing similar benefits.\n\n[Deque]: Documentation/Deque.md\n[OrderedSet]: Documentation/OrderedSet.md\n[OrderedDictionary]: Documentation/OrderedDictionary.md\n\nThe following data structures are currently being worked on but they aren't ready for inclusion in a tagged release:\n\n- [`Heap`][Heap] and [`PriorityQueue`](https://github.com/apple/swift-collections/pull/51), min-max heaps backed by an array.\n- [`SortedSet` and `SortedDictionary`](https://github.com/apple/swift-collections/pull/65), sorted collections backed by in-memory persistent b-trees.\n- [`HashSet` and `HashMap`](https://github.com/apple/swift-collections/pull/31), persistent hashed collections implemented as Compressed Hash-Array Mapped Prefix-Trees (CHAMP).\n- [`BitArray` and `BitSet`](https://github.com/apple/swift-collections/pull/83), dynamic bit vectors.\n- [`SparseSet`](https://github.com/apple/swift-collections/pull/80), a constant time set construct, trading off memory for speed.\n\n[Heap]: Documentation/Heap.md\n\nSwift Collections uses the same modularization approach as [**Swift Numerics**](https://github.com/apple/swift-numerics): it provides a standalone module for each thematic group of data structures it implements. For instance, if you only need a double-ended queue type, you can pull in only that by importing `DequeModule`. `OrderedSet` and `OrderedDictionary` share much of the same underlying implementation, so they are provided by a single module, called `OrderedCollections`. However, there is also a top-level `Collections` module that gives you every collection type with a single import statement:\n\n``` swift\nimport Collections\n\nvar deque: Deque<String> = [\"Ted\", \"Rebecca\"]\ndeque.prepend(\"Keeley\")\ndeque.append(\"Nathan\")\nprint(deque) // [\"Keeley\", \"Ted\", \"Rebecca\", \"Nathan\"]\n```\n\n## Project Status\n\nThe Swift Collections package is source stable. The version numbers follow [Semantic Versioning][semver] -- source breaking changes to public API can only land in a new major version.\n\n[semver]: https://semver.org\n\nThe public API of version 1.0 of the `swift-collections` package consists of non-underscored declarations that are marked `public` in the `Collections`, `DequeModule` and `OrderedCollections` modules.\n\nInterfaces that aren't part of the public API may continue to change in any release, including patch releases. \nIf you have a use case that requires using underscored APIs, please [submit a Feature Request][enhancement] describing it! We'd like the public interface to be as useful as possible -- although preferably without compromising safety or limiting future evolution.\n\nBy \"underscored declarations\" we mean declarations that have a leading underscore anywhere in their fully qualified name. For instance, here are some names that wouldn't be considered part of the public API, even if they were technically marked public:\n\n- `FooModule.Bar._someMember(value:)` (underscored member)\n- `FooModule._Bar.someMember` (underscored type)\n- `_FooModule.Bar` (underscored module)\n- `FooModule.Bar.init(_value:)` (underscored initializer)\n\nNote that contents of the `Tests`, `Utils` and `Benchmarks` subdirectories aren't public API. We don't make any source compatibility promises about them -- they may change at whim, and code may be removed in any new release. Do not rely on anything about them. \n\nFuture minor versions of the package may update these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate. Accordingly, from time to time, we expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release. (This allows the package to make use of new language/stdlib features, build on compiler bug fixes, and adopt new package manager functionality as soon as they are available.) Requiring a new Swift release will only need a minor version bump.\n\n## Using **Swift Collections** in your project\n\nTo use this package in a SwiftPM project, you need to set it up as a package dependency:\n\n```swift\n// swift-tools-version:5.4\nimport PackageDescription\n\nlet package = Package(\n  name: \"MyPackage\",\n  dependencies: [\n    .package(\n      url: \"https://github.com/apple/swift-collections.git\", \n      .upToNextMajor(from: \"1.0.0\") // or `.upToNextMinor\n    )\n  ],\n  targets: [\n    .target(\n      name: \"MyTarget\",\n      dependencies: [\n        .product(name: \"Collections\", package: \"swift-collections\")\n      ]\n    )\n  ]\n)\n```\n\n## Contributing to Swift Collections\n\nWe have a dedicated [Swift Collections Forum][forum] where people can ask and answer questions on how to use or work on this package. It's also a great place to discuss its evolution.\n\n[forum]: https://forums.swift.org/c/related-projects/collections\n\nIf you find something that looks like a bug, please open a [Bug Report][bugreport]! Fill out as many details as you can.\n\n### Working on the package\n\nWe have some basic [documentation on package internals](./Documentation/Internals/README.md) that will help you get started.\n\nBy submitting a pull request, you represent that you have the right to license your contribution to Apple and the community, and agree by submitting the patch that your contributions are licensed under the [Swift License](https://swift.org/LICENSE.txt), a copy of which is [provided in this repository](LICENSE.txt).\n\n#### Fixing a bug or making a small improvement\n\n1. [Submit a PR][PR] with your change. If there is an [existing issue][issues] for the bug you're fixing, please include a reference to it.\n2. Make sure to add tests covering whatever changes you are making.\n\n[PR]: https://github.com/apple/swift-collections/compare\n[issues]: https://github.com/apple/swift-collections/issues\n\n[bugreport]: https://github.com/apple/swift-collections/issues/new?assignees=&labels=bug&template=BUG_REPORT.md\n\n#### Proposing a small enhancement\n\n1. Raise a [Feature Request][enhancement]. Discuss why it would be important to implement it.\n2. Submit a PR with your implementation, participate in the review discussion.\n3. When there is a consensus that the feature is desirable, and the implementation works well, it is fully tested and documented, then it will be merged. \n4. Rejoice!\n\n[enhancement]: https://github.com/apple/swift-collections/issues/new?assignees=&labels=enhancement&template=FEATURE_REQUEST.md\n\n#### Proposing the addition of a new data structure\n\n1. Start a topic on the [forum], explaining why you believe it would be important to implement the data structure. This way we can figure out if it would be right for the package, discuss implementation strategies, and plan to allocate capacity to help.\n2. When maintainers agreed to your implementation plan, start work on it, and submit a PR with your implementation as soon as you have something that's ready to show! We'd love to get involved as early as you like.\n3. Participate in the review discussion, and adapt the code accordingly. Sometimes we may need to go through several revisions! This is fine -- it makes the end result that much better.\n3. When there is a consensus that the feature is ready, and the implementation is fully tested and documented, the PR will be merged by a maintainer.\n4. Celebrate! You've achieved something great!\n\n### Code of Conduct\n\nLike all Swift.org projects, we would like the Swift Collections project to foster a diverse and friendly community. We expect contributors to adhere to the [Swift.org Code of Conduct](https://swift.org/code-of-conduct/). A copy of this document is [available in this repository][coc].\n\n[coc]: CODE_OF_CONDUCT.md\n\n### Contact information\n\nThe current code owner of this package is Karoy Lorentey ([@lorentey](https://github.com/lorentey)). You can contact him [on the Swift forums](https://forums.swift.org/u/lorentey/summary), or by writing an email to klorentey at apple dot com. (Please keep it related to this project.)\n\nIn case of moderation issues, you can also directly contact a member of the [Swift Core Team](https://swift.org/community/#community-structure).\n"
 },
 {
  "repo": "apple/coremltools",
  "language": "Python",
  "readme_contents": "[![Build Status](https://img.shields.io/gitlab/pipeline/zach_nation/coremltools/master)](https://gitlab.com/zach_nation/coremltools/-/pipelines?page=1&scope=branches&ref=master)\n[![PyPI Release](https://img.shields.io/pypi/v/coremltools.svg)](#)\n[![Python Versions](https://img.shields.io/pypi/pyversions/coremltools.svg)](#)\n\n[Core ML Tools](https://coremltools.readme.io/docs)\n=======================\n\nUse *coremltools* to convert machine learning models from third-party libraries to the Core ML format. The Python package contains the supporting tools for converting models from training libraries such as the following:\n\n* [TensorFlow 1.x](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf)\n* [TensorFlow 2.x](https://www.tensorflow.org/api_docs)\n* [PyTorch](https://pytorch.org/)\n* [TensorFlow's Keras APIs](https://keras.io/)\n* Non-neural network frameworks:\n\t* [scikit-learn](https://scikit-learn.org/stable/)\n\t* [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n\t* [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n\nWith coremltools, you can do the following:\n\n* Convert trained models to the Core ML format.\n* Read, write, and optimize Core ML models.\n* Verify conversion/creation (on macOS) by making predictions using Core ML.\n\nAfter conversion, you can integrate the Core ML models with your app using Xcode.\n\n## Version 5\n\nThe coremltools 5 package offers several performance improvements over previous versions, including the following new features: \n\n* [Core ML model package](https://coremltools.readme.io/docs/new-in-coremltools#save-a-core-ml-model-package): A new model container format that separates the model into components and offers more flexible metadata editing and better source control.\n* [ML program](https://coremltools.readme.io/docs/ml-programs): A new model type that represents computation as programmatic instructions, offers more control over the precision of its intermediate tensors and better performance. \n\nTo install coremltools, use the following command:\n\n```shell\npip install coremltools\n```\n\n\n## Core ML\n\n[Core ML](https://developer.apple.com/documentation/coreml) is an Apple framework to integrate machine learning models into your app. Core ML provides a unified representation for all models. Your app uses Core ML APIs and user data to make predictions, and to fine-tune models, all on the user\u2019s device. Core ML optimizes on-device performance by leveraging the CPU, GPU, and Neural Engine while minimizing its memory footprint and power consumption. Running a model strictly on the user\u2019s device removes any need for a network connection, which helps keep the user\u2019s data private and your app responsive.\n\n## Resources\n\nTo install coremltools, see the [\u201cInstallation\u201c page](https://coremltools.readme.io/docs/installation). For more information, see the following:\n\n* [Release Notes](https://github.com/apple/coremltools/releases/) \n* [Guides and examples](https://coremltools.readme.io/) \n* [API Reference](https://apple.github.io/coremltools/index.html)\n* [Core ML Specification](https://apple.github.io/coremltools/mlmodel/index.html)\n* [Building from Source](BUILDING.md)\n* [Contribution Guidelines](CONTRIBUTING.md) \n\n\n"
 },
 {
  "repo": "apple/swift-atomics",
  "language": "Swift",
  "readme_contents": "# Swift Atomics \u269b\ufe0e\ufe0e\n\n[SE-0282]: https://github.com/apple/swift-evolution/blob/master/proposals/0282-atomics.md\n[SE-0282r0]: https://github.com/apple/swift-evolution/blob/3a358a07e878a58bec256639d2beb48461fc3177/proposals/0282-atomics.md\n\nThis package implements an atomics library for Swift, providing atomic operations for a variety of Swift types, including integers and pointer values. The goal is to enable intrepid developers to start building synchronization constructs directly in Swift. \n\nAtomic operations aren't subject to the usual exclusivity rules. The same memory location may be safely read and updated from multiple concurrent threads of execution, as long as all such access is done through atomic operations. For example, here is a trivial atomic counter:\n\n``` swift\nimport Atomics\nimport Dispatch\n\nlet counter = ManagedAtomic<Int>(0)\n\nDispatchQueue.concurrentPerform(iterations: 10) { _ in\n  for _ in 0 ..< 1_000_000 {\n    counter.wrappingIncrement(ordering: .relaxed)\n  }\n}\ncounter.load(ordering: .relaxed) // \u27f9 10_000_000\n```\n\nThe only way to access the counter value is to use one of the methods provided by `ManagedAtomic`, each of which implement a particular atomic operation, and each of which require an explicit ordering value. (Swift supports a subset of the C/C++ memory orderings.) \n\n## Table of Contents\n\n  * [Getting Started](#getting-started)\n  * [Features](#features)\n    * [Lock\\-Free vs Wait\\-Free Operations](#lock-free-vs-wait-free-operations)\n    * [Portability Concerns](#portability-concerns)\n    * [Memory Management](#memory-management)\n  * [Source Stability](#source-stability)\n  * [Contributing to Swift Atomics](#contributing-to-swift-atomics)\n  * [Development](#development)\n\n\n## Getting Started\n\nTo use `Atomics` in your own project, you need to set it up as a package dependency:\n\n```swift\n// swift-tools-version:5.3\nimport PackageDescription\n\nlet package = Package(\n  name: \"MyPackage\",\n  dependencies: [\n    .package(\n      url: \"https://github.com/apple/swift-atomics.git\", \n      .upToNextMajor(from: \"1.0.0\") // or `.upToNextMinor\n    )\n  ],\n  targets: [\n    .target(\n      name: \"MyTarget\",\n      dependencies: [\n        .product(name: \"Atomics\", package: \"swift-atomics\")\n      ]\n    )\n  ]\n)\n```\n\n## Features\n\nThe package implements atomic operations for the following Swift constructs, all of which conform to the public `AtomicValue` protocol:\n\n- Standard signed integer types (`Int`, `Int64`, `Int32`, `Int16`, `Int8`)\n- Standard unsigned integer types (`UInt`, `UInt64`, `UInt32`, `UInt16`, `UInt8`)\n- Booleans (`Bool`)\n- Standard pointer types (`UnsafeRawPointer`, `UnsafeMutableRawPointer`, `UnsafePointer<T>`, `UnsafeMutablePointer<T>`), along with their optional-wrapped forms (such as `Optional<UnsafePointer<T>>`)\n- Unmanaged references (`Unmanaged<T>`, `Optional<Unmanaged<T>>`)\n- A special `DoubleWord` type that consists of two `UInt` values, `low` and `high`, providing double-wide atomic primitives\n- Any `RawRepresentable` type whose `RawValue` is in turn an atomic type (such as simple custom enum types)\n- Strong references to class instances that opted into atomic use (by conforming to the `AtomicReference` protocol)\n\nOf particular note is full support for atomic strong references. This provides a convenient memory reclamation solution for concurrent data structures that fits perfectly with Swift's reference counting memory management model. (Atomic strong references are implemented in terms of `DoubleWord` operations.) However, accessing an atomic strong reference is (relatively) expensive, so we also provide a separate set of efficient constructs (`ManagedAtomicLazyReference` and `UnsafeAtomicLazyReference`) for the common case of a lazily initialized (but otherwise constant) atomic strong reference.\n\n### Lock-Free vs Wait-Free Operations\n\nAll atomic operations exposed by this package are guaranteed to have lock-free implementations. However, we do not guarantee wait-free operation -- depending on the capabilities of the target platform, some of the exposed operations may be implemented by compare-and-exchange loops. That said, all atomic operations map directly to dedicated CPU instructions where available -- to the extent supported by llvm & Clang.\n\n### Portability Concerns\n\nLock-free double-wide atomics requires support for such things from the underlying target platform. Where such support isn't available, this package doesn't implement `DoubleWord` atomics or atomic strong references. While modern multiprocessing CPUs have been providing double-wide atomic instructions for a number of years now, some platforms still target older architectures by default; these require a special compiler option to enable double-wide atomic instructions. This currently includes Linux operating systems running on x86_64 processors, where the `cmpxchg16b` instruction isn't considered a baseline requirement.\n\nTo enable double-wide atomics on Linux/x86_64, you currently have to manually supply a couple of additional options on the SPM build invocation:\n\n```\n$ swift build -Xcc -mcx16 -Xswiftc -DENABLE_DOUBLEWIDE_ATOMICS -c release\n```\n\n(`-mcx16` turns on support for `cmpxchg16b` in Clang, and `-DENABLE_DOUBLEWIDE_ATOMICS` makes Swift aware that double-wide atomics are available. Note that the resulting binaries won't run on some older AMD64 CPUs.)\n\nThe package cannot currently configure this automatically.\n\n### Memory Management\n\nAtomic access is implemented in terms of dedicated atomic storage representations that are kept distinct from the corresponding regular (non-atomic) type. (E.g., the actual integer value underlying the counter above isn't directly accessible.) This has several advantages:\n\n- it helps prevent accidental non-atomic access to atomic variables,\n- it enables custom storage representations (such as the one used by atomic strong references), and\n- it is a better fit with the standard C atomics library that we use to implement the actual operations (as enabled by [SE-0282]).\n\n[SE-0282]: https://github.com/apple/swift-evolution/blob/master/proposals/0282-atomics.md\n\nWhile the underlying pointer-based atomic operations are exposed as static methods on the corresponding `AtomicStorage` types, we strongly recommend the use of higher-level atomic wrappers to manage the details of preparing/disposing atomic storage. This version of the library provides two wrapper types:\n\n- an easy to use, memory-safe `ManagedAtomic<T>` generic class and\n- a less convenient, but more flexible `UnsafeAtomic<T>` generic struct.\n\nBoth constructs provide the following operations on all `AtomicValue` types:\n\n```swift\nfunc load(ordering: AtomicLoadOrdering) -> Value\nfunc store(_ desired: Value, ordering: AtomicStoreOrdering)\nfunc exchange(_ desired: Value, ordering: AtomicUpdateOrdering) -> Value\n\nfunc compareExchange(\n    expected: Value,\n    desired: Value,\n    ordering: AtomicUpdateOrdering\n) -> (exchanged: Bool, original: Value)\n\nfunc compareExchange(\n    expected: Value,\n    desired: Value,\n    successOrdering: AtomicUpdateOrdering,\n    failureOrdering: AtomicLoadOrdering\n) -> (exchanged: Bool, original: Value)\n\nfunc weakCompareExchange(\n    expected: Value,\n    desired: Value,\n    successOrdering: AtomicUpdateOrdering,\n    failureOrdering: AtomicLoadOrdering\n) -> (exchanged: Bool, original: Value)\n```\n\nInteger types come with additional atomic operations for incrementing or decrementing values and bitwise logical operations. `Bool` provides select additional boolean operations along the same vein.\n\nFor an introduction to the APIs provided by this package, for now please see the [first version of SE-0282][SE-0282r0]. \n\nNote that when/if Swift gains support for non-copiable types, we expect to replace both `ManagedAtomic` and `UnsafeAtomic` with a single move-only atomic struct that combines the performance and versatility of `UnsafeAtomic` with the ease-of-use and memory safety of `ManagedAtomic`.\n\nThe current version of the `Atomics` module does not implement APIs for tagged atomics (see [issue #1](https://github.com/apple/swift-atomics/issues/1)), although it does expose a `DoubleWord` type that can be used to implement them. (Atomic strong references are already implemented in terms of `DoubleWord`, although in their current form they do not expose any user-customizable bits.)\n\n## Source Stability\n\nThe Swift Atomics package is source stable. The version numbers follow [Semantic Versioning][semver] -- source breaking changes to public API can only land in a new major version.\n\n[semver]: https://semver.org\n\nThe public API of version 1.0 of the `swift-atomics` package consists of non-underscored declarations that are marked `public` in the `Atomics` module.\n\nBy \"underscored declarations\" we mean declarations that have a leading underscore anywhere in their fully qualified name. For instance, here are some names that wouldn't be considered part of the public API, even if they were technically marked public:\n\n- `FooModule.Bar._someMember(value:)` (underscored member)\n- `FooModule._Bar.someMember` (underscored type)\n- `_FooModule.Bar` (underscored module)\n- `FooModule.Bar.init(_value:)` (underscored initializer)\n\nInterfaces that aren't part of the public API may continue to change in any release, including patch releases. \n\nNote that contents of the `_AtomicsShims` module explicitly aren't public API. (As implied by its underscored module name.) The definitions therein may therefore change at whim, and the entire module may be removed in any new release -- do not import this module directly. We also don't make any source compatibility promises about the contents of the `Utilities` and `Tests` subdirectories.\n\nIf you have a use case that requires using underscored APIs, please [submit a Feature Request][enhancement] describing it! We'd like the public interface to be as useful as possible -- although preferably without compromising safety or limiting future evolution.\n\nFuture minor versions of the package may introduce changes to these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate. Accordingly, from time to time, we expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release. (This allows the package to make use of new language/stdlib features, build on compiler bug fixes, and adopt new package manager functionality as soon as they are available.)\n\nRequiring a new Swift release will only require a minor version bump.\n\n## Contributing to Swift Atomics\n\nSwift Atomics is a standalone library separate from the core Swift project. We expect some of the atomics APIs may eventually get incorporated into the Swift Standard Library. If and when that happens such changes will be proposed to the Swift Standard Library using the established evolution process of the Swift project.\n\nThis library is licensed under the [Swift License]. For more information, see the Swift.org [Community Guidelines], [Contribution Guidelines], as well as the files [LICENSE.txt](./LICENSE.txt), [CONTRIBUTING.md](./CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) at the root of this repository.\n\nSwift Atomics uses GitHub issues to track bugs and enhancement requests. We use pull requests for development.\n\n[Swift License]: https://swift.org/LICENSE.txt\n[Community Guidelines]: https://swift.org/community/\n[Contribution Guidelines]: https://swift.org/contributing/\n\nWe have a dedicated [Swift Atomics Forum][forum] where people can ask and answer questions on how to use or work on this package. It's also a great place to discuss its evolution.\n\n[forum]: https://forums.swift.org/c/related-projects/swift-atomics\n\nIf you find something that looks like a bug, please open a [Bug Report][bugreport]! Fill out as many details as you can.\n\nTo fix a small issue or make a tiny improvement, simply [submit a PR][PR] with the changes you want to make. If there is an [existing issue][issues] for the bug you're fixing, please include a reference to it. Make sure to add tests covering whatever changes you are making.\n\n[PR]: https://github.com/apple/swift-atomics/compare\n[issues]: https://github.com/apple/swift-atomics/issues\n[bugreport]: https://github.com/apple/swift-atomics/issues/new?assignees=&labels=bug&template=BUG_REPORT.md\n\nFor larger feature additions, it's a good idea to discuss your idea in a new [Feature Request][enhancement] or on the [forum] before starting to work on it. If the discussions indicate the feature would be desirable, submit the implementation in a PR, and participate in its review discussion.\n\n[enhancement]: https://github.com/apple/swift-atomics/issues/new?assignees=&labels=enhancement&template=FEATURE_REQUEST.md\n\n\n## Development\n\nThis package defines a large number of similar-but-not-quite-the-same operations. To make it easier to maintain these, we use code generation to produce them.\n\nA number of [source files](./Sources/Atomics) have a `.swift.gyb` extension. These are using a Python-based code generation utility called [gyb](./Utilities/gyb.py) which we also use within the Swift Standard Library (the name is short for Generate Your Boilerplate). To make sure the package remains buildable by SPM, the autogenerated output files are committed into this repository. You should not edit the contents of `autogenerated` subdirectories, or your changes will get overwritten the next time the code is regenerated.\n\nTo regenerate sources (and to update the inventory of XCTest tests), you need to manually run the script [`generate-sources.sh`](./Utilities/generate-sources.sh) in the Utilities folder of this repository. This needs to be done every time you modify one of the template files.\n\nThe same script also runs `swift test --generate-linuxmain` to register any newly added unit tests.\n\nIn addition to gyb, the [`_AtomicsShims.h`](./Sources/_AtomicsShims/include/_AtomicsShims.h) header file uses the C preprocessor to define trivial wrapper functions for every supported atomic operation -- memory ordering pairing.\n\n\u269b\ufe0e\ufe0e\n\n<!-- Local Variables: -->\n<!-- mode: markdown -->\n<!-- fill-column: 10000 -->\n<!-- eval: (setq-local whitespace-style '(face tabs newline empty)) -->\n<!-- eval: (whitespace-mode 1) -->\n<!-- eval: (visual-line-mode 1) -->\n<!-- End: -->\n"
 },
 {
  "repo": "apple/swift-log",
  "language": "Swift",
  "readme_contents": "# SwiftLog\n\nFirst things first: This is the beginning of a community-driven open-source project actively seeking contributions, be it code, documentation, or ideas. Apart from contributing to `SwiftLog` itself, there's another huge gap at the moment: `SwiftLog` is an _API package_ which tries to establish a common API the ecosystem can use. To make logging really work for real-world workloads, we need `SwiftLog`-compatible _logging backends_ which then either persist the log messages in files, render them in nicer colors on the terminal, or send them over to Splunk or ELK.\n\nWhat `SwiftLog` provides today can be found in the [API docs][api-docs].\n\n## Getting started\n\nIf you have a server-side Swift application, or maybe a cross-platform (for example Linux & macOS) app/library, and you would like to log, we think targeting this logging API package is a great idea. Below you'll find all you need to know to get started.\n\n#### Adding the dependency\n\n`SwiftLog` is designed for Swift 5. To depend on the logging API package, you need to declare your dependency in your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-log.git\", from: \"1.0.0\"),\n```\n\nand to your application/library target, add `\"Logging\"` to your `dependencies`, e.g. like this:\n\n```swift\n// Target syntax for Swift up to version 5.1\n.target(name: \"BestExampleApp\", dependencies: [\"Logging\"]),\n\n// Target for Swift 5.2\n.target(name: \"BestExampleApp\", dependencies: [\n    .product(name: \"Logging\", package: \"swift-log\")\n],\n```\n\n\n#### Let's log\n\n```swift\n// 1) let's import the logging API package\nimport Logging\n\n// 2) we need to create a logger, the label works similarly to a DispatchQueue label\nlet logger = Logger(label: \"com.example.BestExampleApp.main\")\n\n// 3) we're now ready to use it\nlogger.info(\"Hello World!\")\n```\n\n#### Output\n\n```\n2019-03-13T15:46:38+0000 info: Hello World!\n```\n\n#### Default `Logger` behavior\n\n`SwiftLog` provides for very basic console logging out-of-the-box by way of `StreamLogHandler`. It is possible to switch the default output to `stderr` like so:\n```swift\nLoggingSystem.bootstrap(StreamLogHandler.standardError)\n```\n\n`StreamLogHandler` is primarily a convenience only and does not provide any substantial customization. Library maintainers who aim to build their own logging backends for integration and consumption should implement the `LogHandler` protocol directly as laid out in [the \"On the implementation of a logging backend\" section](#on-the-implementation-of-a-logging-backend-a-loghandler).\n\nFor further information, please check the [API documentation][api-docs].\n\n<a name=\"backends\"></a>\n#### Selecting a logging backend implementation (applications only)\n\nAs the API has just launched, not many implementations exist yet. If you are interested in implementing one see the \"Implementation considerations\" section below explaining how to do so. List of existing SwiftLog API compatible libraries:\n\n| Repository | Handler Description|\n| ----------- | ----------- |\n| [Kitura/HeliumLogger](https://github.com/Kitura/HeliumLogger)  |a logging backend widely used in the Kitura ecosystem |\n| [ianpartridge/swift-log-**syslog**](https://github.com/ianpartridge/swift-log-syslog) | a [syslog](https://en.wikipedia.org/wiki/Syslog) backend|\n| [Adorkable/swift-log-**format-and-pipe**](https://github.com/Adorkable/swift-log-format-and-pipe) | a backend that allows customization of the output format and the resulting destination |\n| [chrisaljoudi/swift-log-**oslog**](https://github.com/chrisaljoudi/swift-log-oslog) | an OSLog [Unified Logging](https://developer.apple.com/documentation/os/logging) backend for use on Apple platforms. **Important Note:** we recommend using os_log directly as decribed [here](https://developer.apple.com/documentation/os/logging). Using os_log through swift-log using this backend will be less efficient and will also prevent specifying the privacy of the message. The backend always uses `%{public}@` as the format string and eagerly converts all string interpolations to strings.  This has two drawbacks: 1. the static components of the string interpolation would be eagerly copied by the unified logging system, which will result in loss of performance. 2. It makes all messages public, which changes the default privacy policy of os_log, and doesn't allow specifying fine-grained privacy of sections of the message.  In a separate on-going work, Swift APIs for os_log are being improved and made to align closely with swift-log APIs. References: [Unifying Logging Levels](https://forums.swift.org/t/custom-string-interpolation-and-compile-time-interpretation-applied-to-logging/18799), [Making os_log accept string interpolations using compile-time interpretation](https://forums.swift.org/t/logging-levels-for-swifts-server-side-logging-apis-and-new-os-log-apis/20365). |\n| [Brainfinance/StackdriverLogging](https://github.com/Brainfinance/StackdriverLogging) | a structured JSON logging backend for use on Google Cloud Platform with the [Stackdriver logging agent](https://cloud.google.com/logging/docs/agent) |\n| [DnV1eX/GoogleCloudLogging](https://github.com/DnV1eX/GoogleCloudLogging) | a client-side library for logging application events in [Google Cloud](https://console.cloud.google.com/logs) via REST API v2. |\n| [vapor/console-kit](https://github.com/vapor/console-kit/) | print log messages to a terminal with stylized ([ANSI](https://en.wikipedia.org/wiki/ANSI_escape_code)) output |\n| [neallester/swift-log-testing](https://github.com/neallester/swift-log-testing) | provides access to log messages for use in assertions (within test targets) |\n| [wlisac/swift-log-slack](https://github.com/wlisac/swift-log-slack)  | a logging backend that sends critical log messages to Slack |\n| [NSHipster/swift-log-github-actions](https://github.com/NSHipster/swift-log-github-actions) | a logging backend that translates logging messages into [workflow commands for GitHub Actions](https://help.github.com/en/actions/reference/workflow-commands-for-github-actions). |\n| [stevapple/swift-log-telegram](https://github.com/stevapple/swift-log-telegram) | a logging backend that sends log messages to any Telegram chat (Inspired by and forked from [wlisac/swift-log-slack](https://github.com/wlisac/swift-log-slack)) |\n| [jagreenwood/swift-log-datadog](https://github.com/jagreenwood/swift-log-datadog)  | a logging backend which sends log messages to the [Datadog](https://www.datadoghq.com/log-management/) log management service |\n| [google/SwiftLogFireCloud](https://github.com/google/swiftlogfirecloud)  | a logging backend for time series logging which pushes logs as flat files to Firebase Cloud Storage. |\n| [crspybits/swift-log-file](https://github.com/crspybits/swift-log-file)  | a simple local file logger (using `Foundation` `FileManager`) |\n| [sushichop/Puppy](https://github.com/sushichop/Puppy) | a logging backend that supports multiple transports(console, file, syslog, etc.) and has the feature with formatting and file log rotation |\n| [luoxiu/LogDog](https://github.com/luoxiu/LogDog) | user-friendly logging with sinks and appenders |\n| [ShivaHuang/swift-log-SwiftyBeaver](https://github.com/ShivaHuang/swift-log-SwiftyBeaver) | a logging backend for printing colored logging to Xcode console / file, or sending encrypted logging to [SwiftyBeaver](https://swiftybeaver.com) platform. |\n| [Apodini/swift-log-elk](https://github.com/Apodini/swift-log-elk) | a logging backend that formats, caches and sends log data to [elastic/logstash](https://github.com/elastic/logstash) |\n| Your library? | [Get in touch!](https://forums.swift.org/c/server) |\n\n## What is an API package?\n\nGlad you asked. We believe that for the Swift on Server ecosystem, it's crucial to have a logging API that can be adopted by anybody so a multitude of libraries from different parties can all log to a shared destination. More concretely this means that we believe all the log messages from all libraries end up in the same file, database, Elastic Stack/Splunk instance, or whatever you may choose.\n\nIn the real-world however, there are so many opinions over how exactly a logging system should behave, what a log message should be formatted like, and where/how it should be persisted. We think it's not feasible to wait for one logging package to support everything that a specific deployment needs whilst still being easy enough to use and remain performant. That's why we decided to cut the problem in half:\n\n1. a logging API\n2. a logging backend implementation\n\nThis package only provides the logging API itself and therefore `SwiftLog` is a 'logging API package'. `SwiftLog` (using `LoggingSystem.bootstrap`) can be configured to choose any compatible logging backend implementation. This way packages can adopt the API and the _application_ can choose any compatible logging backend implementation without requiring any changes from any of the libraries.\n\nJust for completeness sake: This API package does actually include an overly simplistic and non-configurable logging backend implementation which simply writes all log messages to `stdout`. The reason to include this overly simplistic logging backend implementation is to improve the first-time usage experience. Let's assume you start a project and try out `SwiftLog` for the first time, it's just a whole lot better to see something you logged appear on `stdout` in a simplistic format rather than nothing happening at all. For any real-world application, we advise configuring another logging backend implementation that logs in the style you like.\n\n## The core concepts\n\n### Loggers\n\n`Logger`s are used to emit log messages and therefore the most important type in `SwiftLog`, so their use should be as simple as possible.  Most commonly, they are used to emit log messages in a certain log level. For example:\n\n```swift\n// logging an informational message\nlogger.info(\"Hello World!\")\n\n// ouch, something went wrong\nlogger.error(\"Houston, we have a problem: \\(problem)\")\n```\n\n### Log levels\n\nThe following log levels are supported:\n\n - `trace`\n - `debug`\n - `info`\n - `notice`\n - `warning`\n - `error`\n - `critical`\n\nThe log level of a given logger can be changed, but the change will only affect the specific logger you changed it on. You could say the `Logger` is a _value type_ regarding the log level.\n\n\n### Logging metadata\n\nLogging metadata is metadata that can be attached to loggers to add information that is crucial when debugging a problem. In servers, the usual example is attaching a request UUID to a logger that will then be present on all log messages logged with that logger. Example:\n\n```swift\nvar logger = logger\nlogger[metadataKey: \"request-uuid\"] = \"\\(UUID())\"\nlogger.info(\"hello world\")\n```\n\nwill print\n\n```\n2019-03-13T18:30:02+0000 info: request-uuid=F8633013-3DD8-481C-9256-B296E43443ED hello world\n```\n\nwith the default logging backend implementation that ships with `SwiftLog`. Needless to say, the format is fully defined by the logging backend you choose.\n\n## On the implementation of a logging backend (a `LogHandler`)\n\nNote: If you don't want to implement a custom logging backend, everything in this section is probably not very relevant, so please feel free to skip.\n\nTo become a compatible logging backend that all `SwiftLog` consumers can use, you need to do two things: 1) Implement a type (usually a `struct`) that implements `LogHandler`, a protocol provided by `SwiftLog` and 2) instruct `SwiftLog` to use your logging backend implementation.\n\nA `LogHandler` or logging backend implementation is anything that conforms to the following protocol\n\n```swift\npublic protocol LogHandler {\n    func log(level: Logger.Level, message: Logger.Message, metadata: Logger.Metadata?, file: String, function: String, line: UInt)\n\n    subscript(metadataKey _: String) -> Logger.Metadata.Value? { get set }\n\n    var metadata: Logger.Metadata { get set }\n\n    var logLevel: Logger.Level { get set }\n}\n```\n\nInstructing `SwiftLog` to use your logging backend as the one the whole application (including all libraries) should use is very simple:\n\n    LoggingSystem.bootstrap(MyLogHandler.init)\n\n### Implementation considerations\n\n`LogHandler`s control most parts of the logging system:\n\n#### Under control of a `LogHandler`\n\n##### Configuration\n\n`LogHandler`s control the two crucial pieces of `Logger` configuration, namely:\n\n- log level (`logger.logLevel` property)\n- logging metadata (`logger[metadataKey:]` and `logger.metadata`)\n\nFor the system to work, however, it is important that `LogHandler` treat the configuration as _value types_. This means that `LogHandler`s should be `struct`s and a change in log level or logging metadata should only affect the very `LogHandler` it was changed on.\n\nHowever, in special cases, it is acceptable that a `LogHandler` provides some global log level override that may affect all `LogHandler`s created.\n\n##### Emitting\n- emitting the log message itself\n\n### Not under control of `LogHandler`s\n\n`LogHandler`s do not control if a message should be logged or not. `Logger` will only invoke the `log` function of a `LogHandler` if `Logger` determines that a log message should be emitted given the configured log level.\n\n## Source vs Label\n\nA `Logger` carries an (immutable) `label` and each log message carries a `source` parameter (since SwiftLog 1.3.0). The `Logger`'s label\nidentifies the creator of the `Logger`. If you are using structured logging by preserving metadata across multiple modules, the `Logger`'s\n`label` is not a good way to identify where a log message originated from as it identifies the creator of a `Logger` which is often passed\naround between libraries to preserve metadata and the like.\n\nIf you want to filter all log messages originating from a certain subsystem, filter by `source` which defaults to the module that is emitting the\nlog message.\n\n## Security\n\nPlease see [SECURITY.md](SECURITY.md) for SwiftLog's security process.\n\n## Design\n\nThis logging API was designed with the contributors to the Swift on Server community and approved by the [SSWG (Swift Server Work Group)](https://swift.org/server/) to the 'sandbox level' of the SSWG's [incubation process](https://github.com/swift-server/sswg/blob/master/process/incubation.md).\n\n- [pitch](https://forums.swift.org/t/logging/16027), [discussion](https://forums.swift.org/t/discussion-server-logging-api/18834), [feedback](https://forums.swift.org/t/feedback-server-logging-api-with-revisions/19375)\n- [log levels](https://forums.swift.org/t/logging-levels-for-swifts-server-side-logging-apis-and-new-os-log-apis/20365)\n\n[api-docs]: https://apple.github.io/swift-log/docs/current/Logging/Structs/Logger.html\n"
 },
 {
  "repo": "apple/swift-distributed-actors",
  "language": "Swift",
  "readme_contents": "\n# Swift Distributed Actors\n\nPeer-to-peer server-side focused clustering transport implementation for Swift Distributed Actors.\n\n> **NOTE:** This is a work in progress, early preview project. All APIs may (and will) change. Please read more about our plans the introduction below.\n\n* [Introduction](#introduction)\n* [Development](#development)\n\n## Introduction\n\n### What are Distributed Actors?\n\nDistributed actors are an early and _experimental language feature_ with which we aim to simplify and push the state-of-the-art of distributed systems programming in Swift, in the same way we did with concurrent programming with local actors and Swift's structured concurrency approach embedded in the language.\n\nCurrently we are iterating on the design of distributed actors, and are looking to gather your feedback, use-cases, and general ideas in the proposal\u2019s [pitch thread](https://forums.swift.org/t/pitch-distributed-actors/51669/111), as well as the [Distributed Actors category](https://forums.swift.org/c/server/distributed-actors/79) on the Swift forums. The library and language features described in the proposal, and this blog post are available in [nightly toolchains](https://swift.org/download/#snapshots), so please feel free to download them and get a feel for the feature. We are going to be posting updated proposals and other discussion threads on the forums, so if you are interested, please follow the respective category and threads on the Swift forums.\n\nWe are most interested in general feedback, thoughts about use-cases, and potential transport implementations you would be interested in taking on. As we mature and design the language feature, the library (introduced below), will be following along serve as the _reference implementation_ of one such advanced and powerful actor transport. If you are interested in distributed systems, [contributions to the library](https://github.com/apple/swift-distributed-actors/) itself are also very welcome, and there is [much to be done](https://github.com/apple/swift-distributed-actors/issues) there as well!\n\nIn the near future, we will also provide a more complete \u201creference guide\u201d, examples and and article-style guides written using in the [recently open sourced DocC](https://swift.org/blog/swift-docc/) documentation compiler, which in addition to the API documentation available today will teach about the specific patterns and use-cases this library enables.\n\nThese proposed language features\u2013as all language features\u2013will go through a proper [Swift Evolution process](https://github.com/apple/swift-evolution/blob/main/process.md) before lifting their experimental status. We invite the community to participate and help us shape the language and APIs through review, contributions and sharing experiences. Thank you very much in advance!\n\n\n> This project is released as \"early preview\" and all of its APIs are subject to change, or even removal without any prior warning.\n\nThe library depends on un-released, work-in-progress, and Swift Evolution review pending language features, and as such we cannot recommend using it in production just yet \u2014 the library may depend on specific nightly builds of toolchains etc.\n\nThe primary purpose of open sourcing this library early is proving the ability to implement a feature complete, compelling clustering solution using the `distributed actor` language feature, and co-evolving the two in tandem.\n\n### Introduction: Distributed Actors\n\nDistributed actors are the next step in the evolution of Swift's concurrency model.\n\nWith actors built-into the language, Swift offers developers a safe and intuitive concurrency model that is a great fit for many kinds of applications. Thanks to advanced semantic checks, the compiler is able to guide and help developers write programs which are free from low-level data races. This isn't where the usefulness of the actor model ends though: unlike other concurrency models, the actor model is also tremendously useful in modeling distributed systems, where thanks to the notion of _location transparent_ distributed actors, we can program distributed systems using the familiar notion of actors, and then easily move it to a distributed, e.g. clustered, environment.\n\nWith distributed actors we aim to simplify and push the state of the art of distributed systems programming, the same way we did with concurrent programming with local actors and Swift's structured concurrency models embedded in the language.\n\nThis abstraction is not intended to completely hide away the fact that distributed calls are crossing the network though. In a way, we are doing the opposite, and programming with the assumption that calls *may* be remote. This small, yet crucial, observation allows us to build systems primarily intended for distribution, but that are also testable in local test clusters which may even efficiently simulate various error scenarios.\n\nDistributed actors are similar to (local) actors in the sense that they encapsulate their state, and may only be communicated with through asynchronous calls. The distributed aspect adds to that equation some additional isolation, type system and runtime considerations, however the surface of the feature feels very similar to local actors. Here is a small example of a distributed actor declaration:\n\n\n~~~swift\n// **** APIS AND SYNTAX ARE WORK IN PROGRESS / PENDING SWIFT EVOLUTION ****\n// 1) Actors may be declared with the new 'distributed' modifier\ndistributed actor Worker {\n\n  // 2) An actor's isolated state is only stored on the node where the actor lives.\n  //    Actor Isolation rules ensure that programs only access isolated state in\n  //    correct ways, i.e. in a thread-safe manner, and only when the state is\n  //    known to exist.\n  var data: SomeData\n\n  // 3) Only functions (and computed properties) declared as 'distributed' may be accessed cross actor.\n  //    Distributed function parameters and return types must be Codable,\n  //    because they will be crossing network boundaries during remote calls.\n  distributed func work(item: String) -> WorkItem.Result {\n    // ...\n  }\n}\n~~~\n\nDistributed actors take away a lot of the boilerplate that we'd normally have to build and re-invent every time we build some distributed RPC system. After all, nowhere in this snippet did we have to care about exact serialization and networking details, we just declare what we need to get done - send work requests across the network! This is quite powerful, and we hope you'll enjoy using actors in this capacity, in addition to their concurrency aspect.\n\nTo actually have a distributed actor participate in some distributed system, we must provide it with an `ActorTransport`, which is a user-implementable library component, responsible for performing all the networking necessary to make remote function calls. Developers provide their transport of choice during the instantiation of a distributed actor, like this:\n\n\n~~~swift\n// **** APIS AND SYNTAX ARE WORK IN PROGRESS / PENDING SWIFT EVOLUTION ****\n\n// 4) Distributed actors must have a transport associated with them at initialization\nlet someTransport: ActorTransport = ...\nlet worker = Worker(transport: someTransport)\n\n// 5) Distributed function invocations are asynchronous and throwing, when performed cross-actor,\n//    because of the potential network interactions of such call.\n//\n//    These effects are applied to such functions implicitly, only in contexts where necessary,\n//    for example: when it is known that the target actor is local, the implicit-throwing effect\n//    is not applied to such call.\n_ = try await worker.work(item: \"work-item-32\")\n\n// 6) Remote systems may obtain references to the actor by using the 'resolve' function.\n//    It returns a special \"proxy\" object, that transforms all distributed function calls into messages.\nlet result = try await Worker.resolve(worker.id, using: otherTransport)\n~~~\n\nThis summarizes the distributed actor feature at a very high level. We encourage those interested to read the full proposal available in [Swift Evolution](https://github.com/apple/swift-evolution/pulls?q=is%3Apr+is%3Aopen+distributed), and provide feedback or ask questions in the [Distributed Actors category on the Swift Forums](https://forums.swift.org/c/server/distributed-actors/79).\n\nYou can follow along and provide input on the `distributed actor` language proposal on the Swift forums and [Swift Evolution](https://github.com/apple/swift-evolution/pulls?q=is%3Apr+is%3Aopen+distributed). The [full current draft](https://github.com/apple/swift-evolution/pull/1433) of the language proposal is also available for review, though we expect to make significant changes to it in the near future.\n\nWe would love to hear your feedback and see you participate in the Swift Evolution reviews of this exciting new feature!\n\n\n## Development\n\n### Developing with nightly toolchains\n\nThis library depends on in-progress work in the Swift language itself.\nAs such, it is necessary to download and use nightly built toolchains to develop and use this library until the `distributed actor` language feature becomes a stable released part of the language.\n\n**Obtaining a nightly toolchain**\n\nDistributed actors require \"latest\" nightly toolchains to build correctly.\n\nAt this point in time, the **2021-11-02 nightly toolchain** is sufficient to build the project.\nYou can download it from [https://swift.org/download/](https://swift.org/download/).\n\n```\n# Export the toolchain (nightly snapshot or pull-request generated toolchain), e.g.:\n\nexport TOOLCHAIN=/Library/Developer/Toolchains/swift-DEVELOPMENT-SNAPSHOT-2021-11-02-a.xctoolchain\n\n# Just build the project\n$TOOLCHAIN/usr/bin/swift build --build-tests\n```\n\n#### Running tests & configuring `DYLD_LIBRARY_PATH`\n\nIt is a known limitation of the toolchains that one has to export the `DYLD_LIBRARY_PATH` environment variable \nwith the path to where the `TOOLCHAIN` stores the _Distributed library.\n\nIt is possible to `swift build` the project without passing additional environment variables, however in order to run anything, we must provide the location of the `_Distributed` library *in the custom toolchain* to the dynamic linker as we start the binary.\n\nTo do this on macOS, you have to use the `DYLD_LIBRARY_PATH`. For security reasons, unless you have System Integrity Protection turned off, this environment variable is automatically stripped out when a process creates a child process. This is why we need to invoke the _specific_ `xctest` binary, rather than leave it to SwiftPM to handle.\n\n**Plain `swift test` invocations are expected to fail like this:**\n\n```\n-> % swift test --filter Receptionist\n...\nerror: signalled(6): /Library/Developer/Toolchains/swift-DEVELOPMENT-SNAPSHOT-2021-10-26-a.xctoolchain/usr/libexec/swift/pm/swiftpm-xctest-helper /Users/ktoso/code/swift-distributed-actors/.build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest /var/folders/w1/hmg_v8p532d800g08jtqtddc0000gn/T/TemporaryFile.cfl1uX output:\n    dyld[72407]: Library not loaded: @rpath/XCTest.framework/Versions/A/XCTest\n```\n\n```\n# expected to fail\n-> % xctest .build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest\n2021-10-28 15:34:20.890 xctest[69943:24184188] The bundle \u201cswift-distributed-actorsPackageTests.xctest\u201d couldn\u2019t be loaded. Try reinstalling the bundle.\n2021-10-28 15:34:20.890 xctest[69943:24184188] (dlopen(/Users/ktoso/code/swift-distributed-actors/.build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest/Contents/MacOS/swift-distributed-actorsPackageTests, 0x0109): Library not loaded: /usr/lib/swift/libswift_Distributed.dylib\n  Referenced from: /Users/ktoso/code/swift-distributed-actors/.build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest/Contents/MacOS/swift-distributed-actorsPackageTests\n  Reason: tried: '/usr/lib/swift/libswift_Distributed.dylib' (no such file), '/usr/local/lib/libswift_Distributed.dylib' (no such file), '/usr/lib/libswift_Distributed.dylib' (no such file))\n```\n\nInstead, you must find the `xctest` binary in Xcode:\n\n```\n-> % find /Applications/Xcode-Latest.app  | grep xctest\n/Applications/Xcode-Latest.app/Contents/Developer/usr/bin/xctest\n\n-> % export XCTEST_BINARY=/Applications/Xcode-Latest.app/Contents/Developer/usr/bin/xctest\n```\n\nThen, you can use:\n\n```\nclear\necho $TOOLCHAIN\necho\n\n$TOOLCHAIN/usr/bin/swift build --build-tests && \\\nDYLD_LIBRARY_PATH=\"$TOOLCHAIN/usr/lib/swift/macosx/\" $XCTEST_BINARY \\\n  .build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest\n```\n\nor the following, in order to run only the test class `DistributedReceptionistTests`:\n\n```\n$TOOLCHAIN/usr/bin/swift build --build-tests && \\\nDYLD_LIBRARY_PATH=\"$TOOLCHAIN/usr/lib/swift/macosx/\" $XCTEST_BINARY \\\n  -XCTest \"DistributedActorsTests.DistributedReceptionistTests\" \\\n  .build/x86_64-apple-macosx/debug/swift-distributed-actorsPackageTests.xctest\n```\n\n\nIf you see such issue, you may need to provide the dynamic library path pointing at the specific toolchain you are\nusing to build the project, like this:\n\n```swift\nDYLD_LIBRARY_PATH=\"$TOOLCHAIN/usr/lib/swift/macosx/\" <command>\n```\n\nFor running only a specific test file, please refer to [Running filtered tests](#running-filtered-tests),\nwhich is hitting a similar limitation.\n\n#### Swift Syntax dependency versions\n\n\u26a0\ufe0f Please note that the build needs the _exact_ matching Swift Syntax version that is compatible with the toolchain. This manifests as the following error:\n\n> The loaded '_InternalSwiftSyntaxParser' library is from a toolchain that is not compatible with this version of SwiftSyntax\n\nIn case you encounter this compatibility issue, please check your toolchain used to build, as well as the\n[swift syntax dependency](https://github.com/apple/swift-distributed-actors/blob/main/Package.swift#L287-L298) in `Package.swift`.\nSwiftSyntax uses the toolchain's internal types to perform its parsing, and those are sadly not API stable. As we are developing this \nfeature on nightly builds of the toolchain, and depend on SwiftSyntax as a library, we need to make sure the two match in a compatible way.\n\nThe SwiftSyntax version declared in this project's `Package.swift` will always be in-sync with the recommended toolchain version we recommend above.\nIf you see this issue after pulling the latest changes from this repository, please check if you don't have to also update the toolchain you use to build it.\n\nThis is a current limitation that will be lifted as we remove our dependency on source-generation.\n\n#### Xcode\n\nEditing in Xcode is currently unsupported. This project depends on work in progress language features; once they mature enough, editing in Xcode will become available.\n\n**Other IDEs**\nIt should be possible to open and edit this project in other IDEs. Please note though that since we are using an unreleased Swift 5.6-dev version,\nsome syntax in the Package.swift may not be recognized by those IDE's yet. For example, you may need to comment out all the `.plugin` sections\ndeclarations, in order to import the project into CLion. Once the project is imported though, you can continue using it as usual.\n\n\n**CLion**\n\n> Contributed by [Moritz Lang on the forums](https://forums.swift.org/t/are-there-notes-or-docs-on-how-to-use-a-nightly-development-snapshot-with-projects-swift-distributed-actors/53170/3)\n\nI found the following to work for me, but only using CLion:\n\n* Set the Swift toolchain in CLion to the latest nightly one (for me that was 10-28)\n* Remove the `.plugin` from Package.swift in some other editor (NOT CLion)\n* Open the project in CLion, resolve the Swift package dependencies\n* Close CLion\n* Re-add the `.plugin` to Package.swift\n* Re-open the project in CLion, but DO NOT resolve Swift packages within CLion\n* Add `DYLD_LIBRARY_PATH` to the run configuration\n* Build/Run tests \ud83c\udf89\n\nI suspect the reason this works is that CLion stores that it already resolved the dependencies inside the .idea folder. Additionally, CLion invokes the build command with the --skip-update flag.\n\n\n#### Warnings\n\nThe project currently is emitting many warnings about `Sendable`, this is expected and we are slowly working towards removing them.\n\nMuch of the project's internals use advanced synchronization patterns not recognized by sendable checks, so many of the warnings are incorrect but the compiler has no way of knowing this.\nWe will be removing much of these internals as we move them to use the Swift actor runtime instead.\n\n#### Source generation (to be removed)\n\nThe current approach uses source generation, using a SwiftPM plugin, in order to implement the bridging between\nfunction calls and messages. We are actively working on removing this part of the library and replace it with language \nfeatures powerful enough to express these semantics. \n\nYou can view our proposal to replace the source generator with a language proposal in this [Swift Evolution post](https://forums.swift.org/t/pitch-distributed-actors/51669/104). \n\n### Running samples\n\nTo run samples, it currently is necessary to provide the `DYLD_LIBRARY_PATH` environment variable so Swift is able to locate the new `_Distributed` module.\nThis is a temporary solution, and eventually will not be necessary.\n\nFor example, the following will run the `SampleDiningPhilosophers` example app in _distributed_ mode:\n=======\n\nMuch of the project's internals use advanced synchronization patterns not recognized by sendable checks, so many of the warnings are incorrect but the compiler has no way of knowing this.\nWe will be removing much of these internals as we move them to use the Swift actor runtime instead.\n\n### Running samples\n\n```\necho \"TOOLCHAIN=$TOOLCHAIN\"\n\nDYLD_LIBRARY_PATH=\"$TOOLCHAIN/usr/lib/swift/macosx/\" \\\n  $TOOLCHAIN/usr/bin/swift run \\\n  --package-path Samples \\\n  SampleDiningPhilosophers dist \n```\n\n## Documentation\n\nOnly an initial version of API documentation is available right now. This is mostly because the API surface of this \nlibrary is expected to change intensely, along with the evolving language proposal on which this library depends.\n\nYou can build API documentation by running the Swift DocC compiler. The [recently released DocC compiler](https://swift.org/blog/swift-docc/) is an official part of the Swift\nproject, and it ships with the recent nightly toolchains.\n\nDo build documentation run:\n\n```bash\n./scripts/docs/generate_docc.sh\n```\n\nAnd to preview and browse the documentation as a web-page, run: \n\n```bash\n./scripts/docs/preview_docc.sh\n```\n\nWhich will result in an output similar to this:\n\n```\n========================================\nStarting Local Preview Server\n\t          http://localhost:8000/documentation/distributedactors\n```\n\nYou can then keep this preview server running, and re-run the `generate_docc.sh` script to keep updating the browsed documentation.\n\n## Integration tests\n\nIntegration tests include running actual multiple nodes of a cluster and e.g. killing them off to test the recovery mechanisms of the cluster.\n\nRequirements:\n- macOS: `brew install coreutils` to install `stdbuf`\n\n## Supported Versions\n\nSwift: \n\n- Nightly snapshots of Swift 5.6+\n"
 },
 {
  "repo": "apple/swift-algorithms",
  "language": "Swift",
  "readme_contents": "# Swift Algorithms\n\n**Swift Algorithms** is an open-source package of sequence and collection algorithms, along with their related types.\n\nRead more about the package, and the intent behind it, in the [announcement on swift.org](https://swift.org/blog/swift-algorithms/).\n\n## Contents\n\n#### Combinations / permutations\n\n- [`combinations(ofCount:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Combinations.md): Combinations of particular sizes of the elements in a collection.\n- [`permutations(ofCount:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Permutations.md): Permutations of a particular size of the elements in a collection, or of the full collection.\n- [`uniquePermutations(ofCount:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Permutations.md): Permutations of a collection's elements, skipping any duplicate permutations.\n\n#### Mutating algorithms\n\n- [`rotate(toStartAt:)`, `rotate(subrange:toStartAt:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Rotate.md): In-place rotation of elements.\n- [`stablePartition(by:)`, `stablePartition(subrange:by:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Partition.md): A partition that preserves the relative order of the resulting prefix and suffix.\n\n#### Combining collections\n\n- [`chain(_:_:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Chain.md): Concatenates two collections with the same element type. \n- [`cycled()`, `cycled(times:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Cycle.md): Repeats the elements of a collection forever or a set number of times.\n- [`joined(by:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Joined.md): Concatenate sequences of sequences, using an element or sequence as a separator, or using a closure to generate each separator. \n- [`product(_:_:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Product.md): Iterates over all the pairs of two collections; equivalent to nested `for`-`in` loops.\n\n#### Subsetting operations\n\n- [`compacted()`](https://github.com/apple/swift-algorithms/blob/main/Guides/Compacted.md): Drops the `nil`s from a sequence or collection, unwrapping the remaining elements.\n- [`partitioned(by:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Partition.md): Returns the elements in a sequence or collection that do and do not match a given predicate.\n- [`randomSample(count:)`, `randomSample(count:using:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/RandomSampling.md): Randomly selects a specific number of elements from a collection.\n- [`randomStableSample(count:)`, `randomStableSample(count:using:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/RandomSampling.md): Randomly selects a specific number of elements from a collection, preserving their original relative order.\n- [`striding(by:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Stride.md): Returns every nth element of a collection.\n- [`suffix(while:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Suffix.md): Returns the suffix of a collection where all element pass a given predicate.\n- [`trimmingPrefix(while:)`, `trimmingSuffix(while)`, `trimming(while:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Trim.md): Returns a slice by trimming elements from a collection's start, end, or both. The mutating `trim...` methods trim a collection in place.\n- [`uniqued()`, `uniqued(on:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Unique.md): The unique elements of a collection, preserving their order.\n- [`minAndMax()`, `minAndMax(by:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/MinMax.md): Returns the smallest and largest elements of a sequence.\n\n#### Partial sorting\n\n- [`min(count:)`, `max(count:)`, `min(count:sortedBy:)`, `max(count:sortedBy:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/MinMax.md): Returns the smallest or largest elements of a collection, sorted by a predicate.\n\n#### Other useful operations\n\n- [`adjacentPairs()`](https://github.com/apple/swift-algorithms/blob/main/Guides/AdjacentPairs.md): Lazily iterates over tuples of adjacent elements.\n- [`chunked(by:)`, `chunked(on:)`, `chunks(ofCount:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Chunked.md): Eager and lazy operations that break a collection into chunks based on either a binary predicate or when the result of a projection changes or chunks of a given count.\n- [`firstNonNil(_:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/FirstNonNil.md): Returns the first non-`nil` result from transforming a sequence's elements.\n- [`indexed()`](https://github.com/apple/swift-algorithms/blob/main/Guides/Indexed.md): Iterate over tuples of a collection's indices and elements. \n- [`interspersed(with:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Intersperse.md): Place a value between every two elements of a sequence.\n- [`partitioningIndex(where:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Partition.md): Returns the starting index of the partition of a collection that matches a predicate.\n- [`reductions(_:)`, `reductions(_:_:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Reductions.md): Returns all the intermediate states of reducing the elements of a sequence or collection.\n- [`split(maxSplits:omittingEmptySubsequences:whereSeparator)`, `split(separator:maxSplits:omittingEmptySubsequences)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Split.md): Lazy versions of the Standard Library's eager operations that split sequences and collections into subsequences separated by the specified separator element.\n- [`windows(ofCount:)`](https://github.com/apple/swift-algorithms/blob/main/Guides/Windows.md): Breaks a collection into overlapping subsequences where elements are slices from the original collection.\n\n## Adding Swift Algorithms as a Dependency\n\nTo use the `Algorithms` library in a SwiftPM project, \nadd the following line to the dependencies in your `Package.swift` file:\n\n```swift\n.package(url: \"https://github.com/apple/swift-algorithms\", from: \"1.0.0\"),\n```\n\nInclude `\"Algorithms\"` as a dependency for your executable target:\n\n```swift\n.target(name: \"<target>\", dependencies: [\n    .product(name: \"Algorithms\", package: \"swift-algorithms\"),\n]),\n```\n\nFinally, add `import Algorithms` to your source code.\n\n## Source Stability\n\nThe Swift Algorithms package is source stable; version numbers follow [Semantic Versioning](https://semver.org/). Source breaking changes to public API can only land in a new major version.\n\nThe public API of version 1.0 of the `swift-algorithms` package consists of non-underscored declarations that are marked `public` in the `Algorithms` module. Interfaces that aren't part of the public API may continue to change in any release, including patch releases.\n\nFuture minor versions of the package may introduce changes to these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate. Accordingly, from time to time, we expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release. Requiring a new Swift release will only require a minor version bump.\n"
 },
 {
  "repo": "apple/apple-llvm-infrastructure-tools",
  "language": "C++",
  "readme_contents": "# Apple LLVM Infrastructure Tools\n\nThis is a collection of tools for maintaining LLVM-project&ndash;related\ninfrastructure, including CI, automerging, monorepo transition, and others.\n\n## Deploying `git-apple-llvm`\n\nPrerequisites:\n- Python 3\n- Relatively recent git (git 2.20+ should work)\n\nYou can deploy `git-apple-llvm` by running the `install` target:\n\n```\nsudo make install                 # Installs into /usr/local/bin\nmake install PREFIX=/my/directory # Installs into /my/directory/bin\n```\n\nYou can always uninstall the tools by running the `uninstall` target:\n\n```\nsudo make uninstall\n```\n\n## More documentation\n\n`cd docs/ && make html && open _build/html/index.html` for more documentation.\n"
 },
 {
  "repo": "apple/swift-collections-benchmark",
  "language": "Swift",
  "readme_contents": "# Swift Collections Benchmark\n\nThis package lets you collect and easily visualize performance data about data structure implementations and collection algorithms. It was created to help develop the [Swift Collections] package, but it's useful for so much more!\n\n[Swift Collections]: https://github.com/apple/swift-collections\n\nThis project primarily concentrates on benchmarking Swift code, but it can also be used to run benchmarks (and, especially, to analyze benchmark results) in other languages, too.\n\n## Example\n\nHere is a short benchmark, measuring the performance of `Array.sorted()` and `Set.contains(_:)`:\n\n``` swift\nimport CollectionsBenchmark\n\nvar benchmark = Benchmark(title: \"Demo Benchmark\")\n\nbenchmark.addSimple(\n  title: \"Array<Int> sorted\",\n  input: [Int].self\n) { input in\n  blackHole(input.sorted())\n}\n\nbenchmark.add(\n  title: \"Set<Int> contains\",\n  input: ([Int], [Int]).self\n) { input, lookups in\n  let set = Set(input)\n  return { timer in\n    for value in lookups {\n      precondition(set.contains(value))\n    }\n  }\n}\n\nbenchmark.main()\n```\n\nHere is how you run it:\n\n``` shellsession\n$ swift run -c release benchmark run results --cycles 5\nRunning 2 tasks on 76 sizes from 1 to 1M:\n  Array<Int> sorted\n  Set<Int> contains\nOutput file: /Users/klorentey/Projects/swift-collections-benchmark-demo/Demo/results\nAppending to existing data (if any) for these tasks/sizes.\n\nCollecting data:\n  1.2.4...8...16...32...64...128...256...512...1k...2k...4k...8k...16k...32k...64k...128k...256k...512k...1M -- 5.31s\n  1.2.4...8...16...32...64...128...256...512...1k...2k...4k...8k...16k...32k...64k...128k...256k...512k...1M -- 5.35s\n  1.2.4...8...16...32...64...128...256...512...1k...2k...4k...8k...16k...32k...64k...128k...256k...512k...1M -- 5.29s\n  1.2.4...8...16...32...64...128...256...512...1k...2k...4k...8k...16k...32k...64k...128k...256k...512k...1M -- 5.3s\n  1.2.4...8...16...32...64...128...256...512...1k...2k...4k...8k...16k...32k...64k...128k...256k...512k...1M -- 5.34s\nFinished in 26.6s\n$ swift run -c release benchmark render results chart.png\n$ open chart.png\n```\n\nAnd this is what you get:\n\n![chart.png](Documentation/Assets/demo-chart.png)\n\nToday I learned that sorting 20 integers in an `Array` takes about as much time as looking at all items in a 20-member `Set`. Fascinating! \ud83e\udd13\n\n## Documentation\n\nFor a tour of the features provided by this library, please be sure check out our [Getting Started Guide][guide]!\n\n[guide]: Documentation/01%20Getting%20Started.md\n\n## Project Status\n\nSwift Collections Benchmark is intended primarily as a developer tool, rather than something that would be used in production apps.\n\nIt exposes a source-level API and a command line interface, neither of which are technically stable yet. After an initial period of experimentation, we expect to stabilize both interfaces. Until then, new releases may sometimes come with changes that might break existing benchmark definitions, or that may change the command line interface in ways that could break existing scripts. We'll try our best to keep this to a minimum, though (or mitigate with a multi-release deprecation period), even during this chaotic initial period.\n\n## Adding Swift Collections Benchmark as a Dependency\n\nTo use this package in a SwiftPM project, add the following line to the dependencies in your `Package.swift` file:\n\n```swift\n.package(url: \"https://github.com/apple/swift-collections-benchmark\", from: \"0.0.1\"),\n```\n\nIn the typical case, you'll want to set up a standalone executable target that is dedicated to benchmarking:\n\n```swift\n// swift-tools-version:5.3\nimport PackageDescription\n\nlet package = Package(\n  name: \"MyPackage\",\n  products: [\n    .executable(name: \"my-benchmark\", targets: [\"MyBenchmark\"]),\n  ],\n  dependencies: [\n    .package(url: \"https://github.com/apple/swift-collections-benchmark\", from: \"0.0.1\"),\n    // ... other dependencies ...\n  ],\n  targets: [\n    // ... other targets ...\n    .target(\n      name: \"MyBenchmark\",\n      dependencies: [\n        .product(name: \"CollectionsBenchmark\", package: \"swift-collections-benchmark\"),\n      ]),\n  ]\n)\n```\n\n## Contributing to Swift Collections Benchmark\n\n### Asking questions\n\nWe can use the [Swift Collections Forum][forum] to ask and answer questions on how to use or work on this package. It's also a great place to discuss its evolution.\n\n[forum]: https://forums.swift.org/c/related-projects/collections\n\n### Reporting a bug\n\nIf you find something that looks like a bug, please open a [Bug Report][bugreport]! Fill out as many details as you can.\n\n[bugreport]: https://github.com/apple/swift-collections-benchmark/issues/new?assignees=&labels=bug&template=BUG_REPORT.md\n\n### Fixing a bug or making a small improvement\n\n1. [Submit a PR][PR] with your change. If there is an [existing issue][issues] for the bug you're fixing, please include a reference to it.\n2. Make sure to add test coverage for whatever changes you are making (if possible).\n\n[PR]: https://github.com/apple/swift-collections-benchmark/compare\n[issues]: https://github.com/apple/swift-collections-benchmark/issues\n\n(Note: The package doesn't currently come with many tests, reflecting its origins as a supporting project -- we strive to improve that!)\n\n### Proposing a small enhancement\n\n1. Raise a [Feature Request][enhancement]. Discuss why it would be important to implement it.\n2. Submit a PR with your implementation, participate in the review discussion.\n3. When there is a consensus that the feature is desirable, and the implementation works well, it will be merged. \n4. Rejoice!\n\n[enhancement]: https://github.com/apple/swift-collections-benchmark/issues/new?assignees=&labels=enhancement&template=FEATURE_REQUEST.md\n\n### Proposing a larger feature\n\n1. Raise a [Feature Request][enhancement], or start a topic on the [forum]. Discuss why it would be important to implement it, and potential implementation strategies.\n2. Submit a PR with your implementation, and participate in the review discussion. Sometimes we may need to go through several revisions! This is fine -- it makes the end result that much better.\n3. When there is a consensus that the feature is desirable, and the implementation works well, it will be merged. \n4. Celebrate! \n\n### Licensing\n\nBy submitting a pull request, you represent that you have the right to license your contribution to Apple and the community, and agree by submitting the patch that your contributions are licensed under the [Swift License](https://swift.org/LICENSE.txt), a copy of which is [provided in this repository](LICENSE.txt).\n\n### Code of Conduct\n\nLike all Swift.org projects, we would like the Swift Collections Benchmark project to foster a diverse and friendly community. We expect contributors to adhere to the [Swift.org Code of Conduct](https://swift.org/code-of-conduct/). A copy of this document is [available in this repository][coc].\n\n[coc]: CODE_OF_CONDUCT.md\n\n### Contacting the maintainers\n\nThe current code owner of this package is Karoy Lorentey ([@lorentey](https://github.com/lorentey)). You can contact him [on the Swift forums](https://forums.swift.org/u/lorentey/summary), or by writing an email to klorentey at apple dot com. (Please keep it related to this project.)\n\nIn case of moderation issues, you can also directly contact a member of the [Swift Core Team](https://swift.org/community/#community-structure).\n\n"
 },
 {
  "repo": "apple/cloudkit-sample-sharing",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Sharing\n\n\u26a0\ufe0f For the new Swift 5.5 concurrency APIs, see the `swift-concurrency` branch \u26a0\ufe0f\n\n### Goals\n\nThis project demonstrates sharing CloudKit records across user accounts. It shows how to initiate a share from one user account, and accept the share and subsequently view shared data on another account.\n\n### Prerequisites\n\n* A Mac with [Xcode 12](https://developer.apple.com/xcode/) (or later) installed is required to build and test this project.\n* An active [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed to create a CloudKit container.\n\n### Setup Instructions\n\n* Ensure the simulator or device you run the project on is signed in to an Apple ID account with iCloud enabled. This can be done in the Settings app.\n* If you wish to run the app on a device, ensure the correct developer team is selected in the \u201cSigning & Capabilities\u201d tab of the Sharing app target, and a valid iCloud container is selected under the \u201ciCloud\u201d section.\n\n#### Using Your Own iCloud Container\n\n* Create a new iCloud container through Xcode\u2019s \u201cSigning & Capabilities\u201d tab of the Sharing app target.\n* Update the `containerIdentifier` property in [Config.swift](Sharing/App/Config.swift) with your new iCloud container ID.\n\n### How it Works\n\n#### User One: Initiating the Share\n\n* On either a device or simulator with a signed-in iCloud account, User One creates a new Contact record through the UI with a name and phone number. The Contact is saved to the user\u2019s private iCloud database with the `addContact(name:phoneNumber:completionHandler)` function in ViewModel.swift.\n\n* After the Contacts list is refreshed, the newly added Contact will appear under the \u201cPrivate\u201d section of the UI.\n\n* Tapping the share button on the Contact list entry creates a `CKShare` object and writes it to the database with the `createShare(contact:completionHandler)` function in ViewModel.swift. After the share is created, the `CloudSharingView` is displayed which wraps [UICloudSharingController](https://developer.apple.com/documentation/uikit/uicloudsharingcontroller) in a SwiftUI compatible view. This view allows the user to configure share options and send or copy the share link to share with User Two.\n\n#### User Two: Accepting the Share Invitation\n\n* On a separate device with a different signed-in iCloud account, User Two accepts the share by following the link provided by User One.\n\n* The link initiates a prompt on the user\u2019s device to accept the share, which launches the Sharing app and accepts the share through a database operation defined in SceneDelegate\u2019s `userDidAcceptCloudKitShareWith` delegate callback.\n\n* After the share is accepted and the UI is refreshed, the shared Contact will display in User Two\u2019s Contacts list in the \u201cShared\u201d section. `fetchSharedContacts(completionHandler:)` in ViewModel.swift shows how Contacts in shared database zones are fetched.\n\n### Further Reading\n\n* [Sharing CloudKit Data with Other iCloud Users](https://developer.apple.com/documentation/cloudkit/shared_records/sharing_cloudkit_data_with_other_icloud_users)\n"
 },
 {
  "repo": "apple/cloudkit-sample-queries",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Queries\n\n### Goals\n\nThis project demonstrates use of CloudKit queries against a CloudKit Private Database. It shows how to filter a set of records by using a predicate against a property \u2014 in this case, a set of Contact records with a `name` property, and a `BEGINSWITH` predicate to query for records prefixed by a user-provided string.\n\n### Prerequisites\n\n* A Mac with [Xcode 12](https://developer.apple.com/xcode/) (or later) installed is required to build and test this project.\n* An [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed if you wish to create your own CloudKit container.\n\n### Setup Steps\n\n* Ensure the simulator or device you run the project on is signed in to an Apple ID account with iCloud enabled. This can be done in the Settings app.\n* If you wish to run the app on a device, ensure the correct developer team is selected in the \u201cSigning & Capabilities\u201d tab of the Queries app target, and a valid iCloud container is selected under the \u201ciCloud\u201d section.\n\n#### Using Your Own iCloud Container\n\n* Create a new iCloud container through Xcode\u2019s \u201cSigning & Capabilities\u201d tab of the Queries app target.\n* Update the `containerIdentifier` property in [Config.swift](Queries/Config.swift) with your new iCloud container ID.\n\n### How It Works\n\n* On first launch, the app fetches all Contact records from the remote database and displays the names of those records in the UI.\n* When a user adds a new Contact record through the UI, the record is saved to the remote database and the records are retrieved and displayed again.\n* When a user filters the list through the UI, a new query operation is performed using the `BEGINSWITH` predicate, and only records with the `name` field beginning with the given filter string are returned and displayed.\n\n### Things To Learn\n\n* How to create new records with the `CKModifyRecordsOperation`.\n* How to use `CKQuery` and `CKQueryOperation` to build a query matching all or specific records, and retrieve and process the results with a `recordFetchedBlock`.\n* How to use the `Result` type to provide clear information about the result of asynchronous operations with completion handlers.\n\n### Further Reading\n\n* [Running Your App in the Simulator or on a Device](https://developer.apple.com/documentation/xcode/running_your_app_in_the_simulator_or_on_a_device)\n* [CloudKit Private Database](https://developer.apple.com/documentation/cloudkit/ckcontainer/1399205-privateclouddatabase)\n"
 },
 {
  "repo": "apple/cloudkit-sample-privatedb-sync",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Private Sync with Subscriptions and Push\n\n### Goals\n\nThis project demonstrates using CloudKit Database Subscriptions and push notifications to keep two separate instances of an app in sync. Ideally it is run on both a simulator and a real device, and content changes made on the simulator are received and reflected on the device via CloudKit Subscriptions, similar to the functionality of the Notes or Photos apps.\n\n### Prerequisites\n\n* A Mac with [Xcode 12](https://developer.apple.com/xcode/) (or later) installed is required to build and test this project.\n* An iOS device which will receive CloudKit change notifications is required to install and run the app on.\n* An active [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed to create a CloudKit container and sign the app to run on a device.\n\n**Note**: Simulators cannot register for remote push notifications. Running this sample on a device is **required** to receive `CKSubscription` push notifications and observe syncing functionality.\n\n### Setup Instructions\n\n1. Ensure you are logged into your developer account in Xcode with an active membership.\n1. In the \u201cSigning & Capabilities\u201d tab of the PrivateSync target, ensure your team is selected in the Signing section, and there is a valid container selected under the \u201ciCloud\u201d section.\n1. Ensure that both the simulator you wish to use and the device you will run the app on are logged into the same iCloud account.\n\n#### Using Your Own iCloud Container\n\n* Create a new iCloud container through Xcode\u2019s \u201cSigning & Capabilities\u201d tab of the PrivateSync app target.\n* Update the `containerIdentifier` property in [Config.swift](PrivateSync/Config.swift) with your new iCloud container identifier.\n\n### How it Works\n\n* On first launch, the app creates a custom zone on the Private Database named \u201cContacts\u201d, and subscribes to all record changes on that zone.\n* When running on a device, the app also registers with APNs (Apple Push Notification service), which is the mechanism for receiving information about changes through the aforementioned subscription.\n* After this initialization process, the app fetches the latest changes from the server, using a change token representing the last time changes were fetched and processed if available. On first launch, no local token is available, so all records are returned, and the token returned from this operation is saved.\n* The app\u2019s main UI displays a list of Contacts. When the user adds a new Contact through the UI, a new record is created and saved to the database, and if successful, also saves this to a local store. This will trigger the UI to update and include the new Contact on the main list view.\n* Creating a new record triggers a notification to **other** devices which are registered for push notifications with the app through the `CKRecordZoneSubscription` created on first launch.\n* Devices receiving this notification will react by fetching the latest changes on the zone using the last known change token, and receive only the set of records that have changed since that change token was received. The records are updated locally and the UI now reflects the latest database state once again.\n\n### Example Flow\n\n1. Run the app on a device. Latest changes are fetched and a change token is stored.\n1. Repeat the above on a simulator, and add a new contact through the UI.\n1. The device receives a background push notification flagging that a change has occurred.\n1. The device fetches the changes, passing along the change token received in step 1. Only the new contact added in step 2 is returned and processed, and now shows on the UI.\n\n### Things To Learn\n\n* Creating a custom CloudKit Record Zone.\n* Creating a CloudKit Subscription that listens to database changes and sends a `content-available` push notification on change events.\n* Registering for push notifications with a SwiftUI-compatible `UIApplicationDelegate` class.\n* Receiving and handling a `CKNotification`.\n* Using a cached `CKServerChangeToken` to fetch only record changes and deletions since the last sync.\n* Adding, removing, and merging remote changes into a local cache, and reflecting those changes live in a UI.\n\n### Further Reading\n\n* [Remote Records](https://developer.apple.com/documentation/cloudkit/remote_records)\n* [CKServerChangeToken](https://developer.apple.com/documentation/cloudkit/ckserverchangetoken)\n* [CKSubscription](https://developer.apple.com/documentation/cloudkit/cksubscription) and [CKRecordZoneSubscription](https://developer.apple.com/documentation/cloudkit/ckrecordzonesubscription)\n* [CKRecordZoneNotification](https://developer.apple.com/documentation/cloudkit/ckrecordzonenotification)\n"
 },
 {
  "repo": "apple/cloudkit-sample-privatedb",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Private Database\n\n## Goals\n\nThis buildable (and testable) Xcode project demonstrates a simple use of the CloudKit Private Database. It allows you easily try out reading from and writing to a Private Database for a user in your own container on CloudKit servers.\n\n## Prerequisites\n\n* An [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed to create a CloudKit container.\n\n* A Mac with [Xcode 12](https://developer.apple.com/xcode/) (or later) installed is required to build and test this project.\n\n## Setup Steps\n\n1. Clone this sample code repository\n1. Open `PrivateDatabase.xcodeproj` in Xcode\n1. In the General tab of `PrivateDatabase` Target in Xcode, set your own Bundle Identifier\n1. In the Accounts section of Xcode Preferences, sign into your developer account (in Xcode) if needed\n1. In the Signing & Capabilities tab of `PrivateDatabase` Target in Xcode, choose your account's Team\n1. In the Signing & Capabilities tab of `PrivateDatabaseTests` Target in Xcode, choose your account's Team\n1. In the Signing & Capabilities tab of `PrivateDatabase` Target in Xcode, choose existing iCloud container (or press \"+\" to create a new container)\n1. Update the `containerIdentifier` property in `Config.swift` with your iCloud container name\n1. Launch a Simulator (for example, via the Xcode menu in the menu bar) and ensure the Simulator is logged into an iCloud account in Settings\n1. Test the app in the Simulator (from the Product menu in Xcode)\n1. Run the app in the Simulator (from the Product menu in Xcode)\n\n## How It Works\n\n* Upon launch, the app reads a single record from the CloudKit server.\n\n* Specifically, this record resides in the Default Zone of the currently signed-in iCloud user's Private Database in the app's CloudKit Container.\n\n* The record is of type \"Person\". The \"Person\" record type, as defined in the CloudKit Container's Schema by the app's developer, has a single custom string field called \"name\".\n\n* The specific CloudKit record the app reads has a well-known record ID of \"lastPerson\". (This well-known ID is hardcoded in the app.)\n\n* The app's UI displays the name of the last person to write their name into the \"name\" field of this record on the server.\n\n* When the user of the app enters their own name into a text field in the UI, the app writes the user's name into the \"name\" field of this same CloudKit Record and saves it back to the CloudKit Server.\n\n* Subsequent launches of this app from this device (or other devices) will show this user's name until another user's name is written into the \"name\" field of the same CloudKit Record.\n\n## Things To Learn\n\n* A working Xcode project that interacts with the CloudKit server\n\n* Some basic data flows between CloudKit and a SwiftUI `View`\n\n* Reading from and writing to a CloudKit Private Database\n\n* Writing a CloudKit record using `CKModifyRecordsOperation`\n\n* Overriding the default `savePolicy` when writing\n\n* Fetching an explicit record by ID using the `fetch(withRecordID:)` convenience method on `CKDatabase`\n\n* Some basic error trapping of `CKError` errors, including those embedded in `partialFailure`\n\n* Some basic testing using `XCTest`\n\n## Further Reading\n\n* [Running Your App in the Simulator or on a Device](https://developer.apple.com/documentation/xcode/running_your_app_in_the_simulator_or_on_a_device)\n\n* [CloudKit Private Database](https://developer.apple.com/documentation/cloudkit/ckcontainer/1399205-privateclouddatabase)\n"
 },
 {
  "repo": "apple/cloudkit-sample-encryption",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Encryption\n\n### Goals\n\nThis project demonstrates using encrypted values with CloudKit and iCloud containers. CloudKit encrypts data with key material stored in a customer\u2019s iCloud Keychain. If a customer loses access to their iCloud Keychain, CloudKit cannot access the key material previously used to encrypt data stored in the cloud, meaning that data can no longer be decrypted and accessed by the customer. More information about this is covered in the \u201cError Handling\u201d section below.\n\n### Prerequisites\n\n* A Mac with [**Xcode 13 Beta 1**](https://developer.apple.com/download/) (or later) installed is required to build and test this project.\n* An active [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed to create a CloudKit container.\n\n### Setup Instructions\n\n* Ensure the simulator or device you run the project on is signed in to an Apple ID account with iCloud enabled. This can be done in the Settings app.\n* If you wish to run the app on a device, ensure the correct developer team is selected in the \u201cSigning & Capabilities\u201d tab of the Encryption app target, and a valid iCloud container is selected under the \u201ciCloud\u201d section.\n\n#### Using Your Own iCloud Container\n\n* Create a new iCloud container through Xcode\u2019s \u201cSigning & Capabilities\u201d tab of the Queries app target.\n* Update the `containerIdentifier` property in [Config.swift](Encryption/Config.swift) with your new iCloud container ID.\n\n### How it Works\n\nThis project only differs very slightly from other samples, in that it uses the `encryptedValues` property of [`CKRecord`](https://developer.apple.com/documentation/cloudkit/ckrecord) in two places.\n\nSetting the `phoneNumber` value in ViewModel.swift `addContact`:\n```swift\ncontactRecord.encryptedValues[\"phoneNumber\"] = phoneNumber\n```\n\n\u2026and retrieving the `phoneNumber` value (in Contact.swift `Contact.init(record:)`):\n```swift\nlet phoneNumber = record.encryptedValues[\"phoneNumber\"] as? String\n```\n\nYou can confirm that the value is encrypted by viewing the schema in [CloudKit Dashboard](https://icloud.developer.apple.com) and confirming that the `phoneNumber` custom field under the Contact type shows \u201cEncrypted Bytes\u201d for its \u201cField Type\u201d.\n\n### Notes on Encrypted Fields\n\n* Encrypted fields cannot have indexes.\n* Existing fields in a CloudKit schema are not eligible for encryption.\n* `CKReference` fields cannot be encrypted.\n* `CKAsset` fields are encrypted by default, and therefore should not be set as `encryptedValues` fields.\n* `CKRecordID`, `CKRecordZoneID` or any other data types that is not one of `NSString`, `NSNumber`, `NSDate`, `NSData`, `CLLocation` and `NSArray` cannot be set as `encryptedValues` fields.\n\n### Error Handling\n\n* As described above, CloudKit encrypts data with key material store in a customer\u2019s iCloud Keychain. If this key material is lost, for example by a customer resetting their iCloud Keychain, CloudKit is unable to decrypt previously encrypted data and returns a specific error code.\n* This is demonstrated in the `handleError` function, where a `CKError` with a `zoneNotFound` code may have a `CKErrorUserDidResetEncryptedDataKey` `NSNumber` value in the `userInfo` dictionary.\n* It is outside the scope of this sample, but it is recommended when encountering this error to first delete the relevant zone(s), re-create them, and then re-upload locally-cached data from the device to those zones. This new data is encrypted using the new key material from the user\u2019s iCloud Keychain.\n\n### Things To Learn\n\n* Creating, fetching from, and saving to a custom zone.\n* Saving and retrieving encrypted values in a record in the remote Private Database.\n* Handling errors specifically related to using Encrypted Fields.\n* Using XCTest to asynchronously test creating new temporary records, fetching records, and cleaning up records created during tests with `tearDown` functions.\n\n### Further Reading\n\n* [Encrypting User Data](https://developer.apple.com/documentation/cloudkit/encrypting_user_data)\n"
 },
 {
  "repo": "apple/cloudkit-sample-coredatasync",
  "language": "Swift",
  "readme_contents": "# CloudKit Samples: Core Data with CloudKit\n\n### Goals\n\nThis project demonstrates using Core Data with `NSPersistentCloudKitContainer`, which encapsulates the Core Data stack in an application and mirrors persistent stores to a CloudKit private database. Changes made locally are automatically sent to the remote CloudKit database, and remote changes are fetched and brought into local stores across devices through background push notifications.\n\n### Prerequisites\n\n* A Mac with [Xcode 12](https://developer.apple.com/xcode/) (or later) installed is required to build and test this project.\n* An iOS device which will receive CloudKit change notifications is required to install and run the app on, as simulators cannot receive remote push notifications.\n* An active [Apple Developer Program membership](https://developer.apple.com/support/compare-memberships/) is needed to create a CloudKit container and sign the app to run on a device.\n\n### Setup Instructions\n\n1. Ensure you are logged into your developer account in Xcode with an active membership.\n1. In the \u201cSigning & Capabilities\u201d tab of the CoreDataSync target, ensure your team is selected in the Signing section, and there is a valid container selected under the \u201ciCloud\u201d section.\n1. Ensure that both the simulator you wish to use and the device you will run the app on are logged into the same iCloud account.\n\n#### Using Your Own iCloud Container\n\n* Create a new iCloud container through Xcode\u2019s \u201cSigning & Capabilities\u201d tab of the CoreDataSync app target.\n* Update the `containerIdentifier` property in [Config.swift](CoreDataSync/App/Config.swift) with your new iCloud container ID.\n\n### How it Works\n\n* The `CoreDataSync` model defines a `Contact` entity, which stores a name (`String`) and photo (`UIImage` Transformable).\n* The `PersistenceController` class creates an `NSPersistentCloudKitContainer` object with the `CoreDataSync` model.\n* The main list view uses the [`@FetchRequest`](https://developer.apple.com/documentation/swiftui/fetchrequest) property wrapper to retrieve Contact objects from the Core Data store.\n* Creating new Contacts and deleting existing Contacts is done through normal Core Data operations.\n* `NSPersistentCloudKitContainer` syncs with the user\u2019s private database in the iCloud container listed in the app\u2019s entitlements file.\n\n### Example Flow\n\n1. Run the app on a device. After initializing the store, CoreData+CloudKit mirroring will fetch any existing records from the remote private database.\n1. Repeat the above on a simulator, and add a new contact through the UI.\n1. The device receives a background push notification which is automatically processed by CoreData+CloudKit. Changes are fetched and merged into the local store, and both simulator and device once again show the same content. Deleting a contact by swiping left on a cell produces similar behavior.\n\nNote that because remote push notifications are not supported on simulators, changes on a running device will not be automatically reflected on a running simulator as they are the other way around. Running the app again on a simulator will cause CoreData+CloudKit to sync changes made on a device.\n\n### Things To Learn\n\n* Showing a live list of Core Data managed objects with SwiftUI using the `@FetchRequest` property wrapper.\n* Adding and deleting objects with `NSManagedObjectContext`.\n* Automatic syncing of Core Data objects with `NSPersistentCloudKitContainer`.\n* `NSPersistentCloudKitContainer` processing of push notifications to receive and display updates initiated on another device.\n* Storage and decoding of image types in Core Data and CloudKit Private Database.\n\n### Further Reading\n\n* [NSPersistentCloudKitContainer](https://developer.apple.com/documentation/coredata/nspersistentcloudkitcontainer)\n* [Mirroring a Core Data Store with CloudKit](https://developer.apple.com/documentation/coredata/mirroring_a_core_data_store_with_cloudkit)\n"
 },
 {
  "repo": "apple/example-package-playingcard",
  "language": "Swift",
  "readme_contents": "This example package will be cloned and built as a dependency if you build the `Dealer` example:\n\n    git clone https://github.com/apple/example-package-dealer.git\n    cd example-package-dealer\n    swift run Dealer\n"
 },
 {
  "repo": "apple/swift-distributed-tracing",
  "language": "Swift",
  "readme_contents": "# Swift Distributed Tracing\n\nA Distributed Tracing API for Swift.\n\nThis is a collection of Swift libraries enabling the instrumentation of server side applications using tools such as tracers. Our goal is to provide a common foundation that allows to freely choose how to instrument systems with minimal changes to your actual code.\n\nWhile Swift Distributed Tracing allows building all kinds of _instruments_, which can co-exist in applications transparently, its primary use is instrumenting multi-threaded and distributed systems with Distributed Traces.\n\n\n---\n\nThis project uses the context progagation type defined independently in:\n\n- \ud83e\uddf3 [swift-distributed-tracing-baggage](https://github.com/apple/swift-distributed-tracing-baggage) -- [`Baggage`](https://apple.github.io/swift-distributed-tracing-baggage/docs/current/InstrumentationBaggage/Structs/Baggage.html) (zero dependencies)\n\n---\n\n## Table of Contents\n\n* [Compatibility](#compatibility)\n    + [Tracing Backends](#tracing-backends)\n    + [Libraries & Frameworks](#libraries--frameworks)\n* [Getting Started](#getting-started)\n    + [Dependencies & Tracer backend](#dependencies--tracer-backend)\n    + [Benefiting from instrumented libraries/frameworks](#benefiting-from-instrumented-libraries-frameworks)\n    + [Instrumenting your code](#instrumenting-your-code)\n    + [More examples](#more-examples)\n* [In-Depth Guide](#in-depth-guide)\n* In-Depth Guide for **Application Developers**\n    + [Setting up instruments & tracers](#setting-up-instruments--tracers)\n    + [Bootstrapping the InstrumentationSystem](#bootstrapping-the-instrumentationsystem)\n    + [Context propagation](#passing-context-objects)\n    + [Creating context objects](#creating-context-objects--and-when-not-to-do-so-)\n    + [Working with `Span`s](#spans)\n* In-Depth Guide for: **Library/Framework developers**\n    + [Instrumenting your software](#library-framework-developers--instrumenting-your-software)\n    + [Extracting & injecting Baggage](#extracting--injecting-baggage)\n    + [Tracing your library](#tracing-your-library)\n* In-Depth Guide for: **Instrument developers**\n    + [Creating an `Instrument`](#instrument-developers--creating-an-instrument)\n    + [Creating a `Tracer`](#creating-a--tracer-)\n* [Contributing](#contributing)\n\n---\n\n## Compatibility\n\nThis project is designed in a very open and extensible manner, such that various instrumentation and tracing systems can be built on top of it. \n\nThe purpose of the tracing package is to serve as common API for all tracer and instrumentation implementations. Thanks to this, libraries may only need to be instrumented once, and then be used with any tracer which conforms to this API.\n\n<a name=\"backends\"></a>\n### Tracing Backends\n \nCompatible `Tracer` implementations:\n\n| Library | Status | Description |\n| ------- | ------ | ----------- |\n| [@slashmo](https://github.com/slashmo) / [**OpenTelemetry** Swift](https://github.com/slashmo/opentelemetry-swift) | Complete | Exports spans to OpenTelemetry Collector; **X-Ray** & **Jaeger** propagation available via extensions. |\n| [@pokrywka](https://github.com/pokryfka) / [AWS **xRay** SDK Swift](https://github.com/pokryfka/aws-xray-sdk-swift) | Complete (?) | ... |\n| _Your library?_ | ... | [Get in touch!](https://forums.swift.org/c/server/43) |\n\nIf you know of any other library please send in a [pull request](https://github.com/apple/swift-distributed-tracing/compare) to add it to the list, thank you!\n\n### Libraries & Frameworks\n\nAs this API package was just released, no projects have yet fully adopted it, the following table for not serves as reference to prior work in adopting tracing work. As projects move to adopt tracing completely, the table will be used to track adoption phases of the various libraries.\n\n| Library | Integrates | Status |\n| ------- | ---------- | ------ |\n| AsyncHTTPClient | Tracing | Old* [Proof of Concept PR](https://github.com/swift-server/async-http-client/pull/289) |\n| Swift gRPC | Tracing | Old* [Proof of Concept PR](https://github.com/grpc/grpc-swift/pull/941) |\n| Swift AWS Lambda Runtime | Tracing | Old* [Proof of Concept PR](https://github.com/swift-server/swift-aws-lambda-runtime/pull/167) |\n| Swift NIO | Baggage | Old* [Proof of Concept PR](https://github.com/apple/swift-nio/pull/1574) |\n| RediStack (Redis) | Tracing | Signalled intent to adopt tracing. |\n| Soto AWS Client | Tracing | Signalled intent to adopt tracing. |\n| _Your library?_ | ... | [Get in touch!](https://forums.swift.org/c/server/43) | \n\n> `*` Note that this package was initially developed as a Google Summer of Code project, during which a number of Proof of Concept PR were opened to a number of projects.\n>\n> These projects are likely to adopt the, now official, Swift Distributed Tracing package in the shape as previewed in those PRs, however they will need updating. Please give the library developers time to adopt the new APIs (or help them by submitting a PR doing so!).\n\nIf you know of any other library please send in a [pull request](https://github.com/apple/swift-distributed-tracing/compare) to add it to the list, thank you!\n\n---\n\n## Getting Started\n\nIn this short getting started example, we'll go through bootstrapping, immediately benefiting from tracing, and instrumenting our own synchronous and asynchronous APIs. The following sections will explain all the pieces of the API in more depth. When in doubt, you may want to refer to the [OpenTelemetry](https://opentelemetry.io), [Zipkin](https://zipkin.io), or [Jaeger](https://www.jaegertracing.io) documentations because all the concepts for different tracers are quite similar. \n\n### Dependencies & Tracer backend\n\nIn order to use tracing you will need to bootstrap a tracing backend ([available backends](#backends)). \n\nWhen developing an *application* locate the specific tracer library you would like to use and add it as an dependency directly:\n\n```swift\n.package(url: \"<https://example.com/some-awesome-tracer-backend.git\", from: \"...\"),\n```\n\nAlternatively, or when developing a *library/framework*, you should not depend on a specific tracer, and instead only depend on the tracing package directly, by adding the following to your `Package.swift`:\n\n```\n.package(url: \"https://github.com/apple/swift-distributed-tracing.git\", from: \"0.1.0\"),\n```\n\nTo your main target, add a dependency on `Tracing` library and the instrument you want to use:\n\n```swift\n.target(\n    name: \"MyApplication\", \n    dependencies: [\n        \"Tracing\",\n        \"<AwesomeTracing>\", // the specific tracer\n    ]\n),\n```\n\nThen (in an application, libraries should _never_ invoke `bootstrap`), you will want to bootstrap the specific tracer you want to use in your application. A `Tracer` is a type of `Instrument` and can be offered used to globally bootstrap the tracing system, like this:\n\n\n```swift\nimport Tracing // the tracing API\nimport AwesomeTracing // the specific tracer\n\nInstrumentationSystem.bootstrap(AwesomeTracing())\n```\n\nIf you don't bootstrap  (or other instrument) the default no-op tracer is used, which will result in no trace data being collected.\n\n### Benefiting from instrumented libraries/frameworks\n\n**Automatically reported spans**: When using an already instrumented library, e.g. an HTTP Server which automatically emits spans internally, this is all you have to do to enable tracing. It should now automatically record and emit spans using your configured backend.\n\n**Using baggage and logging context**: The primary transport type for tracing metadata is called `Baggage`, and the primary type used to pass around baggage context and loggers is `LoggingContext`. Logging context combines baggage context values with a smart `Logger` that automatically includes any baggage values (\"trace metadata\") when it is used for logging. For example, when using an instrumented HTTP server, the API could look like this:\n\n```swift\nSomeHTTPLibrary.handle { (request, context) in \n  context.logger.info(\"Wow, tracing!\") // automatically includes tracing metadata such as \"trace-id\"\n  return try doSomething(request context: context)\n}\n```\n\nIn this snippet, we use the context logger to log a very useful message. However it is even more useful than it seems at first sight: if a tracer was installed and extracted tracing information from the incoming request, it would automatically log our message _with_ the trace information, allowing us to co-relate all log statements made during handling of this specific request:\n\n```\n05:46:38 example-trace-id=1111-23-1234556 info: Wow tracing!\n05:46:38 example-trace-id=9999-22-9879797 info: Wow tracing!\n05:46:38 example-trace-id=9999-22-9879797 user=Alice info: doSomething() for user Alice\n05:46:38 example-trace-id=1111-23-1234556 user=Charlie info: doSomething() for user Charlie\n05:46:38 example-trace-id=1111-23-1234556 user=Charlie error: doSomething() could not complete request!\n05:46:38 example-trace-id=9999-22-9879797 user=alice info: doSomething() completed\n```\n\nThanks to tracing, and trace identifiers, even if not using tracing visualization libraries, we can immediately co-relate log statements and know that the request `1111-23-1234556` has failed. Since our application can also _add_ values to the context, we can quickly notice that the error seems to occur for the user `Charlie` and not for user `Alice`. Perhaps the user Charlie has exceeded some quotas, does not have permissions or we have a bug in parsing names that include the letter `h`? We don't know _yet_, but thanks to tracing we can much quicker begin our investigation.\n\n**Passing context to client libraries**: When using client libraries that support distributed tracing, they will accept a `Baggage.LoggingContext` type as their _last_ parameter in many calls.\n\nWhen using client libraries that support distributed tracing, they will accept a `Baggage.LoggingContext` type as their _last_ parameter in many calls. Please refer to [Context argument naming/positioning](#context-propagation-by-explicit-loggingcontext-passing) in the [Context propagation](#context-propagation-by-explicit-loggingcontext-passing) section of this readme to learn more about how to properly pass context values around.\n\n### Instrumenting your code\n\nAdding a span to synchronous functions can be achieved like this:\n\n```swift\nfunc handleRequest(_ op: String, context: LoggingContext) -> String {\n  let tracer = InstrumentationSystem.tracer\n  let span = tracer.startSpan(operationName: \"handleRequest(\\(name))\", context: context)\n  defer { span.end() }\n  \n  return \"done:\\(op)\"\n}\n```\n\nThrowing can be handled by either recording errors manually into a span by calling `span.recordError(error:)`, or by wrapping a potentially throwing operation using the `withSpan(operation:context:body:)` function, which automatically records any thrown error and ends the span at the end of the body closure scope:\n\n```swift\nfunc handleRequest(_ op: String, context: LoggingContext) -> String {\n  return try InstrumentationSystem.tracer\n        .withSpan(operationName: \"handleRequest(\\(name))\", context: context) {\n    return try dangerousOperation() \n  }\n}\n```\n\nIf this function were asynchronous, and returning a [Swift NIO](https://github.com/apple/swift-nio) `EventLoopFuture`,\nwe need to end the span when the future completes. We can do so in its `onComplete`:\n\n```swift\nfunc handleRequest(_ op: String, context: LoggingContext) -> EventLoopFuture<String> {\n  let tracer = InstrumentationSystem.tracer\n  let span = tracer.startSpan(operationName: \"handleRequest(\\(name))\", context: context)\n  \n  let future: EventLoopFuture<String> = someOperation(op)\n  future.whenComplete { _ in \n    span.end() // oh no, ignored errors!\n  }\n  \n  return future\n}\n```\n\nThis is better, however we ignored the possibility that the future perhaps has failed. If this happens, we would like to report the span as _errored_ because then it will show up as such in tracing backends and we can then easily search for failed operations etc.\n\nTo do this within the future we could manually invoke the `span.recordError` API before ending the span like this:\n\n```swift\nfunc handleRequest(_ op: String, context: LoggingContext) -> EventLoopFuture<String> {\n  let tracer = InstrumentationSystem.tracer\n  let span = tracer.startSpan(operationName: \"handleRequest(\\(name))\", context: context)\n\n  let future: EventLoopFuture<String> = someOperation(op)\n  future.whenComplete { result in\n    switch result {\n    case .failure(let error): span.recordError(error)\n    case .success(let value): // ... record additional *attributes* into the span\n    }\n    span.end()\n  }\n\n  return future\n}\n```\n\nWhile this is verbose, this is only the low-level building blocks that this library provides, higher level helper utilities can be  \n\n> Eventually convenience wrappers will be provided, automatically wrapping future types etc. We welcome such contributions, but likely they should live in `swift-distributed-tracing-extras`.\n\nOnce a system, or multiple systems have been instrumented, a Tracer been selected and your application runs and emits some trace information, you will be able to inspect how your application is behaving by looking at one of the various trace UIs, such as e.g. Zipkin:\n\n![Simple example trace in Zipkin Web UI](images/zipkin_trace.png)\n\n### More examples\n\nIt sometimes is easier to grasp the usage of tracing by looking at a \"real\" application - which is why we have implemented an example application, spanning multiple nodes and using various databases - tracing through all of them. You can view the example application here: [slashmo/swift-tracing-examples](https://github.com/slashmo/swift-tracing-examples/tree/main/hotrod).\n\n### Future work: Tracing asynchronous functions\n\n> \u26a0\ufe0f This section refers to in-development upcoming Swift Concurrency features and can be tried out using nightly snapshots of the Swift toolchain.\n\nWith Swift's ongoing work towards asynchronous functions, actors, and tasks, tracing in Swift will become more pleasant than it is today.\n\nFirstly, a lot of the callback heavy code will be folded into normal control flow, which is easy and correct to integrate with tracing like this:\n\n```swift\nfunc perform(context: LoggingContext) async -> String { \n  let span = InstrumentationSystem.tracer.startSpan(operationName: #function, context: context)\n  defer { span.end() }\n  \n  return await someWork()\n}\n```\n\n\n## In-Depth Guide\n\nWhen instrumenting server applications there are typically three parties involved:\n\n1. [Application developers](#application-developers-setting-up-instruments) creating server-side applications\n2. [Library/Framework developers](#libraryframework-developers-instrumenting-your-software) providing building blocks to create these applications\n3. [Instrument developers](#instrument-developers-creating-an-instrument) providing tools to collect distributed metadata about your application\n\nFor applications to be instrumented correctly these three parts have to play along nicely.\n\n## Application Developers\n\n### Setting up instruments & tracers\n\nAs an end-user building server applications you get to choose what instruments to use to instrument your system. Here's\nall the steps you need to take to get up and running:\n\nAdd a package dependency for this repository in your `Package.swift` file, and one for the specific instrument you want\nto use, in this case `FancyInstrument`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-distributed-tracing.git\", .branch(\"main\")),\n.package(url: \"<https://repo-of-fancy-instrument.git>\", from: \"<4.2.0>\"),\n```\n\nTo your main target, add a dependency on the `Instrumentation library` and the instrument you want to use:\n\n```swift\n.target(\n    name: \"MyApplication\", \n    dependencies: [\n        \"FancyInstrument\"\n    ]\n),\n```\n\n### Bootstrapping the `InstrumentationSystem`\n\nInstead of providing each instrumented library with a specific instrument explicitly, you *bootstrap* the\n`InstrumentationSystem` which acts as a singleton that libraries/frameworks access when calling out to the configured\n`Instrument`:\n\n```swift\nInstrumentationSystem.bootstrap(FancyInstrument())\n```\n\n#### Recommended bootstrap order\n\nSwift offers developers a suite of observability libraries: logging, metrics and tracing. Each of those systems offers a `bootstrap` function. It is useful to stick to a recommended boot order in order to achieve predictable initialization of applications and sub-systems.\n\nSpecifically, it is recommended to bootstrap systems in the following order:\n\n1. [Swift Log](https://github.com/apple/swift-log#default-logger-behavior)'s `LoggingSystem`\n2. [Swift Metrics](https://github.com/apple/swift-metrics#selecting-a-metrics-backend-implementation-applications-only)' `MetricsSystem`\n3. Swift Tracing's `InstrumentationSystem`\n4. Finally, any other parts of your application\n\nThis is because tracing systems may attempt to emit metrics about their status etc. \n\n#### Bootstrapping multiple instruments using MultiplexInstrument\n\nIt is important to note that `InstrumentationSystem.bootstrap(_: Instrument)` must only be called once. In case you\nwant to bootstrap the system to use multiple instruments, you group them in a `MultiplexInstrument` first, which you\nthen pass along to the `bootstrap` method like this:\n\n```swift\nInstrumentationSystem.bootstrap(MultiplexInstrument([FancyInstrument(), OtherFancyInstrument()]))\n```\n\n`MultiplexInstrument` will then call out to each instrument it has been initialized with.\n\n\n<a name=\"passing-context-objects\"></a>\n### Context propagation, by explicit `LoggingContext` passing\n\n> `LoggingContext` naming has been carefully selected and it reflects the type's purpose and utility: It binds a [Swift Log `Logger`](https://github.com/apple/swift-log) with an associated distributed tracing [Baggage](https://github.com/apple/swift-distributed-tracing-baggage).\n> \n> It _also_ is used for tracing, by tracers reaching in to read or modify the carried baggage. \n\nFor instrumentation and tracing to work, certain pieces of metadata (usually in the form of identifiers), must be\ncarried throughout the entire system\u2013including across process and service boundaries. Because of that, it's essential\nfor a context object to be passed around your application and the libraries/frameworks you depend on, but also carried\nover asynchronous boundaries like an HTTP call to another service of your app.\n\n`LoggingContext` should always be passed around explicitly.\n\nLibraries which support tracing are expected to accept a `LoggingContext` parameter, which can be passed through the entire application. Make sure to always pass along the context that's previously handed to you. E.g., when making an HTTP request using `AsyncHTTPClient` in a `NIO` handler, you can use the `ChannelHandlerContext`s `baggage` property to access the `LoggingContext`.\n\n#### Context argument naming/positioning\n\n> \ud83d\udca1 This general style recommendation has been ironed out together with the Swift standard library, core team, the SSWG as well as members of the community. Please respect these recommendations when designing APIs such that all APIs are able to \"feel the same\" yielding a great user experience for our end users \u2764\ufe0f\n> \n> It is possible that the ongoing Swift Concurrency efforts, and \"Task Local\" values will resolve this explicit context passing problem, however until these arrive in the language, please adopt the \"context is the last parameter\" style as outlined here.\n\nPropagating baggage context through your system is to be done explicitly, meaning as a parameter in function calls, following the \"flow\" of execution.\n\nWhen passing baggage context explicitly we strongly suggest sticking to the following style guideline:\n\n- Assuming the general parameter ordering of Swift function is as follows (except DSL exceptions):\n  1. Required non-function parameters (e.g. `(url: String)`),\n  2. Defaulted non-function parameters (e.g. `(mode: Mode = .default)`),\n  3. Required function parameters, including required trailing closures (e.g. `(onNext elementHandler: (Value) -> ())`),\n  4. Defaulted function parameters, including optional trailing closures (e.g. `(onComplete completionHandler: (Reason) -> ()) = { _ in }`).\n- Logging Context should be passed as **the last parameter in the required non-function parameters group in a function declaration**.\n\nThis way when reading the call side, users of these APIs can learn to \"ignore\" or \"skim over\" the context parameter and the method signature remains human-readable and \u201cSwifty\u201d.\n\nExamples:\n\n- `func request(_ url: URL,` **`context: LoggingContext`** `)`, which may be called as `httpClient.request(url, context: context)`\n- `func handle(_ request: RequestObject,` **`context: LoggingContext`** `)`\n  - if a \"framework context\" exists and _carries_ the baggage context already, it is permitted to pass that context\n    together with the baggage;\n  - it is _strongly recommended_ to store the baggage context as `baggage` property of `FrameworkContext`, and conform `FrameworkContext` to `LoggingContext` in such cases, in order to avoid the confusing spelling of `context.context`, and favoring the self-explanatory `context.baggage` spelling when the baggage is contained in a framework context object.\n- `func receiveMessage(_ message: Message, context: FrameworkContext)`\n- `func handle(element: Element,` **`context: LoggingContext`** `, settings: Settings? = nil)`\n  - before any defaulted non-function parameters\n- `func handle(element: Element,` **`context: LoggingContext`** `, settings: Settings? = nil, onComplete: () -> ())`\n  - before defaulted parameters, which themselfes are before required function parameters\n- `func handle(element: Element,` **`context: LoggingContext`** `, onError: (Error) -> (), onComplete: (() -> ())? = nil)`\n\nIn case there are _multiple_ \"framework-ish\" parameters, such as passing a NIO `EventLoop` or similar, we suggest:\n\n- `func perform(_ work: Work, for user: User,` _`frameworkThing: Thing, eventLoop: NIO.EventLoop,`_ **`context: LoggingContext`** `)`\n  - pass the baggage as **last** of such non-domain specific parameters as it will be _by far more_ omnipresent than any\n    specific framework parameter - as it is expected that any framework should be accepting a context if it can do so.\n    While not all libraries are necessarily going to be implemented using the same frameworks.\n\nWe feel it is important to preserve Swift's human-readable nature of function definitions. In other words, we intend to\nkeep the read-out-loud phrasing of methods to remain _\"request that URL (ignore reading out loud the context parameter)\"_\nrather than _\"request (ignore this context parameter when reading) that URL\"_.\n\n#### When to use what context type?\n\nGenerally libraries should favor accepting the general `LoggingContext` type, and **not** attempt to wrap it, as it will result in difficult to compose APIs between multiple libraries. Because end users are likely going to be combining various libraries in a single application, it is important that they can \"just pass along\" the same context object through all APIs, regardless which other library they are calling into.\n\nFrameworks may need to be more opinionated here, and e.g. already have some form of \"per request context\" contextual object which they will conform to `LoggingContext`. _Within_ such framework it is fine and expected to accept and pass the explicit `SomeFrameworkContext`, however when designing APIs which may be called _by_ other libraries, such framework should be able to accept a generic `LoggingContext` rather than its own specific type.\n\n#### Existing context argument\n\nWhen adapting an existing library/framework to support `LoggingContext` and it already has a \"framework context\" which is expected to be passed through \"everywhere\", we suggest to follow these guidelines for adopting LoggingContext:\n\n1. Add a `Baggage` as a property called `baggage` to your own `context` type, so that the call side for your\n   users becomes `context.baggage` (rather than the confusing `context.context`)\n2. If you cannot or it would not make sense to carry baggage inside your framework's context object, pass (and accept (!)) the `LoggingContext` in your framework functions like follows:\n- if they take no framework context, accept a `context: LoggingContext` which is the same guideline as for all other cases\n- if they already _must_ take a context object and you are out of words (or your API already accepts your framework context as \"context\"), pass the baggage as **last** parameter (see above) yet call the parameter `baggage` to disambiguate your `context` object from the `baggage` context object.\n\nExamples:\n\n- `Lamda.Context` may contain `baggage` and a `logger` and should be able to conform to `LoggingContext`\n  - passing context to a `Lambda.Context` unaware library becomes: `http.request(url: \"...\", context: context)`.\n- `ChannelHandlerContext` offers a way to set/get baggage on the underlying channel via `context.baggage = ...`\n  - this context is not passed outside a handler, but within it may be passed as is, and the baggage may be accessed on it directly through it.\n  - Example: https://github.com/apple/swift-nio/pull/1574\n\n### Creating context objects (and when not to do so)\n\nGenerally application developers _should not_ create new context objects, but rather keep passing on a context value that they were given by e.g. the web framework invoking the their code. \n\nIf really necessary, or for the purposes of testing, one can create a baggage or context using one of the two factory functions:\n\n- [`DefaultLoggingContext.topLevel(logger:)`](https://github.com/apple/swift-distributed-tracing-baggage/blob/main/Sources/Baggage/LoggingContext.swift#L232-L259) or [`Baggage.topLevel`](https://github.com/apple/swift-distributed-tracing-baggage-core/blob/main/Sources/CoreBaggage/Baggage.swift#L79-L103) - which creates an empty context/baggage, without any values. It should _not_ be used too frequently, and as the name implies in applications it only should be used on the \"top level\" of the application, or at the beginning of a contextless (e.g. timer triggered) event processing.\n- [`DefaultLoggingContext.TODO(logger:reason:)`](https://github.com/apple/swift-distributed-tracing-baggage/blob/main/Sources/Baggage/LoggingContext.swift#L262-L292) or [`Baggage.TODO`](https://github.com/apple/swift-distributed-tracing-baggage-core/blob/main/Sources/CoreBaggage/Baggage.swift#L107-L136) - which should be used to mark a parameter where \"before this code goes into production, a real context should be passed instead.\" An application can be run with `-DBAGGAGE_CRASH_TODOS` to cause the application to crash whenever a TODO context is still in use somewhere, making it easy to diagnose and avoid breaking context propagation by accidentally leaving in a `TODO` context in production.\n\nPlease refer to the respective functions documentation for details.\n\nIf using a framework which itself has a \"`...Context`\" object you may want to inspect it for similar factory functions, as `LoggingContext` is a protocol, that may be conformed to by frameworks to provide a smoother user experience.\n\n<a name=\"spans\"></a>\n### Working with `Span`s\n\nThe primary purpose of this API is to start and end so-called `Span` types.\n\nSpans form hierarchies with their parent spans, and end up being visualized using various tools, usually in a format similar to gant charts. So for example, if we had multiple operations that compose making dinner, they would be modelled as child spans of a main `makeDinner` span. Any sub tasks are again modelled as child spans of any given operation, and so on, resulting in a trace view similar to:\n\n```\n>-o-o-o----- makeDinner ----------------o---------------x    [15s]\n  \\-|-|- chopVegetables--------x        |                    [2s]\n    | |  \\- chop -x |                   |                    [1s]\n    | |             \\--- chop -x        |                    [1s]\n    \\-|- marinateMeat -----------x      |                    [3s]\n      \\- preheatOven -----------------x |                    [10s]\n                                        \\--cook---------x    [5s]\n```\n\nThe above trace is achieved by starting and ending spans in all the mentioned functions, for example, like this:\n\n```swift\nlet tracer: Tracer\n\nfunc makeDinner(context: LoggingContext) async throws -> Meal {\n  tracer.withSpan(operationName: \"makeDinner\", context) {\n    let veggiesFuture = try chopVegetables(context: span.context)\n    let meatFuture = marinateMeat(context: span.context)\n    let ovenFuture = try preheatOven(temperature: 350, context: span.context)\n    ...\n    return cook(veggies, meat, oven)\n  }\n}\n```\n\n> \u2757\ufe0f It is tremendously important to **always `end()` a started `Span`**! make sure to end any started span on _every_ code path, including error paths \n> \n> Failing to do so is an error, and a tracer *may* decide to either crash the application or log warnings when an not-ended span is deinitialized.\n\n\n## Library/Framework developers: Instrumenting your software\n\n### Extracting & injecting Baggage\n\nWhen hitting boundaries like an outgoing HTTP request you call out to the [configured instrument(s)](#Bootstrapping-the-Instrumentation-System):\n\nAn HTTP client e.g. should inject the given `LoggingContext` into the HTTP headers of its outbound request:\n\n```swift\nfunc get(url: String, context: LoggingContext) {\n  var request = HTTPRequest(url: url)\n  InstrumentationSystem.instrument.inject(\n    context.baggage,\n    into: &request.headers,\n    using: HTTPHeadersInjector()\n  )\n}\n```\n\nOn the receiving side, an HTTP server should use the following `Instrument` API to extract the HTTP headers of the given\n`HTTPRequest` into:\n\n```swift\nfunc handler(request: HTTPRequest, context: LoggingContext) {\n  InstrumentationSystem.instrument.extract(\n    request.headers,\n    into: &context.baggage,\n    using: HTTPHeadersExtractor()\n  )\n  // ...\n}\n```\n\n> In case your library makes use of the `NIOHTTP1.HTTPHeaders` type we already have an `HTTPHeadersInjector` &\n`HTTPHeadersExtractor` available as part of the `NIOInstrumentation` library.\n\nFor your library/framework to be able to carry `LoggingContext` across asynchronous boundaries, it's crucial that you carry the context throughout your entire call chain in order to avoid dropping metadata.\n\n### Tracing your library\n\nWhen your library/framework can benefit from tracing, you should make use of it by integrating the `Tracing` library. \n\nIn order to work with the tracer [configured by the end-user](#Bootstrapping-the-Instrumentation-System), it adds a property to `InstrumentationSystem` that gives you back a `Tracer`. You can then use that tracer to start `Span`s. In an HTTP client you e.g.\nshould start a `Span` when sending the outgoing HTTP request:\n\n```swift\nfunc get(url: String, context: LoggingContext) {\n  var request = HTTPRequest(url: url)\n\n  // inject the request headers into the baggage as explained above\n\n  // start a span for the outgoing request\n  let tracer = InstrumentationSystem.tracer\n  var span = tracer.startSpan(named: \"HTTP GET\", context: context, ofKind: .client)\n\n  // set attributes on the span\n  span.attributes.http.method = \"GET\"\n  // ...\n\n  self.execute(request).always { _ in\n    // set some more attributes & potentially record an error\n\n    // end the span\n    span.end()\n  }\n}\n```\n\n> \u26a0\ufe0f Make sure to ALWAYS end spans. Ensure that all paths taken by the code will result in ending the span.\n> Make sure that error cases also set the error attribute and end the span.\n\n> In the above example we used the semantic `http.method` attribute that gets exposed via the\n`TracingOpenTelemetrySupport` library.\n\n## Instrument developers: Creating an instrument\n\nCreating an instrument means adopting the `Instrument` protocol (or `Tracer` in case you develop a tracer).\n`Instrument` is part of the `Instrumentation` library & `Tracing` contains the `Tracer` protocol.\n\n`Instrument` has two requirements:\n\n1. A method to inject values inside a `LoggingContext` into a generic carrier (e.g. HTTP headers)\n2. A method to extract values from a generic carrier (e.g. HTTP headers) and store them in a `LoggingContext`\n\nThe two methods will be called by instrumented libraries/frameworks at asynchronous boundaries, giving you a chance to\nact on the provided information or to add additional information to be carried across these boundaries.\n\n> Check out the [`Baggage` documentation](https://github.com/apple/swift-distributed-tracing-baggage) for more information on\nhow to retrieve values from the `LoggingContext` and how to set values on it.\n\n### Creating a `Tracer`\n\nWhen creating a tracer you need to create two types:\n\n1. Your tracer conforming to `Tracer`\n2. A span class conforming to `Span`\n\n> The `Span` conforms to the standard rules defined in [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-specification/blob/v0.7.0/specification/trace/api.md#span), so if unsure about usage patterns, you can refer to this specification and examples referring to it.\n\n### Defining, injecting and extracting Baggage\n\n```swift\nimport Tracing\n\nprivate enum TraceIDKey: BaggageKey {\n  typealias Value = String\n}\n\nextension Baggage {\n  var traceID: String? {\n    get {\n      return self[TraceIDKey.self]\n    }\n    set {\n      self[TraceIDKey.self] = newValue\n    }\n  }\n}\n\nvar context = DefaultLoggingContext.topLevel(logger: ...)\ncontext.baggage.traceID = \"4bf92f3577b34da6a3ce929d0e0e4736\"\nprint(context.baggage.traceID ?? \"new trace id\")\n```\n\n## Contributing\n\nPlease make sure to run the `./scripts/soundness.sh` script when contributing, it checks formatting and similar things.\n\nYou can ensure it always is run and passes before you push by installing a pre-push hook with git:\n\n``` sh\necho './scripts/soundness.sh' > .git/hooks/pre-push\n```\n\n### Formatting \n\nWe use a specific version of [`nicklockwood/swiftformat`](https://github.com/nicklockwood/swiftformat).\nPlease take a look at our [`Dockerfile`](docker/Dockerfile) to see which version is currently being used and install it\non your machine before running the script.\n"
 },
 {
  "repo": "apple/swift-distributed-tracing-baggage",
  "language": "Swift",
  "readme_contents": "# \ud83e\uddf3 Distributed Tracing: Baggage\n\n[![Swift 5.2](https://img.shields.io/badge/Swift-5.2-ED523F.svg?style=flat)](https://swift.org/download/)\n[![Swift 5.3](https://img.shields.io/badge/Swift-5.3-ED523F.svg?style=flat)](https://swift.org/download/)\n[![Swift 5.4](https://img.shields.io/badge/Swift-5.4-ED523F.svg?style=flat)](https://swift.org/download/)\n[![Swift 5.5](https://img.shields.io/badge/Swift-5.5-ED523F.svg?style=flat)](https://swift.org/download/)\n\n> \u26a0\ufe0f Automatic propagation through task-locals only supported in Swift >= 5.5\n\n`Baggage` is a minimal (zero-dependency) context propagation container, intended to \"carry\" baggage items\nfor purposes of cross-cutting tools to be built on top of it.\n\nIt is modeled after the concepts explained in [W3C Baggage](https://w3c.github.io/baggage/) and the \nin the spirit of [Tracing Plane](https://cs.brown.edu/~jcmace/papers/mace18universal.pdf) 's \"Baggage Context\" type,\nalthough by itself it does not define a specific serialization format.\n\nSee https://github.com/apple/swift-distributed-tracing for actual instrument types and implementations which can be used to\ndeploy various cross-cutting instruments all reusing the same baggage type. More information can be found in the\n[SSWG meeting notes](https://gist.github.com/ktoso/4d160232407e4d5835b5ba700c73de37#swift-baggage-context--distributed-tracing).\n\n## Dependency\n\n In order to depend on this library you can use the Swift Package Manager, and add the following dependency to your `Package.swift`:\n\n```swift\ndependencies: [\n  .package(\n    url: \"https://github.com/apple/swift-distributed-tracing-baggage.git\",\n    from: \"0.2.0\"\n  )\n]\n```\n\nand depend on the module in your target:\n\n```swift \ntargets: [\n    .target(\n        name: \"MyAwesomeApp\",\n        dependencies: [\n            .product(\n              name: \"InstrumentationBaggage\", \n              package: \"swift-distributed-tracing-baggage\"\n            ),\n        ]\n    ),\n    // ... \n]\n```\n\n## Documentation\n\nPlease refer to in-depth discussion and documentation in the [Swift Distributed Tracing](https://github.com/apple/swift-distributed-tracing) repository.\n\n## Contributing\n\nPlease make sure to run the `./scripts/soundness.sh` script when contributing, it checks formatting and similar things.\n\nYou can ensure it always runs and passes before you push by installing a pre-push hook with git:\n\n```\necho './scripts/soundness.sh' > .git/hooks/pre-push\nchmod +x .git/hooks/pre-push\n```\n"
 },
 {
  "repo": "apple/swift-nio-ssl",
  "language": "C",
  "readme_contents": "# SwiftNIO SSL\n\nSwiftNIO SSL is a Swift package that contains an implementation of TLS based on BoringSSL. This package allows users of [SwiftNIO](https://github.com/apple/swift-nio) to write protocol clients and servers that use TLS to secure data in flight.\n\nThe name is inspired primarily by the names of the library this package uses (BoringSSL), and not because we don't know the name of the protocol. We know the protocol is TLS!\n\nTo get started, check out the [API docs](https://apple.github.io/swift-nio-ssl/docs/current/NIOSSL/index.html).\n\n## Using SwiftNIO SSL\n\nSwiftNIO SSL provides two `ChannelHandler`s to use to secure a data stream: the `NIOSSLClientHandler` and the `NIOSSLServerHandler`. Each of these can be added to a `Channel` to secure the communications on that channel.\n\nAdditionally, we provide a number of low-level primitives for configuring your TLS connections. These will be shown below.\n\nTo secure a server connection, you will need a X.509 certificate chain in a file (either PEM or DER, but PEM is far easier), and the associated private key for the leaf certificate. These objects can then be wrapped up in a `TLSConfiguration` object that is used to initialize the `ChannelHandler`.\n\nFor example:\n\n```swift\nlet configuration = TLSConfiguration.makeServerConfiguration(\n    certificateChain: try NIOSSLCertificate.fromPEMFile(\"cert.pem\").map { .certificate($0) },\n    privateKey: .file(\"key.pem\")\n)\nlet sslContext = try NIOSSLContext(configuration: configuration)\n\nlet server = ServerBootstrap(group: group)\n    .childChannelInitializer { channel in\n        // important: The handler must be initialized _inside_ the `childChannelInitializer`\n        let handler = try NIOSSLServerHandler(context: sslContext)\n\n        [...]\n        channel.pipeline.addHandler(handler)\n        [...]\n    }\n```\n\nFor clients, it is a bit simpler as there is no need to have a certificate chain or private key (though clients *may* have these things). Setup for clients may be done like this:\n\n```swift\nlet configuration = TLSConfiguration.makeClientConfiguration()\nlet sslContext = try NIOSSLContext(configuration: configuration)\n\nlet client = ClientBootstrap(group: group)\n    .channelInitializer { channel in\n        // important: The handler must be initialized _inside_ the `channelInitializer`\n        let handler = try NIOSSLClientHandler(context: sslContext)\n\n        [...]\n        channel.pipeline.addHandler(handler)\n        [...]\n    }\n```\nNote that SwiftNIO SSL currently requires Swift 5.2 and above. Release 2.13.x and prior support Swift 5.0 and 5.1\n"
 },
 {
  "repo": "apple/swift-tools-support-async",
  "language": "Swift",
  "readme_contents": "# swift-tools-support-async\n\nCommon infrastructural helpers on top of NIO for [llbuild2](https://github.com/apple/swift-llbuild2) and [swiftpm-on-llbuild2](https://github.com/apple/swiftpm-on-llbuild2) projects. This is **NOT** a general purpose package and is unlikely to ever become stable.\n\n## License\n\nCopyright (c) 2020 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee http://swift.org/LICENSE.txt for license information.\n\nSee http://swift.org/CONTRIBUTORS.txt for Swift project authors."
 },
 {
  "repo": "apple/swift-package-collection-generator",
  "language": "Swift",
  "readme_contents": "# Swift Package Collection Generator\n\nA **package collection** ([SE-0291](https://github.com/apple/swift-evolution/blob/main/proposals/0291-package-collections.md))\nis a curated list of packages and associated metadata which makes it easier to discover an existing package for a particular use\ncase. SwiftPM will allow users to subscribe to package collections and make their contents accessible to any clients of libSwiftPM.\n\nThis repository provides a set of Swift packages and tooling for the generation and consumption of package collections.\n\nSwift toolchain version 5.4 or greater is required.\n\n#### Branches\n\nThe `main` branch depends on SwiftPM's `main` branch and may be unstable. It is recommended to use versioned branches such as `5.5`, which depend on the corresponding SwiftPM's `release/<version>` branch, instead.\n\n\n## Installation\n\nCurrently, the package collection generator is a standalone tool that's not integrated with the Swift toolchain. To use it from the command line, first build the project from source by cloning the repository and running the following from the root directory:\n\n```zsh\nswift build --configuration release\n```\n\nThen, either run the final executables directly (e.g., `.build/release/package-collection-generate`) or install them on your system path:\n\n```zsh\ninstall .build/release/package-collection-generate /usr/local/bin/package-collection-generate\ninstall .build/release/package-collection-diff /usr/local/bin/package-collection-diff\ninstall .build/release/package-collection-sign /usr/local/bin/package-collection-sign\ninstall .build/release/package-collection-validate /usr/local/bin/package-collection-validate\n```\n\nFinally, another way to run the tool is via `swift run`, which builds and runs the specified executable. For example:\n\n```zsh\nswift run package-collection-generate\n```\n\n## Package Collection Format\n\nPackage collections can be created and published by anyone. To make sure SwiftPM can consume\nthem, all package collections must adhere to the same format. See the [v1 format](PackageCollectionFormats/v1.md)\nfor details.\n\n## Generating a Package Collection\n\n[`package-collection-generate`](Sources/PackageCollectionGenerator/README.md) is a Swift\ncommand-line tool that helps generate package collections.\n\n## Validating a Package Collection\n\n[`package-collection-validate`](Sources/PackageCollectionValidator/README.md) is a Swift\ncommand-line tool that validates package collections against the defined format.\n- This should be run against the final output generated by the `package-collection-generate` command.\n\n## Comparing Package Collections\n\n[`package-collection-diff`](Sources/PackageCollectionDiff/README.md) is a Swift\ncommand-line tool that compares two package collections to determine if they are different from each other.\n"
 },
 {
  "repo": "apple/swift-crypto",
  "language": "C",
  "readme_contents": "# Swift Crypto\n\nSwift Crypto is an open-source implementation of a substantial portion of the API of [Apple CryptoKit](https://developer.apple.com/documentation/cryptokit) suitable for use on Linux platforms. It enables cross-platform or server applications with the advantages of CryptoKit.\n\n## Using Swift Crypto\n\nSwift Crypto is available as a Swift Package Manager package. To use it, add the following dependency in your `Package.swift`:\n\n```swift\n// swift-crypto 1.x and 2.x are almost API compatible, so most clients should\n// allow either\n.package(url: \"https://github.com/apple/swift-crypto.git\", \"1.0.0\" ..< \"3.0.0\"),\n```\n\nand to your target, add `Crypto` to your dependencies. You can then `import Crypto` to get access to Swift Crypto's functionality.\n\n## Functionality\n\nSwift Crypto exposes the portions of the CryptoKit API that do not rely on specialised hardware to any Swift application. It provides safe APIs that abstract over the complexity of many cryptographic primitives that need to be used in modern applications. These APIs encourage safe usages of the underlying primitives, follow cryptographic best practices, and should be the first choice for building applications that need to use cryptography.\n\nThe current features of Swift Crypto cover key exchange, key derivation, encryption and decryption, hashing, message authentication, and more.\n\nFor specific API documentation, please see our documentation.\n\n## Implementation\n\nSwift Crypto compiles in two distinct modes depending on the platform for which it is being built.\n\nWhen building Swift Crypto for use on an Apple platform where CryptoKit is already available, Swift Crypto compiles its entire API surface down to nothing and simply re-exports the API of CryptoKit. This means that when using Apple platforms Swift Crypto simply delegates all work to the core implementation of CryptoKit, as though Swift Crypto was not even there.\n\nWhen building Swift Crypto for use on Linux, Swift Crypto builds substantially more code. In particular, we build:\n\n1. A vendored copy of BoringSSL's libcrypto.\n2. The common API of Swift Crypto and CryptoKit.\n3. The backing implementation of this common API, which calls into BoringSSL.\n\nThe API code, and some cryptographic primitives which are directly implemented in Swift, are exactly the same for both Apple CryptoKit and Swift Crypto. The backing BoringSSL-based implementation is unique to Swift Crypto.\n\n## Evolution\n\nThe vast majority of the Swift Crypto code is intended to remain in lockstep with the current version of Apple CryptoKit. For this reason, patches that extend the API of Swift Crypto will be evaluated cautiously. For any such extension there are two possible outcomes for adding the API.\n\nFirstly, if the API is judged to be generally valuable and suitable for contribution to Apple CryptoKit, the API will be merged into a Staging namespace in Swift Crypto. This Staging namespace is a temporary home for any API that is expected to become available in Apple CryptoKit but that is not available today. This enables users to use the API soon after merging. When the API is generally available in CryptoKit the API will be deprecated in the Staging namespace and made available in the main Swift Crypto namespace.\n\nSecondly, if the API is judged not to meet the criteria for acceptance in general CryptoKit but is sufficiently important to have available for server use-cases, it will be merged into a Server namespace. APIs are not expected to leave this namespace, as it indicates that they are not generally available but can only be accessed when using Swift Crypto.\n\nNote that Swift Crypto does not intend to support all possible cryptographic primitives. Swift Crypto will focus on safe, modern cryptographic primitives that are broadly useful and that do not easily lend themselves to misuse. This means that some cryptographic algorithms may never be supported: for example, 3DES is highly unlikely to ever be supported by Swift Crypto due to the difficulty of safely deploying it and its legacy status. Please be aware when proposing the addition of new primitives to Swift Crypto that the proposal may be refused for this reason.\n\n### Code Organisation\n\nFiles in this repository are divided into two groups, based on whether they have a name that ends in `_boring` or are in a `BoringSSL` directory, or if they are not.\n\nFiles that meet the above criteria are specific to the Swift Crypto implementation. Changes to these files can be made fairly easily, so long as they meet the criteria below. If your file needs to `import CCryptoBoringSSL` or access a BoringSSL API, it needs to be marked this way.\n\nFiles that do not have the `_boring` suffix are part of the public API of CryptoKit. Changing these requires passing a higher bar, as any change in these files must be accompanied by a change in CryptoKit itself.\n\n## Contributing\n\nBefore contributing please read [CONTRIBUTING.md](CONTRIBUTING.md), also make sure to read the two following sections.\n\n#### Contributing new primitives\n\nTo contribute a new cryptographic primitive to Swift Crypto, you should address the following questions:\n\n1. What is the new primitive for?\n2. How widely is it deployed?\n3. Is it specified in any public specifications or used by any such specification?\n4. How easy is it to misuse?\n5. In what way does Swift Crypto fail to satisfy that use-case today?\n\nIn addition, new primitive implementations will only be accepted in cases where the implementation is thoroughly tested, including being tested with all currently available test vectors. If the [Wycheproof](https://github.com/google/wycheproof) project provides vectors for the algorithm those should be tested as well. It must be possible to ensure that we can appropriately regression test our implementations.\n\n#### Contributing bug fixes\n\nIf you discover a bug with Swift Crypto, please report it via GitHub.\n\nIf you are interested in fixing a bug, feel free to open a pull request. Please also submit regression tests with bug fixes to ensure that they are not regressed in future.\n\nIf you have issues with CryptoKit, instead of Swift Crypto, please use [Feedback Assistant](https://feedbackassistant.apple.com) to file those issues as you normally would.\n\n### Get started contributing\n\n#### `gyb`\n\nSome of the files in this project are autogenerated (metaprogramming) using the Swift Utils tools called [gyb](https://github.com/apple/swift/blob/main/utils/gyb.py) (_\"generate your boilerplate\"_). `gyb` is included in [`./scripts/gyb`](scripts/gyb).\n\n`gyb` will generate some `Foobar.swift` Swift file from some `Foobar.swift.gyb` _template_ file. **You should not edit `Foobar.swift` directly**, since all manual edits in that generated file will be overwritten the next time `gyb` is run.\n\nYou run `gyb` for a single file like so:\n\n```bash\n./scripts/gyb --line-directive \"\" Sources/Foobar.swift.gyb -o Sources/Foobar.swift\n```\n\nMore conveniently you can run the bash script `./scripts/generate_boilerplate_files_with_gyb.sh` to generate all Swift files from their corresponding gyb template.\n\n**If you add a new `.gyb` file, you should append a `// MARK: - Generated file, do NOT edit` warning** inside it, e.g.\n\n```swift\n// MARK: - Generated file, do NOT edit\n// any edits of this file WILL be overwritten and thus discarded\n// see section `gyb` in `README` for details.\n```\n\n### Security\n\nIf you believe you have identified a vulnerability in Swift Crypto, please [report that vulnerability to Apple through the usual channel](https://support.apple.com/en-us/HT201220).\n\n### Swift versions\n\nSwift Crypto supports Swift 5.2 and later. Swift Crypto supported Swift 5.1 until version 1.2.\n\n### Compatibility\n\nSwift Crypto follows [SemVer 2.0.0](https://semver.org/#semantic-versioning-200). Our public API is the same as that of CryptoKit (except where we lack an implementation entirely), as well as everything in the Server and Staging namespaces. Any symbol beginning with an underscore, and any product beginning with an underscore, is not subject to semantic versioning: these APIs may change without warning. We do not maintain a stable ABI, as Swift Crypto is a source-only distribution.\n\nWhat this means for you is that you should depend on Swift Crypto with a version range that covers everything from the minimum Swift Crypto version you require up to the next major version.\nIn SwiftPM that can be easily done specifying for example `from: \"1.0.0\"` meaning that you support Swift Crypto in every version starting from 1.0.0 up to (excluding) 2.0.0.\nSemVer and Swift Crypto's Public API guarantees should result in a working program without having to worry about testing every single version for compatibility.\n\nSwift Crypto 2.0.0 was released in September 2021. The only breaking change between Swift Crypto 2.0.0 and 1.0.0 was the addition of new cases in the `CryptoKitError` enumeration. For most users, then, it's safe to depend on either the 1.0.0 _or_ 2.0.0 series of releases.\n\nTo do so, please use the following dependency in your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-crypto.git\", \"1.0.0\" ..< \"3.0.0\"),\n```\n\n### Developing Swift Crypto on macOS\n\nSwift Crypto normally defers to the OS implementation of CryptoKit on macOS. Naturally, this makes developing Swift Crypto on macOS tricky. To get Swift Crypto to build the open source implementation on macOS, in `Package.swift`, uncomment the line that reads: `//.define(\"CRYPTO_IN_SWIFTPM_FORCE_BUILD_API\")`, as this will force Swift Crypto to build its public API.\n\n"
 },
 {
  "repo": "apple/swift-docc-plugin",
  "language": "Swift",
  "readme_contents": "# Swift-DocC Plugin\n\nThe Swift-DocC plugin is a Swift Package Manager command plugin that supports building\ndocumentation for SwiftPM libraries and executables.\n\n## Usage\n\nPlease see \n[the plugin's documentation](https://apple.github.io/swift-docc-plugin/documentation/swiftdoccplugin/)\nfor more detailed usage instructions.\n\n**Note:** The Swift-DocC plugin is under **active-development** and is not ready for production\nuse. \n\nWe anticipate releasing a `1.0` version of the Swift-DocC plugin aligned with\nthe release of Swift `5.6`.\n\n### Adding the Swift-DocC Plugin as a Dependency\n\nTo use the Swift-DocC plugin with your package, first add it as a dependency:\n\n```swift\nlet package = Package(\n    // name, platforms, products, etc.\n    dependencies: [\n        // other dependencies\n        .package(url: \"https://github.com/apple/swift-docc-plugin\", branch: \"main\"),\n    ],\n    targets: [\n        // targets\n    ]\n)\n```\n\nSwift 5.6 is required in order to run the plugin. Development snapshots that include Swift 5.6\ncan be found on [Swift.org](https://www.swift.org/download/#snapshots).\n\n### Converting Documentation\n\nYou can then invoke the plugin from the root of your repository like so:\n\n```shell\nswift package generate-documentation\n```\n\nThis will generate documentation for all compatible targets in your package and print\nthe location of the resulting DocC archives.\n\nIf you'd like to generate documentation for a specific target and output that\nto a specific directory, you can do something like the following:\n\n```shell\nswift package --allow-writing-to-directory ./docs --target MyFramework \\\n    generate-documentation --output-path ./docs\n```\n\nNotice that the output path must also be passed to SwiftPM via the \n`--allow-writing-to-directory` option. Otherwise SwiftPM will throw an error\nas it's a sandbox violation for a plugin to write to a package directory without explicit\npermission.\n\nAny flag passed after the `generate-documentation` plugin invocation is passed\nalong to the `docc` command-line tool. For example, to take advantage of Swift-DocC's new support\nfor hosting in static environments like GitHub Pages, you could run the following:\n\n```shell\nswift package --allow-writing-to-directory ./docs --target MyFramework \\\n    generate-documentation --output-path ./docs \\\n    --transform-for-static-hosting --hosting-base-path MyFramework\n```\n\n### Previewing Documentation\n\nThe Swift-DocC plugin also supports previewing documentation with a local web server. However,\nunlike converting documentation, previewing is limited to a single target a time.\n\nTo preview documentation for the MyFramework target, you could run the following:\n\n```shell\nswift package --disable-sandbox --target MyFramework preview-documentation\n```\n\n### Hosting Documentation\n\nFor details on how to best build documentation for hosting online and a specific\ntutorial for publishing to GitHub Pages, please see \n[the plugin's documentation](https://apple.github.io/swift-docc-plugin/documentation/swiftdoccplugin/).\n\n### Submitting a Feature Request\n\nFor feature requests, please feel free to create an issue\non [Swift JIRA](https://bugs.swift.org/) with the `New Feature` type\nor start a discussion on the [Swift Forums](https://forums.swift.org/c/development/swift-docc).\n\nDon't hesitate to submit a feature request if you see a way\nthe Swift-DocC plugin can be improved to better meet your needs.\n\nAll user-facing features must be discussed\nin the [Swift Forums](https://forums.swift.org/c/development/swift-docc)\nbefore being enabled by default.\n\n## Contributing to the Swift-DocC Plugin\n\nPlease see the [contributing guide](/CONTRIBUTING.md) for more information.\n\n<!-- Copyright (c) 2022 Apple Inc and the Swift Project authors. All Rights Reserved. -->\n"
 },
 {
  "repo": "apple/ARKitScenes",
  "language": "Python",
  "readme_contents": "# ARKitScenes\n\nThis repo accompanies the research paper, [ARKitScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding \nUsing Mobile RGB-D Data](https://openreview.net/forum?id=tjZjv_qh_CE) and contains the data, scripts to visualize \nand process assets, and training code described in our paper.\n\n![image](https://user-images.githubusercontent.com/7753049/144107932-39b010fc-6111-4b13-9c68-57dd903d78c5.png)\n\n![image](https://user-images.githubusercontent.com/7753049/144108052-6a1d3a67-3948-4ded-bd08-6f1572fdf97a.png)\n\nhttps://user-images.githubusercontent.com/7753049/145264511-73669f1f-cf27-4da9-8e81-8c002f3b3afa.mov\n\n## Paper\n[ARKitScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding \nUsing Mobile RGB-D Data](https://openreview.net/forum?id=tjZjv_qh_CE)\n\nupon using these data or source code, please cite\n```buildoutcfg\n@inproceedings{\ndehghan2021arkitscenes,\ntitle={{ARK}itScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},\nauthor={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Tal Dimry and Yuri Feigin and Peter Fu and Thomas Gebauer and Brandon Joffe and Daniel Kurz and Arik Schwartz and Elad Shulman},\nbooktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\nyear={2021},\nurl={https://openreview.net/forum?id=tjZjv_qh_CE}\n}\n```\n\n## Overview\nARKitScenes is not only the first RGB-D dataset that is captured with now widely available depth sensor, but also is the \nlargest indoor scene understanding data ever collected. In addition to the raw and processed data, ARKitScenes includes \nhigh resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding \nboxes for a large taxonomy of furniture. We further provide helper scripts for two downstream tasks: \n3D object detection and RGB-D guided upsampling. We hope that our dataset can help push the boundaries of \nexisting state-of-the-art methods and introduce new challenges that better represent real world scenarios.\n\n## Key features\n\u2022 ARKitScenes is the first RGB-D dataset captured with the widely available\nApple LiDAR scanner. Along with the raw data we provide the camera pose and surface\nreconstruction for each scene.\n\n\u2022 ARKitScenes is the largest indoor 3D dataset consisting of 5,047 captures of 1,661 unique\nscenes.\n\n\u2022 We provide high quality ground truth of (a) registered RGB-D frames and (b) oriented\nbounding boxes of room defining objects.\n\nBelow is an overview of RGB-D datasets and their ground truth assets compared with ARKitScenes.\nHR and LR represent High Resolution and Low Resolution respectively, and are available for a subset of 2,257 captures of 841 unique\nscenes.\n\n![image](https://user-images.githubusercontent.com/7753049/144108117-b789a5be-cc08-44f0-a76c-f1549c59825e.png)\n\n\n## Data collection\n\nIn the figure below, we provide  (a) illustration of iPad Pro scanning set up. (b) mesh overlay to assist data collection with iPad Pro. (c) example of one of the scan patterns captured with the iPad pro, the red markers show the chosen locations of the stationary laser scanner in that room.\n\n![image](https://user-images.githubusercontent.com/7753049/144108161-0ae7ba6a-305f-4a22-93b1-0b2d1e78154e.png)\n\n## Data download\n\nTo download the data please follow the [data](DATA.md) documentation\n \n## Tasks\n\nHere we provide the two tasks mentioned in our paper, namely, 3D Object Detection (3DOD) and depth upsampling.\n\n### [3DOD](threedod/README.md)\n\n### [Depth upsampling](depth_upsampling/README.md)\n\n## License\nThe ARKitScenes dataset is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License. To view a copy of this license, visit https://creativecommons.org/licenses/by-nc-sa/4.0/.\nFor queries regarding a commercial license, contact ARKitScenes-license@group.apple.com\nIf you have any other questions raise an issue in the repository and contact ARKitScenes@group.apple.com\n"
 },
 {
  "repo": "apple/swift-metrics",
  "language": "Swift",
  "readme_contents": "# SwiftMetrics\n\nA Metrics API package for Swift.\n\nAlmost all production server software needs to emit metrics information for observability. Because it's unlikely that all parties can agree on one specific metrics backend implementation, this API is designed to establish a standard that can be implemented by various metrics libraries which then post the metrics data to backends like [Prometheus](https://prometheus.io/), [Graphite](https://graphiteapp.org), publish over [statsd](https://github.com/statsd/statsd), write to disk, etc.\n\nThis is the beginning of a community-driven open-source project actively seeking contributions, be it code, documentation, or ideas. Apart from contributing to SwiftMetrics itself, we need metrics compatible libraries which send the metrics over to backend such as the ones mentioned above. What SwiftMetrics provides today is covered in the [API docs](https://apple.github.io/swift-metrics/), but it will continue to evolve with community input.\n\n## Getting started\n\nIf you have a server-side Swift application, or maybe a cross-platform (e.g. Linux, macOS) application or library, and you would like to emit metrics, targeting this metrics API package is a great idea. Below you'll find all you need to know to get started.\n\n### Adding the dependency\n\nTo add a dependency on the metrics API package, you need to declare it in your `Package.swift`:\n\n```swift\n// swift-metrics 1.x and 2.x are almost API compatible, so most clients should use\n.package(url: \"https://github.com/apple/swift-metrics.git\", \"1.0.0\" ..< \"3.0.0\"),\n```\n\nand to your application/library target, add \"Metrics\" to your dependencies:\n\n```swift\n.target(\n    name: \"BestExampleApp\",\n    dependencies: [\n        // ... \n        .product(name: \"Metrics\", package: \"swift-metrics\"),\n    ]\n),\n```\n\n###  Emitting metrics information\n\n```swift\n// 1) let's import the metrics API package\nimport Metrics\n\n// 2) we need to create a concrete metric object, the label works similarly to a `DispatchQueue` label\nlet counter = Counter(label: \"com.example.BestExampleApp.numberOfRequests\")\n\n// 3) we're now ready to use it\ncounter.increment()\n```\n\n### Selecting a metrics backend implementation (applications only)\n\nNote: If you are building a library, you don't need to concern yourself with this section. It is the end users of your library (the applications) who will decide which metrics backend to use. Libraries should never change the metrics implementation as that is something owned by the application.\n\nSwiftMetrics only provides the metrics system API. As an application owner, you need to select a metrics backend (such as the ones mentioned above) to make the metrics information useful.\n\nSelecting a backend is done by adding a dependency on the desired backend client implementation and invoking the `MetricsSystem.bootstrap` function at the beginning of the program:\n\n```swift\nMetricsSystem.bootstrap(SelectedMetricsImplementation())\n```\n\nThis instructs the `MetricsSystem` to install `SelectedMetricsImplementation` (actual name will differ) as the metrics backend to use.\n\nAs the API has just launched, not many implementations exist yet. If you are interested in implementing one see the \"Implementing a metrics backend\" section below explaining how to do so. List of existing SwiftMetrics API compatible libraries:\n\n- [SwiftPrometheus](https://github.com/MrLotU/SwiftPrometheus), support for [Prometheus](https://prometheus.io)\n- [StatsD Client](https://github.com/apple/swift-statsd-client), support for StatsD\n- [OpenTelemetry Swift](https://github.com/open-telemetry/opentelemetry-swift), support for [OpenTelemetry](https://opentelemetry.io/) which also implements other metrics and tracing backends \n- Your library? [Get in touch!](https://forums.swift.org/c/server)\n\n### Swift Metrics Extras\n\nYou may also be interested in some \"extra\" modules which are collected in the [Swift Metrics Extras](https://github.com/apple/swift-metrics-extras) repository.\n\n## Detailed design\n\n### Architecture\n\nWe believe that for the Swift on Server ecosystem, it's crucial to have a metrics API that can be adopted by anybody so a multitude of libraries from different parties can all provide metrics information. More concretely this means that we believe all the metrics events from all libraries should end up in the same place, be one of the backends mentioned above or wherever else the application owner may choose.\n\nIn the real world, there are so many opinions over how exactly a metrics system should behave, how metrics should be aggregated and calculated, and where/how to persist them. We think it's not feasible to wait for one metrics package to support everything that a specific deployment needs while still being simple enough to use and remain performant. That's why we decided to split the problem into two:\n\n1. a metrics API\n2. a metrics backend implementation\n\nThis package only provides the metrics API itself, and therefore, SwiftMetrics is a \"metrics API package.\" SwiftMetrics can be configured (using `MetricsSystem.bootstrap`) to choose any compatible metrics backend implementation. This way, packages can adopt the API, and the application can choose any compatible metrics backend implementation without requiring any changes from any of the libraries.\n\nThis API was designed with the contributors to the Swift on Server community and approved by the SSWG (Swift Server Work Group) to the \"sandbox level\" of the SSWG's incubation process.\n\n[pitch](https://forums.swift.org/t/metrics/19353) |\n[discussion](https://forums.swift.org/t/discussion-server-metrics-api/) |\n[feedback](https://forums.swift.org/t/feedback-server-metrics-api/)\n\n### Metric types\n\nThe API supports four metric types:\n\n`Counter`: A counter is a cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart. For example, you can use a counter to represent the number of requests served, tasks completed, or errors.\n\n```swift\ncounter.increment(by: 100)\n```\n\n`Recorder`: A recorder collects observations within a time window (usually things like response sizes) and *can* provide aggregated information about the data sample, for example count, sum, min, max and various quantiles.\n\n```swift\nrecorder.record(100)\n```\n\n`Gauge`: A Gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \"counts\" that can go up and down, like the number of active threads. Gauges are modeled as a `Recorder` with a sample size of 1 that does not perform any aggregation.\n\n```swift\ngauge.record(100)\n```\n\n`Timer`: A timer collects observations within a time window (usually things like request duration) and provides aggregated information about the data sample, for example min, max and various quantiles. It is similar to a `Recorder` but specialized for values that represent durations.\n\n```swift\ntimer.recordMilliseconds(100)\n```\n\n### Implementing a metrics backend (e.g. Prometheus client library)\n\nNote: Unless you need to implement a custom metrics backend, everything in this section is likely not relevant, so please feel free to skip.\n\nAs seen above, each constructor for `Counter`, `Timer`, `Recorder` and `Gauge` provides a metric object. This uncertainty obscures the selected metrics backend calling these constructors by design. _Each application_ can select and configure its desired backend. The application sets up the metrics backend it wishes to use. Configuring the metrics backend is straightforward:\n\n```swift\nlet metricsImplementation = MyFavoriteMetricsImplementation()\nMetricsSystem.bootstrap(metricsImplementation)\n```\n\nThis instructs the `MetricsSystem` to install `MyFavoriteMetricsImplementation` as the metrics backend (`MetricsFactory`) to use. This should only be done once at the beginning of the program.\n\nGiven the above, an implementation of a metric backend needs to conform to `protocol MetricsFactory`:\n\n```swift\npublic protocol MetricsFactory {\n    func makeCounter(label: String, dimensions: [(String, String)]) -> CounterHandler\n    func makeRecorder(label: String, dimensions: [(String, String)], aggregate: Bool) -> RecorderHandler\n    func makeTimer(label: String, dimensions: [(String, String)]) -> TimerHandler\n\n    func destroyCounter(_ handler: CounterHandler)\n    func destroyRecorder(_ handler: RecorderHandler)\n    func destroyTimer(_ handler: TimerHandler)\n}\n```\n\nThe `MetricsFactory` is responsible for instantiating the concrete metrics classes that capture the metrics and perform aggregation and calculation of various quantiles as needed.\n\n**Counter**\n\n```swift\npublic protocol CounterHandler: AnyObject {\n    func increment(by: Int64)\n    func reset()\n}\n```\n\n**Timer**\n\n```swift\npublic protocol TimerHandler: AnyObject {\n    func recordNanoseconds(_ duration: Int64)\n}\n```\n\n**Recorder**\n\n```swift\npublic protocol RecorderHandler: AnyObject {\n    func record(_ value: Int64)\n    func record(_ value: Double)\n}\n```\n\n#### Dealing with Overflows\n\nImplementaton of metric objects that deal with integers, like `Counter` and `Timer` should be careful with overflow. The expected behavior is to cap at `.max`, and never crash the program due to overflow . For example:\n\n```swift\nclass ExampleCounter: CounterHandler {\n    var value: Int64 = 0\n    func increment(by amount: Int64) {\n        let result = self.value.addingReportingOverflow(amount)\n        if result.overflow {\n            self.value = Int64.max\n        } else {\n            self.value = result.partialValue\n        }\n    }\n}\n```\n\n#### Full example\n\nHere is a full, but contrived, example of an in-memory implementation:\n\n```swift\nclass SimpleMetricsLibrary: MetricsFactory {\n    init() {}\n\n    func makeCounter(label: String, dimensions: [(String, String)]) -> CounterHandler {\n        return ExampleCounter(label, dimensions)\n    }\n\n    func makeRecorder(label: String, dimensions: [(String, String)], aggregate: Bool) -> RecorderHandler {\n        let maker: (String, [(String, String)]) -> RecorderHandler = aggregate ? ExampleRecorder.init : ExampleGauge.init\n        return maker(label, dimensions)\n    }\n\n    func makeTimer(label: String, dimensions: [(String, String)]) -> TimerHandler {\n        return ExampleTimer(label, dimensions)\n    }\n\n    // implementation is stateless, so nothing to do on destroy calls\n    func destroyCounter(_ handler: CounterHandler) {}\n    func destroyRecorder(_ handler: RecorderHandler) {}\n    func destroyTimer(_ handler: TimerHandler) {}\n\n    private class ExampleCounter: CounterHandler {\n        init(_: String, _: [(String, String)]) {}\n\n        let lock = NSLock()\n        var value: Int64 = 0\n        func increment(by amount: Int64) {\n            self.lock.withLock {\n                self.value += amount\n            }\n        }\n\n        func reset() {\n            self.lock.withLock {\n                self.value = 0\n            }\n        }\n    }\n\n    private class ExampleRecorder: RecorderHandler {\n        init(_: String, _: [(String, String)]) {}\n\n        private let lock = NSLock()\n        var values = [(Int64, Double)]()\n        func record(_ value: Int64) {\n            self.record(Double(value))\n        }\n\n        func record(_ value: Double) {\n            // TODO: sliding window\n            lock.withLock {\n                values.append((Date().nanoSince1970, value))\n                self._count += 1\n                self._sum += value\n                self._min = Swift.min(self._min, value)\n                self._max = Swift.max(self._max, value)\n            }\n        }\n\n        var _sum: Double = 0\n        var sum: Double {\n            return self.lock.withLock { _sum }\n        }\n\n        private var _count: Int = 0\n        var count: Int {\n            return self.lock.withLock { _count }\n        }\n\n        private var _min: Double = 0\n        var min: Double {\n            return self.lock.withLock { _min }\n        }\n\n        private var _max: Double = 0\n        var max: Double {\n            return self.lock.withLock { _max }\n        }\n    }\n\n    private class ExampleGauge: RecorderHandler {\n        init(_: String, _: [(String, String)]) {}\n\n        let lock = NSLock()\n        var _value: Double = 0\n        func record(_ value: Int64) {\n            self.record(Double(value))\n        }\n\n        func record(_ value: Double) {\n            self.lock.withLock { _value = value }\n        }\n    }\n\n    private class ExampleTimer: ExampleRecorder, TimerHandler {\n        func recordNanoseconds(_ duration: Int64) {\n            super.record(duration)\n        }\n    }\n}\n```\n\n## Security\n\nPlease see [SECURITY.md](SECURITY.md) for details on the security process.\n\n## Getting involved\n\nDo not hesitate to get in touch as well, over on https://forums.swift.org/c/server\n"
 },
 {
  "repo": "apple/swift-metrics-extras",
  "language": "Swift",
  "readme_contents": "# SwiftMetricsExtras\n\nExtra packages complementing the core [SwiftMetrics](https://github.com/apple/swift-metrics) API.\n\nAlmost all production server software needs to emit metrics information for observability. Because it's unlikely that all parties can agree on one specific metrics backend implementation, this API is designed to establish a standard that can be implemented by various metrics libraries which then post the metrics data to backends like [Prometheus](https://prometheus.io/), [Graphite](https://graphiteapp.org), publish over [statsd](https://github.com/statsd/statsd), write to disk, etc.\n\nThis is the beginning of a community-driven open-source project actively seeking contributions, be it code, documentation, or ideas. Apart from contributing to SwiftMetrics itself, we need metrics compatible libraries which send the metrics over to backend such as the ones mentioned above. What SwiftMetrics provides today is covered in the [API docs](https://apple.github.io/swift-metrics/), but it will continue to evolve with community input.\n\n## What makes a good contribution to Metrics Extras?\n\nNot good: \n\n- Most metrics contributions depend or implement some specific metrics backend\u2013such implementations should have their own repository and are not good candidates for this repository.\n\nGood: \n\n- However, if you have some useful metrics helpers, such as e.g. gathering cloud provider specific metrics independent of actual metrics backend which they would be emitted to,\nor other such metric system agnostic metrics additions\u2013such additions are perfect examples of contributions very welcome to this package. \n\n### Adding the dependency\n\nTo add a dependency on the extras package, you need to declare it in your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-metrics-extras.git\", \"1.0.0\" ..< \"2.0.0\"),\n```\n\nand to your application/library target, add the specific module you would like to depend on to your dependencies:\n\n```swift\n.target(name: \"BestExampleApp\", dependencies: [\"ExampleExtraMetrics\"]),\n```\n\n## Modules\n\nSwift Metrics Extras ships the following extra modules:\n\n- [System Metrics](Sources/SystemMetrics)\n- [MetricsTestUtils](Sources/MetricsTestUtils)\n\n### System Metrics\n\nThe System Metrics package provides default process metrics for applications. The following metrics are exposed:\n\n- Virtual memory in Bytes.\n- Resident memory in Bytes.\n- Application start time in Seconds.\n- Total CPU seconds.\n- Maximum number of file descriptors.\n- Number of file descriptors currently in use.\n\n***NOTE:*** Currently these metrics are only implemented on Linux platforms, and not on Darwin or Windows.\n\n#### Using System Metrics\n\nAfter [adding swift-metrics-extras as a dependency](#adding-the-dependency) you can import the `SystemMetrics` module.\n\n```swift\nimport SystemMetrics\n```\n\nThis makes the System Metrics API available. This adds a new method to `MetricsSystem` called `bootstrapWithSystemMetrics`. Calling this method will call `MetricsSystem.bootstrap` as well as bootstrapping System Metrics.\n\n`bootstrapWithSystemMetrics` takes a `SystemMetrics.Configuration` object to configure the system metrics. The config has the following properties:\n\n- interval: The interval at which SystemMetrics are being calculated & exported.\n- dataProvider: A closure returing `SystemMetrics.Data?`. When `nil` no metrics are exported (the default on non-linux platforms). `SystemMetrics.Data` holds all the values mentioned above.\n- labels: `SystemMetrics.Labels` hold a string label for each of the above mentioned metrics that will be used for the metric labels, along with a prefix that will be used for all above mentioned metrics.\n\nSwift Metrics backend implementations are encouraged to provide static extensions to `SystemMetrics.Configuration` that fit the requirements of their specific backends. For example:\n```swift\npublic extension SystemMetrics.Configuration {\n    /// `SystemMetrics.Configuration` with Prometheus style labels.\n    ///\n    /// For more information see `SystemMetrics.Configuration`\n    static let prometheus = SystemMetrics.Configuration(\n        labels: .init(\n            prefix: \"process_\",\n            virtualMemoryBytes: \"virtual_memory_bytes\",\n            residentMemoryBytes: \"resident_memory_bytes\",\n            startTimeSeconds: \"start_time_seconds\",\n            cpuSecondsTotal: \"cpu_seconds_total\",\n            maxFds: \"max_fds\",\n            openFds: \"open_fds\"\n        )\n    )\n}\n```\nThis allows end users to add System Metrics like this:\n\n```swift\nMetricsSystem.bootstrapWithSystemMetrics(myPrometheusInstance, config: .prometheus)\n```\n"
 },
 {
  "repo": "apple/swift-nio-http2",
  "language": "Swift",
  "readme_contents": "# SwiftNIO HTTP/2\n\nThis project contains HTTP/2 support for Swift projects using [SwiftNIO](https://github.com/apple/swift-nio). To get started, check the [API docs](https://apple.github.io/swift-nio-http2/docs/current/NIOHTTP2/index.html).\n\n## Building\n\n`swift-nio-http2` is a SwiftPM project and can be built and tested very simply:\n\n```bash\n$ swift build\n$ swift test\n```\n\n## Versions\n\nJust like the rest of the SwiftNIO family, swift-nio-http2 follows [SemVer 2.0.0](https://semver.org/#semantic-versioning-200) with a separate document\ndeclaring [SwiftNIO's Public API](https://github.com/apple/swift-nio/blob/main/docs/public-api.md).\n\n### `swift-nio-http2` 1.x\n\n`swift-nio-http2` versions 1.x are a pure-Swift implementation of the HTTP/2 protocol for SwiftNIO. It's part of the SwiftNIO 2 family of repositories and does not have any dependencies besides [`swift-nio`](https://github.com/apple/swift-nio) and Swift 5. As the latest version, it lives on the [`main`](https://github.com/apple/swift-nio-http2) branch.\n\nTo depend on `swift-nio-http2`, put the following in the `dependencies` of your `Package.swift`:\n\n    .package(url: \"https://github.com/apple/swift-nio-http2.git\", from: \"1.5.0\"),\n    \nSwiftNIO HTTP2 1.18.x and later support Swift 5.2 and above. 1.17.x and earlier also support Swift 5.0 and 5.1.\n\n### `swift-nio-http2` 0.x\n\nThe legacy `swift-nio-http` 0.x is part of the SwiftNIO 1 family of repositories and works on Swift 4.1 and newer but requires [nghttp2](https://nghttp2.org) to be installed on your system. The source code can be found on the [`nghttp2-support-branch`](https://github.com/apple/swift-nio-http2/tree/nghttp2-support-branch).\n\n\n## Developing SwiftNIO HTTP/2\n\nFor the most part, SwiftNIO development is as straightforward as any other SwiftPM project. With that said, we do have a few processes that are worth understanding before you contribute. For details, please see [`CONTRIBUTING.md`](/CONTRIBUTING.md) in this repository.\n\n"
 },
 {
  "repo": "apple/swift-http-structured-headers",
  "language": "Swift",
  "readme_contents": "# swift-http-structured-headers\n\nA Swift implementation of the HTTP Structured Header Field Value specification.\n\nProvides parsing and serialization facilities for structured header field values, as well as implementations of `Encoder` and `Decoder` to allow using `Codable` data types as the payloads of HTTP structured header fields.\n\n## About Structured Header Field Values\n\nHTTP Structured Header Field Values are a HTTP extension recorded in [RFC 8941](https://www.rfc-editor.org/rfc/rfc8941.html). They provide a set of data types and algorithms for handling HTTP header field values in a consistent way, allowing a single parser and serializer to handle a wide range of header field values.\n\n## Swift HTTP Structured Header Field Values\n\nThis package provides a parser and serializer that implement RFC 8941. They are entirely complete, able to handle all valid HTTP structured header field values. This package also provides `Encoder` and `Decoder` objects for working with Codable in Swift. This allows rapid prototyping and experimentation with HTTP structured header field values, as well as interaction with the wider Swift Codable community.\n\nThis package provides two top-level modules: `StructuredFieldValues` and `RawStructuredFieldValues`.\n\nThe base module, `RawStructuredFieldValues`, provides a low-level implementation of a serializer and parser. Both of these have been written to avoid using Foundation, making them suitable for a range of use-cases where Foundation is not available. They rely entirely on the Swift standard library and are implemented as generically as possible. One of the limitations due to the absence of Foundation is that this interface is not capable of performing Base64 encoding or decoding: users are free to bring whatever encoder and decoder they choose to use.\n\nThis API is low-level, exposing the raw parse tree as the format for the serializer and parser. This allows high-performance and high-flexibility parsing and serialization, at the cost of being verbose and complex. Users are required to understand the structured header format and to operate the slightly awkward types, but maximal fidelity is retained and the performance overhead is low.\n\nThe upper-level module, `StructuredFieldValues`, brings along the `Encoder` and `Decoder` and also adds a dependency on Foundation. This Foundation dependency is necessary to correctly handle the base64 formatting, as well as to provide a good natural container for binary data: `Data`. This interface is substantially friendlier and easier to work with, using Swift's `Codable` support to provide a great user experience.\n\nIn most cases users should prefer to use `StructuredFieldValues` unless they know they need the performance advantages of `RawStructuredFieldValues`. The experience will be much better.\n\n## Working with Structured Header Field Values\n\n`swift-http-structured-headers` has a simple, easy-to-use high-level API for working with structured header field values. To begin with, let's consider the [HTTP Client Hints specification](https://www.rfc-editor.org/rfc/rfc8942.html). This defines the following new header field:\n\n>    The Accept-CH response header field indicates server support for the hints indicated in its value.  Servers wishing to receive user agent information through Client Hints SHOULD add Accept-CH response header to their responses as early as possible.\n>\n> Accept-CH is a Structured Header. Its value MUST be an sf-list whose members are tokens. Its ABNF is:\n>\n>     Accept-CH = sf-list\n>\n> For example:\n>\n>     Accept-CH: Sec-CH-Example, Sec-CH-Example-2\n\n`swift-http-structured-headers` can parse and serialize this field very simply:\n\n```swift\nlet field = Array(\"Sec-CH-Example, Sec-CH-Example-2\".utf8)\n\nstruct AcceptCH: StructuredFieldValue {\n    static let structuredFieldType: StructuredFieldType = .list\n    \n    var items: [String]\n}\n\n// Decoding\nlet decoder = StructuredFieldValueDecoder()\nlet parsed = try decoder.decode(AcceptCH.self, from: field)\n\n// Encoding\nlet encoder = StructuredFieldValueEncoder()\nlet serialized = try encoder.encode(AcceptCH(items: [\"Sec-CH-Example\", \"Sec-CH-Example-2\"]))\n```\n\nHowever, structured header field values can be substantially more complex. Structured header fields can make use of 4 containers and 6 base item types. The containers are:\n\n1. Dictionaries. These are top-level elements and associate token keys with values. The values may be items, or may be inner lists, and each value may also have parameters associated with them. `StructuredFieldValues` can model dictionaries as either Swift objects (where the property names are dictionary keys).\n2. Lists. These are top-level elements, providing a sequence of items or inner lists. Each item or inner list may have parameters associated with them. `StructuredFieldValues` models these as Swift objects with one key, `items`, that must be a collection of entries.\n3. Inner Lists. These are lists that may be sub-entries of a dictionary or a list. The list entries are items, which may have parameters associated with them: additionally, an inner list may have parameters associated with itself as well. `StructuredFieldValues` models these as either Swift `Array`s _or_, if it's important to extract parameters, as a two-field Swift `struct` where one field is called `items` and contains an `Array`, and other field is called `parameters` and contains a dictionary.\n4. Parameters. Parameters associated token keys with items without parameters. These are used to store metadata about objects within a field. `StructuredFieldValues` models these as either Swift objects (where the property names are the parameter keys) or as Swift dictionaries.\n\nThe base types are:\n\n1. Booleans. `StructuredFieldValues` models these as Swift's `Bool` type.\n2. Integers. `StructuredFieldValues` models these as any fixed-width integer type.\n3. Decimals. `StructuredFieldValues` models these as any floating-point type, or as Foundation's `Decimal`.\n4. Tokens. `StructuredFieldValues` models these as Swift's `String` type, where the range of characters is restricted.\n5. Strings. `StructuredFieldValues` models these as Swift's `String` type.\n6. Binary data. `StructuredFieldValues` models this as Foundation's `Data` type.\n\nFor any Structured Header Field Value Item, the item may either be represented directly by the appropriate type, or by a Swift struct with two properties: `item` and `parameters`. This latter mode is how parameters on a given item may be captured.\n\nThe top-level Structured Header Field Value must identify what kind of header field it corresponds to: `.item`, `.list`, or `.dictionary`. This is inherent in the type of the field and will be specified in the relevant field specification.\n\n## Lower Levels\n\nIn some cases the Codable interface will not be either performant enough or powerful enough for the intended use-case. In cases like this, users can use the types in the `RawStructuredFieldValues` module instead.\n\nThere are two core types: `StructuredFieldValueParser` and `StructuredFieldValueSerializer`. Rather than work with high-level Swift objects, these two objects either produce or accept a Swift representation of the data tree for a given structured header field.\n\nThis exposes the maximum amount of information about the header field. It allows users to handle situations where Codable cannot necessarily provide the relevant information, such in cases where dictionary ordering is semantic, or where it's necessary to control whether fields are tokens or strings more closely.\n\nThese APIs also have lower overhead than the `StructuredFieldValues` APIs.\n\nThe cost is that the APIs are substantially more verbose. Consider the above header field, `Accept-CH`. To parse or serialize this in `RawStructuredFieldValues` would look like this:\n\n```swift\nlet field = Array(\"Sec-CH-Example, Sec-CH-Example-2\".utf8)\nvar parser = StructuredFieldValueParser(field)\nlet parsed = parser.parseListFieldValue()\n\nprint(parsed)\n// [\n//     .item(Item(bareItem: .token(\"Sec-CH-Example\"), parameters: [])),\n//     .item(Item(bareItem: .token(\"Sec-CH-Example-2\"), parameters: [])),\n// ]\n\nvar serializer = StructuredFieldValueSerializer()\nlet serialized = serializer.writeListFieldValue(parsed)\n```\n\nNotice the substantially more verbose types involved in this operation. These types are highly generic, giving the opportunity for parsing and serializing that greatly reduces the runtime overhead. They also make it easier to distinguish between tokens and strings, and to observe the order of objects in dictionaries or parameters, which can be lost at the Codable level.\n\nIn general, users should consider this API only when they are confident they need either the flexibility or the performance. This may be valuable for header fields that do not evolve often, or that are highly dynamic.\n\n## Security\n\nswift-http-structured-headers has a security policy outlined in [SECURITY.md](SECURITY.md).\n"
 },
 {
  "repo": "apple/swift-nio-extras",
  "language": "Swift",
  "readme_contents": "# NIOExtras\n\nNIOExtras is a good place for code that is related to NIO but not core. It can also be used to incubate APIs for tasks that are possible with core-NIO but are cumbersome today.\n\nWhat makes a good contribution to NIOExtras?\n\n- a protocol encoder/decoder pair (also called \"codec\") that is often used but is small enough so it doesn't need its own repository\n- a helper to achieve a task that is harder-than-necessary to achieve with core-NIO\n\n## Code Quality / Stability\n\nAll code will go through code review like in the other repositories related to the SwiftNIO project.\n\n`swift-nio-extras` part of the SwiftNIO 2 family of repositories and depends on the following:\n\n- [`swift-nio`](https://github.com/apple/swift-nio), version 2.30.0 or better.\n- Swift 5.2.\n- `zlib` and its development headers installed on the system. But don't worry, you'll find `zlib` on pretty much any UNIX system that can compile any sort of code.\n\nTo depend on `swift-nio-extras`, put the following in the `dependencies` of your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-nio-extras.git\", from: \"1.0.0\"),\n```\n\n### Support for older Swift versions\n\nEarlier versions of SwiftNIO (2.29.x and lower) and SwiftNIOExtras (1.9.x and lower) supported Swift 5.0 and 5.1. \n\nOn the [`nio-extras-0.1`](https://github.com/apple/swift-nio-extras/tree/nio-extras-0.1) branch, you can find the `swift-nio-extras` version for the SwiftNIO 1 family. It requires Swift 4.1 or better.\n\n## Current Contents\n\n- [`QuiescingHelper`](Sources/NIOExtras/QuiescingHelper.swift): Helps to quiesce\n  a server by notifying user code when all previously open connections have closed.\n- [`LineBasedFrameDecoder`](Sources/NIOExtras/LineBasedFrameDecoder.swift) Splits incoming `ByteBuffer`s on line endings.\n- [`FixedLengthFrameDecoder`](Sources/NIOExtras/FixedLengthFrameDecoder.swift) Splits incoming `ByteBuffer`s by a fixed number of bytes.\n- [`LengthFieldBasedFrameDecoder`](Sources/NIOExtras/LengthFieldBasedFrameDecoder.swift) Splits incoming `ByteBuffer`s by a number of bytes specified in a fixed length header contained within the buffer.\n- [`LengthFieldPrepender`](Sources/NIOExtras/LengthFieldPrepender.swift) Prepends the number of bytes to outgoing `ByteBuffer`s as a fixed length header. Can be used in a codec pair with the `LengthFieldBasedFrameDecoder`.\n- [`RequestResponseHandler`](Sources/NIOExtras/RequestResponseHandler.swift) Matches a request and a promise with the corresponding response.\n- [`HTTPResponseCompressor`](Sources/NIOHTTPCompression/HTTPResponseCompressor.swift) Compresses the body of every HTTP/1 response message.\n- [`DebugInboundsEventHandler`](Sources/NIOExtras/DebugInboundEventsHandler.swift) Prints out all inbound events that travel through the `ChannelPipeline`.\n- [`DebugOutboundsEventHandler`](Sources/NIOExtras/DebugOutboundEventsHandler.swift) Prints out all outbound events that travel through the `ChannelPipeline`.\n- [`WritePCAPHandler`](Sources/NIOExtras/WritePCAPHandler.swift) A `ChannelHandler` that writes `.pcap` containing the traffic of the `ChannelPipeline` that you can inspect with Wireshark/tcpdump.\n"
 },
 {
  "repo": "apple/swift-nio-transport-services",
  "language": "Swift",
  "readme_contents": "# NIO Transport Services\n\nExtensions for [SwiftNIO](https://github.com/apple/swift-nio) to support Apple platforms as first-class citizens.\n\n## About NIO Transport Services\n\nNIO Transport Services is an extension to SwiftNIO that provides first-class support for Apple platforms by using [Network.framework](https://developer.apple.com/documentation/network) to provide network connectivity, and [Dispatch](https://developer.apple.com/documentation/dispatch) to provide concurrency. NIOTS provides an alternative [EventLoop](https://apple.github.io/swift-nio/docs/current/NIO/Protocols/EventLoop.html), [EventLoopGroup](https://apple.github.io/swift-nio/docs/current/NIO/Protocols/EventLoopGroup.html), and several alternative [Channels](https://apple.github.io/swift-nio/docs/current/NIO/Protocols/Channel.html) and Bootstraps.\n\nIn addition to providing first-class support for Apple platforms, NIO Transport Services takes advantage of the richer API of Network.framework to provide more insight into the behaviour of the network than is normally available to NIO applications. This includes the ability to wait for connectivity until a network route is available, as well as all of the extra proxy and VPN support that is built directly into Network.framework.\n\nAll regular NIO applications should work just fine with NIO Transport Services, simply by changing the event loops and bootstraps in use.\n\n## Why Transport Services?\n\nNetwork.framework is Apple's reference implementation of the [proposed post-sockets API](https://datatracker.ietf.org/wg/taps/charter/) that is currently being worked on by the Transport Services Working Group (taps) of the IETF. To indicate the proposed long-term future of interfaces like Network.framework, we decided to call this module NIOTransportServices. Also, NIONetworkFramework didn't appeal to us much as a name.\n\n## How to Use?\n\nNIO Transport Services primarily uses SwiftPM as its build tool, so we recommend using that as well. If you want to depend on NIO Transport Services in your own project, it's as simple as adding a dependencies clause to your Package.swift:\n\n```\ndependencies: [\n    .package(url: \"https://github.com/apple/swift-nio-transport-services.git\", from: \"1.1.1\")\n]\n```\n\nand then adding the NIOTransportServices module to your target dependencies.\n\nIf your project is set up as an Xcode project and you're using Xcode 11+, you can add NIO Transport Services as a dependency to your Xcode project by clicking File -> Swift Packages -> Add Package Dependency. In the upcoming dialog, please enter `https://github.com/apple/swift-nio-transport-services.git` and click Next twice. Finally, make sure `NIOTransportServices` is selected and click finish. Now will be able to `import NIOTransportServices` in your project.\n\nYou can also use SwiftNIO Transport Services in an iOS project through CocoaPods:\n\n    pod 'SwiftNIO', '~> 2.0.0'\n    pod 'SwiftNIOTransportServices', '~> 1.0.0'\n\nIf you want to develop SwiftNIO with Xcode 10, you have to generate an Xcode project:\n\n```\nswift package generate-xcodeproj\n```\n\nand add the project as a sub-project by dragging it into your iOS project and adding the framework (`NIOTransportServices.framework`) in 'Build Phases' -> 'Link Binary Libraries'.\n\nDo note however that Network.framework requires macOS 10.14+, iOS 12+, or tvOS 12+.\n\n### Supported Platforms\n\nNIOTransportServices is supported where Network.framework is supported: macOS\n10.14+, iOS 12+, tvOS 12+, and watchOS 6+.\n\nIn order to allow dependencies to use NIOTransportServices when it's available\nand fallback to NIO when it isn't, all code is behind import guards checking\nthe availability of Network.framework. As such NIOTransportServices may be\nbuilt on platforms where Network.framework is *not* available.\nNIOTransportServices can be built on macOS 10.12+, iOS 10+, tvOS 10+, watchOS\n6+ and Linux but is only functionally useful on macOS 10.14+, iOS 12+, tvOS 12+\nand watchOS 6+.\n\n## Versioning\n\nJust like the rest of the SwiftNIO family, `swift-nio-transport-services` follows [SemVer 2.0.0](https://semver.org/#semantic-versioning-200) with a separate document\ndeclaring [SwiftNIO's Public API](https://github.com/apple/swift-nio/blob/main/docs/public-api.md).\n\n### `swift-nio-transport-services ` 1.x\n\n`swift-nio-transport-services ` versions 1.x is part of the SwiftNIO 2 family of repositories and does not have any dependencies besides [`swift-nio`](https://github.com/apple/swift-nio), Swift 5, and an Apple OS supporting `Network.framework`. As the latest version, it lives on the [`main`](https://github.com/apple/swift-nio-transport-services) branch.\n\nTo depend on `swift-nio-transport-services `, put the following in the `dependencies` of your `Package.swift`:\n\n    .package(url: \"https://github.com/apple/swift-nio-transport-services.git\", from: \"1.0.0\"),\n\n### `swift-nio-transport-services ` 0.x\n\nThe legacy `swift-nio-transport-services` 0.x is part of the SwiftNIO 1 family of repositories and works with Swift 4.1 and newer. The source code can be found on the [`swift-nio-transport-services-swift-4-maintenance`](https://github.com/apple/swift-nio-transport-services/tree/swift-nio-transport-services-swift-4-maintenance) branch.\n\n## Developing NIO Transport Services\n\nFor the most part, NIO Transport Services development is as straightforward as any other SwiftPM project. With that said, we do have a few processes that are worth understanding before you contribute. For details, please see `CONTRIBUTING.md` in this repository.\n\nPlease note that all work on NIO Transport Services is covered by the [SwiftNIO Code of Conduct](https://github.com/apple/swift-nio/blob/main/CODE_OF_CONDUCT.md).\n\n"
 },
 {
  "repo": "apple/darwin-xnu",
  "language": "C",
  "readme_contents": "What is XNU?\n===========\n\nXNU kernel is part of the Darwin operating system for use in macOS and iOS operating systems. XNU is an acronym for X is Not Unix.\nXNU is a hybrid kernel combining the Mach kernel developed at Carnegie Mellon University with components from FreeBSD and a C++ API for writing drivers called IOKit.\nXNU runs on x86_64 for both single processor and multi-processor configurations.\n\nXNU Source Tree\n===============\n\n  * `config` - configurations for exported apis for supported architecture and platform\n  * `SETUP` - Basic set of tools used for configuring the kernel, versioning and kextsymbol management.\n  * `EXTERNAL_HEADERS` - Headers sourced from other projects to avoid dependency cycles when building. These headers should be regularly synced when source is updated.\n  * `libkern` - C++ IOKit library code for handling of drivers and kexts.\n  * `libsa` -  kernel bootstrap code for startup\n  * `libsyscall` - syscall library interface for userspace programs\n  * `libkdd` - source for user library for parsing kernel data like kernel chunked data.\n  * `makedefs` - top level rules and defines for kernel build.\n  * `osfmk` - Mach kernel based subsystems\n  * `pexpert` - Platform specific code like interrupt handling, atomics etc.\n  * `security` - Mandatory Access Check policy interfaces and related implementation.\n  * `bsd` - BSD subsystems code\n  * `tools` - A set of utilities for testing, debugging and profiling kernel.\n\nHow to build XNU\n================\n\nBuilding `DEVELOPMENT` kernel\n-----------------------------\n\nThe xnu make system can build kernel based on `KERNEL_CONFIGS` & `ARCH_CONFIGS` variables as arguments.\nHere is the syntax:\n\n    make SDKROOT=<sdkroot> ARCH_CONFIGS=<arch> KERNEL_CONFIGS=<variant>\n\nWhere:\n\n  * \\<sdkroot>: path to macOS SDK on disk. (defaults to `/`)\n  * \\<variant>: can be `debug`, `development`, `release`, `profile` and configures compilation flags and asserts throughout kernel code.\n  * \\<arch>   : can be valid arch to build for. (E.g. `X86_64`)\n\nTo build a kernel for the same architecture as running OS, just type\n\n    $ make\n    $ make SDKROOT=macosx.internal\n\nAdditionally, there is support for configuring architectures through `ARCH_CONFIGS` and kernel configurations with `KERNEL_CONFIGS`.\n\n    $ make SDKROOT=macosx.internal ARCH_CONFIGS=X86_64 KERNEL_CONFIGS=DEVELOPMENT\n    $ make SDKROOT=macosx.internal ARCH_CONFIGS=X86_64 KERNEL_CONFIGS=\"RELEASE DEVELOPMENT DEBUG\"\n\n\nNote:\n  * By default, architecture is set to the build machine architecture, and the default kernel\n    config is set to build for DEVELOPMENT.\n\n\nThis will also create a bootable image, kernel.[config],  and a kernel binary\nwith symbols, kernel.[config].unstripped.\n\nTo intall the kernel into a DSTROOT, use the `install_kernels` target:\n\n    $ make install_kernels DSTROOT=/tmp/xnu-dst\n\nHint:\nFor a more satisfying kernel debugging experience, with access to all\nlocal variables and arguments, but without all the extra check of the\nDEBUG kernel, add something like:\n\tCFLAGS_DEVELOPMENTARM64=\"-O0 -g -DKERNEL_STACK_MULTIPLIER=2\"\n\tCXXFLAGS_DEVELOPMENTARM64=\"-O0 -g -DKERNEL_STACK_MULTIPLIER=2\"\nto your make command.\nReplace DEVELOPMENT and ARM64 with the appropriate build and platform.\n\n\n  * To build with RELEASE kernel configuration\n\n        make KERNEL_CONFIGS=RELEASE SDKROOT=/path/to/SDK\n\n\nBuilding FAT kernel binary\n--------------------------\n\nDefine architectures in your environment or when running a make command.\n\n    $ make ARCH_CONFIGS=\"X86_64\" exporthdrs all\n\nOther makefile options\n----------------------\n\n * $ make MAKEJOBS=-j8    # this will use 8 processes during the build. The default is 2x the number of active CPUS.\n * $ make -j8             # the standard command-line option is also accepted\n * $ make -w              # trace recursive make invocations. Useful in combination with VERBOSE=YES\n * $ make BUILD_LTO=0      # build without LLVM Link Time Optimization\n * $ make REMOTEBUILD=user@remotehost # perform build on remote host\n * $ make BUILD_JSON_COMPILATION_DATABASE=1 # Build Clang JSON Compilation Database\n\nThe XNU build system can optionally output color-formatted build output. To enable this, you can either\nset the `XNU_LOGCOLORS` environment variable to `y`, or you can pass `LOGCOLORS=y` to the make command.\n\n\nDebug information formats\n=========================\n\nBy default, a DWARF debug information repository is created during the install phase; this is a \"bundle\" named kernel.development.\\<variant>.dSYM\nTo select the older STABS debug information format (where debug information is embedded in the kernel.development.unstripped image), set the BUILD_STABS environment variable.\n\n    $ export BUILD_STABS=1\n    $ make\n\n\nBuilding KernelCaches\n=====================\n\nTo test the xnu kernel, you need to build a kernelcache that links the kexts and\nkernel together into a single bootable image.\nTo build a kernelcache you can use the following mechanisms:\n\n  * Using automatic kernelcache generation with `kextd`.\n    The kextd daemon keeps watching for changing in `/System/Library/Extensions` directory.\n    So you can setup new kernel as\n\n        $ cp BUILD/obj/DEVELOPMENT/X86_64/kernel.development /System/Library/Kernels/\n        $ touch /System/Library/Extensions\n        $ ps -e | grep kextd\n\n  * Manually invoking `kextcache` to build new kernelcache.\n\n        $ kextcache -q -z -a x86_64 -l -n -c /var/tmp/kernelcache.test -K /var/tmp/kernel.test /System/Library/Extensions\n\n\n\nRunning KernelCache on Target machine\n=====================================\n\nThe development kernel and iBoot supports configuring boot arguments so that we can safely boot into test kernel and, if things go wrong, safely fall back to previously used kernelcache.\nFollowing are the steps to get such a setup:\n\n  1. Create kernel cache using the kextcache command as `/kernelcache.test`\n  2. Copy exiting boot configurations to alternate file\n\n         $ cp /Library/Preferences/SystemConfiguration/com.apple.Boot.plist /next_boot.plist\n\n  3. Update the kernelcache and boot-args for your setup\n\n         $ plutil -insert \"Kernel Cache\" -string \"kernelcache.test\" /next_boot.plist\n         $ plutil -replace \"Kernel Flags\" -string \"debug=0x144 -v kernelsuffix=test \" /next_boot.plist\n\n  4. Copy the new config to `/Library/Preferences/SystemConfiguration/`\n\n         $ cp /next_boot.plist /Library/Preferences/SystemConfiguration/boot.plist\n\n  5. Bless the volume with new configs.\n\n         $ sudo -n bless  --mount / --setBoot --nextonly --options \"config=boot\"\n\n     The `--nextonly` flag specifies that use the `boot.plist` configs only for one boot.\n     So if the kernel panic's you can easily power reboot and recover back to original kernel.\n\n\n\n\nCreating tags and cscope\n========================\n\nSet up your build environment and from the top directory, run:\n\n    $ make tags     # this will build ctags and etags on a case-sensitive volume, only ctags on case-insensitive\n    $ make TAGS     # this will build etags\n    $ make cscope   # this will build cscope database\n\n\nHow to install a new header file from XNU\n=========================================\n\nTo install IOKit headers, see additional comments in [iokit/IOKit/Makefile]().\n\nXNU installs header files at the following locations -\n\n    a. $(DSTROOT)/System/Library/Frameworks/Kernel.framework/Headers\n    b. $(DSTROOT)/System/Library/Frameworks/Kernel.framework/PrivateHeaders\n    c. $(DSTROOT)/usr/include/\n    d. $(DSTROOT)/System/DriverKit/usr/include/\n    e. $(DSTROOT)/System/Library/Frameworks/System.framework/PrivateHeaders\n\n`Kernel.framework` is used by kernel extensions.\\\nThe `System.framework` and `/usr/include` are used by user level applications. \\\n`/System/DriverKit/usr/include` is used by userspace drivers. \\\nThe header files in framework's `PrivateHeaders` are only available for ** Apple Internal Development **.\n\nThe directory containing the header file should have a Makefile that\ncreates the list of files that should be installed at different locations.\nIf you are adding the first header file in a directory, you will need to\ncreate Makefile similar to `xnu/bsd/sys/Makefile`.\n\nAdd your header file to the correct file list depending on where you want\nto install it. The default locations where the header files are installed\nfrom each file list are -\n\n    a. `DATAFILES` : To make header file available in user level -\n       `$(DSTROOT)/usr/include`\n\n    b. `DRIVERKIT_DATAFILES` : To make header file available to DriverKit userspace drivers -\n       `$(DSTROOT)/System/DriverKit/usr/include`\n\n    c. `PRIVATE_DATAFILES` : To make header file available to Apple internal in\n       user level -\n       `$(DSTROOT)/System/Library/Frameworks/System.framework/PrivateHeaders`\n\n    d. `KERNELFILES` : To make header file available in kernel level -\n       `$(DSTROOT)/System/Library/Frameworks/Kernel.framework/Headers`\n       `$(DSTROOT)/System/Library/Frameworks/Kernel.framework/PrivateHeaders`\n\n    e. `PRIVATE_KERNELFILES` : To make header file available to Apple internal\n       for kernel extensions -\n       `$(DSTROOT)/System/Library/Frameworks/Kernel.framework/PrivateHeaders`\n\nThe Makefile combines the file lists mentioned above into different\ninstall lists which are used by build system to install the header files. There\nare two types of install lists: machine-dependent and machine-independent.\nThese lists are indicated by the presence of `MD` and `MI` in the build\nsetting, respectively. If your header is architecture-specific, then you should\nuse a machine-dependent install list (e.g. `INSTALL_MD_LIST`). If your header\nshould be installed for all architectures, then you should use a\nmachine-independent install list (e.g. `INSTALL_MI_LIST`).\n\nIf the install list that you are interested does not exist, create it\nby adding the appropriate file lists.  The default install lists, its\nmember file lists and their default location are described below -\n\n    a. `INSTALL_MI_LIST` : Installs header file to a location that is available to everyone in user level.\n        Locations -\n           $(DSTROOT)/usr/include\n       Definition -\n           INSTALL_MI_LIST = ${DATAFILES}\n\n    b. `INSTALL_DRIVERKIT_MI_LIST` : Installs header file to a location that is\n        available to DriverKit userspace drivers.\n        Locations -\n           $(DSTROOT)/System/DriverKit/usr/include\n       Definition -\n           INSTALL_DRIVERKIT_MI_LIST = ${DRIVERKIT_DATAFILES}\n\n    c.  `INSTALL_MI_LCL_LIST` : Installs header file to a location that is available\n       for Apple internal in user level.\n       Locations -\n           $(DSTROOT)/System/Library/Frameworks/System.framework/PrivateHeaders\n       Definition -\n           INSTALL_MI_LCL_LIST = ${PRIVATE_DATAFILES}\n\n    d. `INSTALL_KF_MI_LIST` : Installs header file to location that is available\n       to everyone for kernel extensions.\n       Locations -\n            $(DSTROOT)/System/Library/Frameworks/Kernel.framework/Headers\n       Definition -\n            INSTALL_KF_MI_LIST = ${KERNELFILES}\n\n    e. `INSTALL_KF_MI_LCL_LIST` : Installs header file to location that is\n       available for Apple internal for kernel extensions.\n       Locations -\n            $(DSTROOT)/System/Library/Frameworks/Kernel.framework/PrivateHeaders\n       Definition -\n            INSTALL_KF_MI_LCL_LIST = ${KERNELFILES} ${PRIVATE_KERNELFILES}\n\n    f. `EXPORT_MI_LIST` : Exports header file to all of xnu (bsd/, osfmk/, etc.)\n       for compilation only. Does not install anything into the SDK.\n       Definition -\n            EXPORT_MI_LIST = ${KERNELFILES} ${PRIVATE_KERNELFILES}\n\n    g. `INSTALL_MODULEMAP_INCDIR_MI_LIST` : Installs module map file to a\n       location that is available to everyone in user level, installing at the\n       root of INCDIR.\n       Locations -\n           $(DSTROOT)/usr/include\n       Definition -\n           INSTALL_MODULEMAP_INCDIR_MI_LIST = ${MODULEMAP_INCDIR_FILES}\n\nIf you want to install the header file in a sub-directory of the paths\ndescribed in (1), specify the directory name using two variables\n`INSTALL_MI_DIR` and `EXPORT_MI_DIR` as follows -\n\n    INSTALL_MI_DIR = dirname\n    EXPORT_MI_DIR = dirname\n\nA single header file can exist at different locations using the steps\nmentioned above.  However it might not be desirable to make all the code\nin the header file available at all the locations.  For example, you\nwant to export a function only to kernel level but not user level.\n\n You can use C language's pre-processor directive (#ifdef, #endif, #ifndef)\n to control the text generated before a header file is installed.  The kernel\n only includes the code if the conditional macro is TRUE and strips out\n code for FALSE conditions from the header file.\n\n Some pre-defined macros and their descriptions are -\n\n    a. `PRIVATE` : If defined, enclosed definitions are considered System\n\tPrivate Interfaces. These are visible within xnu and\n\texposed in user/kernel headers installed within the AppleInternal\n\t\"PrivateHeaders\" sections of the System and Kernel frameworks.\n    b. `KERNEL_PRIVATE` : If defined, enclosed code is available to all of xnu\n\tkernel and Apple internal kernel extensions and omitted from user\n\theaders.\n    c. `BSD_KERNEL_PRIVATE` : If defined, enclosed code is visible exclusively\n\twithin the xnu/bsd module.\n    d. `MACH_KERNEL_PRIVATE`: If defined, enclosed code is visible exclusively\n\twithin the xnu/osfmk module.\n    e. `XNU_KERNEL_PRIVATE`: If defined, enclosed code is visible exclusively\n\twithin xnu.\n    f. `KERNEL` :  If defined, enclosed code is available within xnu and kernel\n       extensions and is not visible in user level header files.  Only the\n       header files installed in following paths will have the code -\n\n            $(DSTROOT)/System/Library/Frameworks/Kernel.framework/Headers\n            $(DSTROOT)/System/Library/Frameworks/Kernel.framework/PrivateHeaders\n    g. `DRIVERKIT`: If defined, enclosed code is visible exclusively in the\n    DriverKit SDK headers used by userspace drivers.\n\nConditional compilation\n=======================\n\n`xnu` offers the following mechanisms for conditionally compiling code:\n\n    a. *CPU Characteristics* If the code you are guarding has specific\n    characterstics that will vary only based on the CPU architecture being\n    targeted, use this option. Prefer checking for features of the\n    architecture (e.g. `__LP64__`, `__LITTLE_ENDIAN__`, etc.).\n    b. *New Features* If the code you are guarding, when taken together,\n    implements a feature, you should define a new feature in `config/MASTER`\n    and use the resulting `CONFIG` preprocessor token (e.g. for a feature\n    named `config_virtual_memory`, check for `#if CONFIG_VIRTUAL_MEMORY`).\n    This practice ensures that existing features may be brought to other\n    platforms by simply changing a feature switch.\n    c. *Existing Features* You can use existing features if your code is\n    strongly tied to them (e.g. use `SECURE_KERNEL` if your code implements\n    new functionality that is exclusively relevant to the trusted kernel and\n    updates the definition/understanding of what being a trusted kernel means).\n\nIt is recommended that you avoid compiling based on the target platform. `xnu`\ndoes not define the platform macros from `TargetConditionals.h`\n(`TARGET_OS_OSX`, `TARGET_OS_IOS`, etc.).\n\n\nThere is a deprecated `TARGET_OS_EMBEDDED` macro, but this should be avoided\nas it is in general too broad a definition for most functionality.\nPlease refer to TargetConditionals.h for a full picture.\n\nHow to add a new syscall\n========================\n\n\n\n\nTesting the kernel\n==================\n\nXNU kernel has multiple mechanisms for testing.\n\n  * Assertions - The DEVELOPMENT and DEBUG kernel configs are compiled with assertions enabled. This allows developers to easily\n    test invariants and conditions.\n\n  * XNU Power On Self Tests (`XNUPOST`): The XNUPOST config allows for building the kernel with basic set of test functions\n    that are run before first user space process is launched. Since XNU is hybrid between MACH and BSD, we have two locations where\n    tests can be added.\n\n        xnu/osfmk/tests/     # For testing mach based kernel structures and apis.\n        bsd/tests/           # For testing BSD interfaces.\n    Please follow the documentation at [osfmk/tests/README.md](osfmk/tests/README.md)\n\n  * User level tests: The `tools/tests/` directory holds all the tests that verify syscalls and other features of the xnu kernel.\n    The make target `xnu_tests` can be used to build all the tests supported.\n\n        $ make RC_ProjectName=xnu_tests SDKROOT=/path/to/SDK\n\n    These tests are individual programs that can be run from Terminal and report tests status by means of std posix exit codes (0 -> success) and/or stdout.\n    Please read detailed documentation in [tools/tests/unit_tests/README.md](tools/tests/unit_tests/README.md)\n\n\nKernel data descriptors\n=======================\n\nXNU uses different data formats for passing data in its api. The most standard way is using syscall arguments. But for complex data\nit often relies of sending memory saved by C structs. This packaged data transport mechanism is fragile and leads to broken interfaces\nbetween user space programs and kernel apis. `libkdd` directory holds user space library that can parse custom data provided by the\nsame version of kernel. The kernel chunked data format is described in detail at [libkdd/README.md](libkdd/README.md).\n\n\nDebugging the kernel\n====================\n\nThe xnu kernel supports debugging with a remote kernel debugging protocol (kdp). Please refer documentation at [technical note] [TN2063]\nBy default the kernel is setup to reboot on a panic. To debug a live kernel, the kdp server is setup to listen for UDP connections\nover ethernet. For machines without ethernet port, this behavior can be altered with use of kernel boot-args. Following are some\ncommon options.\n\n  * `debug=0x144` - setups debug variables to start kdp debugserver on panic\n  * `-v` - print kernel logs on screen. By default XNU only shows grey screen with boot art.\n  * `kdp_match_name=en1` - Override default port selection for kdp. Supported for ethernet, thunderbolt and serial debugging.\n\nTo debug a panic'ed kernel, use llvm debugger (lldb) along with unstripped symbol rich kernel binary.\n\n    sh$ lldb kernel.development.unstripped\n\nAnd then you can connect to panic'ed machine with `kdp_remote [ip addr]` or `gdb_remote [hostip : port]` commands.\n\nEach kernel is packaged with kernel specific debug scripts as part of the build process. For security reasons these special commands\nand scripts do not get loaded automatically when lldb is connected to machine. Please add the following setting to your `~/.lldbinit`\nif you wish to always load these macros.\n\n    settings set target.load-script-from-symbol-file true\n\nThe `tools/lldbmacros` directory contains the source for each of these commands. Please follow the [README.md](tools/lldbmacros/README.md)\nfor detailed explanation of commands and their usage.\n\n[TN2118]: https://developer.apple.com/library/mac/technotes/tn2004/tn2118.html#//apple_ref/doc/uid/DTS10003352 \"Kernel Core Dumps\"\n[TN2063]: https://developer.apple.com/library/mac/technotes/tn2063/_index.html \"Understanding and Debugging Kernel Panics\"\n[Kernel Programming Guide]: https://developer.apple.com/library/mac/documentation/Darwin/Conceptual/KernelProgramming/build/build.html#//apple_ref/doc/uid/TP30000905-CH221-BABDGEGF\n"
 },
 {
  "repo": "apple/swift-system",
  "language": "Swift",
  "readme_contents": "# Swift System\n\nSwift System provides idiomatic interfaces to system calls and low-level currency types. Our vision is for System to act as the single home for low-level system interfaces for all supported Swift platforms.\n\n## Multi-platform not Cross-platform\n\nSystem is a multi-platform library, not a cross-platform one. It provides a separate set of APIs and behaviors on every supported platform, closely reflecting the underlying OS interfaces. A single import will pull in the native platform interfaces specific for the targeted OS.\n\nOur immediate goal is to simplify building cross-platform libraries and applications such as SwiftNIO and SwiftPM. System does not eliminate the need for `#if os()` conditionals to implement cross-platform abstractions, but it does make it safer and more expressive to fill out the platform-specific parts.\n\n## Usage\n\n```swift\nimport SystemPackage\n\nlet message: String = \"Hello, world!\" + \"\\n\"\nlet path: FilePath = \"/tmp/log\"\nlet fd = try FileDescriptor.open(\n  path, .writeOnly, options: [.append, .create], permissions: .ownerReadWrite)\ntry fd.closeAfter {\n  _ = try fd.writeAll(message.utf8)\n}\n```\n\n## Adding `SystemPackage` as a Dependency\n\nTo use the `SystemPackage` library in a SwiftPM project,\nadd the following line to the dependencies in your `Package.swift` file:\n\n```swift\n.package(url: \"https://github.com/apple/swift-system\", from: \"1.0.0\"),\n```\n\nFinally, include `\"SystemPackage\"` as a dependency for your executable target:\n\n```swift\nlet package = Package(\n    // name, platforms, products, etc.\n    dependencies: [\n        .package(url: \"https://github.com/apple/swift-system\", from: \"1.0.0\"),\n        // other dependencies\n    ],\n    targets: [\n        .target(name: \"MyTarget\", dependencies: [\n            .product(name: \"SystemPackage\", package: \"swift-system\"),\n        ]),\n        // other targets\n    ]\n)\n```\n\n## Source Stability\n\nThe Swift System package is source stable. The version numbers follow [Semantic Versioning][semver] -- source breaking changes to public API can only land in a new major version.\n\n[semver]: https://semver.org\n\n## Contributing\n\nBefore contributing, please read [CONTRIBUTING.md](CONTRIBUTING.md). \n\n## LICENSE\n\nSee [LICENSE](LICENSE.txt) for license information. \n"
 },
 {
  "repo": "apple/swift-format",
  "language": "Swift",
  "readme_contents": "# swift-format\n\n`swift-format` provides the formatting technology for\n[SourceKit-LSP](https://github.com/apple/sourcekit-lsp) and the building\nblocks for doing code formatting transformations.\n\nThis package can be used as a [command line tool](#command-line-usage)\nor linked into other applications as a Swift Package Manager dependency and\ninvoked via an [API](#api-usage).\n\n> NOTE: No default Swift code style guidelines have yet been proposed. The\n> style that is currently applied by `swift-format` is just one possibility,\n> and the code is provided so that it can be tested on real-world code and\n> experiments can be made by modifying it.\n\n## Matching swift-format to Your Swift Version\n\n`swift-format` depends on [SwiftSyntax](https://github.com/apple/swift-syntax)\nand the standalone parsing library that is distributed as part of the Swift\ntoolchain, so you should check out and build `swift-format` from the release\ntag or branch that is compatible with the version of Swift you are using.\n\nThe major and minor version components of `swift-format` and SwiftSyntax must\nbe the same\u2014this is expressed in the `SwiftSyntax` dependency in\n[Package.swift](Package.swift)\u2014and those version components must match the\nSwift toolchain that is installed and used to build and run the formatter:\n\n| Xcode Release   | Swift Version          | `swift-format` Branch / Tags     |\n|:----------------|:-----------------------|:---------------------------------|\n| \u2013               | Swift at `main`        | `main`                           |\n| Xcode 13.0\u201313.2 | Swift 5.5              | `swift-5.5-branch` / `0.50500.x` |\n| Xcode 12.5      | Swift 5.4              | `swift-5.4-branch` / `0.50400.x` |\n| Xcode 12.0\u201312.4 | Swift 5.3              | `swift-5.3-branch` / `0.50300.x` |\n| Xcode 11.4\u201311.7 | Swift 5.2              | `swift-5.2-branch` / `0.50200.x` |\n| Xcode 11.0\u201311.3 | Swift 5.1              | `swift-5.1-branch`               |\n\nFor example, if you are using Xcode 13.1 (Swift 5.5), you will need\n`swift-format` 0.50500.0.\n\n## Getting swift-format\n\nIf you are mainly interested in using swift-format (rather than developing it),\nthen once you have identified the version you need, you can check out the\nsource and build it using the following commands:\n\n```sh\nVERSION=0.50500.0  # replace this with the version you need\ngit clone https://github.com/apple/swift-format.git\ncd swift-format\ngit checkout \"tags/$VERSION\"\nswift build -c release\n```\n\nNote that the `git checkout` command above will leave the repository in a\n\"detached HEAD\" state. This is fine if building and running the tool is all you\nwant to do.\n\nOnce the build has finished, the `swift-format` executable will be located at\n`.build/release/swift-format`.\n\nTo test that the formatter was built succesfully and is compatible with your\nSwift toolchain, you can also run the following command:\n\n```sh\nswift test --parallel\n```\n\nWe recommend using the `--parallel` flag to speed up the test run since there\nare a large number of tests.\n\n## Command Line Usage\n\nThe general invocation syntax for `swift-format` is as follows:\n\n```sh\nswift-format [SUBCOMMAND] [OPTIONS...] [FILES...]\n```\n\nThe tool supports a number of subcommands, each of which has its own options\nand are described below. Descriptions of the subcommands that are available\ncan also be obtained by running `swift-format --help`, and the description of\na specific subcommand can be obtained by using the `--help` flag after the\nsubcommand name; for example, `swift-format lint --help`.\n\n### Formatting\n\n```sh\nswift-format [format] [OPTIONS...] [FILES...]\n```\n\nThe `format` subcommand formats one or more Swift source files (or source code\nfrom standard input if no file paths are given on the command line). Writing\nout the `format` subcommand is optional; it is the default behavior if no other\nsubcommand is given.\n\nThis subcommand supports all of the\n[common lint and format options](#options-supported-by-formatting-and-linting),\nas well as the formatting-only options below:\n\n*   `-i/--in-place`: Overwrites the input files when formatting instead of\n    printing the results to standard output. _No backup of the original file is\n    made before it is overwritten._\n\n### Linting\n\n```sh\nswift-format lint [OPTIONS...] [FILES...]\n```\n\nThe `lint` subcommand checks one or more Swift source files (or source code\nfrom standard input if no file paths are given on the command line) for style\nviolations and prints diagnostics to standard error for any violations that\nare detected.\n\nThis subcommand supports all of the\n[common lint and format options](#options-supported-by-formatting-and-linting),\nas well as the linting-only options below:\n\n*   `-s/--strict`: If this option is specified, lint warnings will cause the\n    tool to exit with a non-zero exit code (failure). By default, lint warnings\n    do not prevent a successful exit; only fatal errors (for example, trying to\n    lint a file that does not exist) cause the tool to exit unsuccessfully.\n\n### Options Supported by Formatting and Linting\n\nThe following options are supported by both the `format` and `lint`\nsubcommands:\n\n*   `--assume-filename <path>`: The file path that should be used in\n    diagnostics when linting or formatting from standard input. If this option\n    is not provided, then `<stdin>` will be used as the filename printed in\n    diagnostics.\n\n*   `--color-diagnostics/--no-color-diagnostics`: By default, `swift-format`\n    will print diagnostics in color if standard error is connected to a\n    terminal and without color otherwise (for example, if standard error is\n    being redirected to a file). These flags can be used to force colors on\n    or off respectively, regardless of whether the output is going to a\n    terminal.\n\n*   `--configuration <file>`: The path to a JSON file that contains\n    [configurable settings](#configuring-the-command-line-tool) for\n    `swift-format`. If omitted, a default configuration is use (which\n    can be seen by running `swift-format dump-configuration`).\n\n*   `--ignore-unparsable-files`: If this option is specified and a source file\n    contains syntax errors or can otherwise not be parsed successfully by the\n    Swift syntax parser, it will be ignored (no diagnostics will be emitted\n    and it will not be formatted). Without this option, an error will be\n    emitted for any unparsable files.\n\n*   `-p/--parallel`: Process files in parallel, simultaneously across\n    multiple cores.\n\n*   `-r/--recursive`: If specified, then the tool will process `.swift` source\n    files in any directories listed on the command line and their descendants.\n    Without this flag, it is an error to list a directory on the command line.\n\n### Viewing the Default Configuration\n\n```sh\nswift-format dump-configuration\n```\n\nThe `dump-configuration` subcommand dumps the default configuration in JSON\nformat to standard output. This can be used to simplify generating a custom\nconfiguration, by redirecting it to a file and editing it.\n\n### Configuring the Command Line Tool\n\nFor any source file being checked or formatted, `swift-format` looks for a\nJSON-formatted file named `.swift-format` in the same directory. If one is\nfound, then that file is loaded to determine the tool's configuration. If the\nfile is not found, then it looks in the parent directory, and so on.\n\nIf no configuration file is found, a default configuration is used. The\nsettings in the default configuration can be viewed by running\n`swift-format --mode dump-configuration`, which will dump it to standard\noutput.\n\nIf the `--configuration <file>` option is passed to `swift-format`, then that\nconfiguration will be used unconditionally and the file system will not be\nsearched.\n\nSee [Documentation/Configuration.md](Documentation/Configuration.md) for a\ndescription of the configuration file format and the settings that are\navailable.\n\n### Miscellaneous\n\nRunning `swift-format -v` or `swift-format --version` will print version\ninformation about `swift-format` version and then exit.\n\n## API Usage\n\n`swift-format` can be easily integrated into other tools written in Swift.\nInstead of invoking the formatter by spawning a subprocess, users can depend on\n`swift-format` as a Swift Package Manager dependency and import the\n`SwiftFormat` module, which contains the entry points into the formatter's\ndiagnostic and correction behavior.\n\nFormatting behavior is provided by the `SwiftFormatter` class and linting\nbehavior is provided by the `SwiftLinter` class. These APIs can be passed\neither a Swift source file `URL` or a `Syntax` node representing a\nSwiftSyntax syntax tree. The latter capability is particularly useful for\nwriting code generators, since it significantly reduces the amount of trivia\nthat the generator needs to be concerned about adding to the syntax nodes it\ncreates. Instead, it can pass the in-memory syntax tree to the `SwiftFormat`\nAPI and receive perfectly formatted code as output.\n\nPlease see the documentation in the\n[`SwiftFormatter`](Sources/SwiftFormat/SwiftFormatter.swift) and\n[`SwiftLinter`](Sources/SwiftFormat/SwiftLinter.swift) classes for more\ninformation about their usage.\n\n### Checking Out the Source Code for Development\n\nThe `main` branch is used for development. Pull requests should be created\nto merge into the `main` branch; changes that are low-risk and compatible with\nthe latest release branch may be cherry-picked into that branch after they have\nbeen merged into `main`.\n\nIf you are interested in developing `swift-format`, there is additional\ndocumentation about that [here](Documentation/Development.md).\n"
 },
 {
  "repo": "apple/ml-cvnets",
  "language": "Python",
  "readme_contents": "# Converting models trained using CVNets to CoreML\n\nFor conversion, we assume that you are using `MAC OS` machine.\n\n## Classification networks\n\nWe can convert the classification models using the following command\n\n```\nexport EVAL_DIR='' # Location of results \nexport CKPT_NAME='' # Name of the pre-trained model weight file (e.g., checkpoint_ema.pt)\ncvnets-convert --common.config-file \"${EVAL_DIR}/config.yaml\" --common.results-loc $EVAL_DIR --model.classification.pretrained \"${EVAL_DIR}/${CKPT_NAME}\"  --conversion.coreml-extn mlmodel\n```\n\n## Detection networks\n\nWe can convert the detection models trained on MS-COCO (81 classes, including background) using the following command\n\n```\nexport EVAL_DIR='' # Location of results \nexport CKPT_NAME='' # Name of the pre-trained model weight file (e.g., checkpoint_ema.pt)\ncvnets-convert --common.config-file \"${EVAL_DIR}/config.yaml\" --common.results-loc $EVAL_DIR --model.detection.pretrained \"${EVAL_DIR}/${CKPT_NAME}\"  --conversion.coreml-extn mlmodel --model.detection.n-classes 81\n```\n\n## Segmentation networks\n\nWe can convert the segmentation models trained on the PASCAL VOC 2012 dataset (21 classes, including background) using the following command\n\n```\nexport EVAL_DIR='' # Location of results \nexport CKPT_NAME='' # Name of the pre-trained model weight file (e.g., checkpoint_ema.pt)\ncvnets-convert --common.config-file \"${EVAL_DIR}/config.yaml\" --common.results-loc $EVAL_DIR --model.segmentation.pretrained \"${EVAL_DIR}/${CKPT_NAME}\"  --conversion.coreml-extn mlmodel --model.segmentation.n-classes 21\n```"
 },
 {
  "repo": "apple/ml-knowledge-conflicts",
  "language": "Python",
  "readme_contents": "# Entity-Based Knowledge Conflicts in Question Answering\n\n[**Run Instructions**](#run-instructions) | [**Paper**](https://aclanthology.org/2021.emnlp-main.565/) | [**Citation**](#citation) | [**License**](#license)\n\nThis repository provides the **Substitution Framework** described in Section 2 of our paper Entity-Based Knowledge Conflicts in Question Answering.\nGiven a quesion answering dataset, we derive a new dataset where the context passages have been modified to have new answers to their question.\nBy training on the original examples and evaluating on the derived examples, we simulate a parametric-contextual knowledge conflict --- useful for understanding how model's employ sources of knowledge to arrive at a decision.\n\nOur dataset derivation follows two steps: (1) identifying named entity answers, and (2) replacing all occurrences of the answer in the context with a substituted entity, effectively changing the answer.\nThe answer substitutions depend on the chosen [substitution policy](#our-substitution-functions).\n\n## Run Instructions\n\n### 1. Setup\n\nSetup requirements and download SpaCy and WikiData dependencies. \n```\nbash setup.sh\n```\n\n### 2. (Optional) Download and Process Wikidata\n\nThis optional stage reproduces `wikidata/entity_info.json.gz`, downloaded during Setup.\n\nDownload the Wikidata dump from October 2020 [here](https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2) and the Wikipedia pageviews from June 2, 2020 [here](https://dumps.wikimedia.org/other/pageview_complete/2021/2021-06/pageviews-20210602-user.bz2).\n\n**NOTE:** We don't use the newest Wikidata dump because Wikidata doesn't keep old dumps so reproducibility is an issue.\nIf you'd like to use the newest dump, it is available [here](https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2).\nWikipedia pageviews, on the other hand, are kept around and can be found [here](https://dumps.wikimedia.org/other/pageview_complete/).\nBe sure to download the `*-user.bz2` file and not the `*-automatic.bz2` or the `*-spider.bz2` files.\n\nTo extract out Wikidata information, run the following (takes ~8 hours)\n```\npython extract_wikidata_info.py --wikidata_dump wikidata-20201026-all.json.bz2 --popularity_dump pageviews-20210602-user.bz2 --output_file entity_info.json.gz\n```\n\nThe output file of this step is available [here](https://docs-assets.developer.apple.com/ml-research/models/kc-ner/entity_info.json.gz). \n\n### 3. Load and Preprocess Dataset\n\n```\nPYTHONPATH=. python src/load_dataset.py -d MRQANaturalQuestionsTrain -w wikidata/entity_info.json.gz\nPYTHONPATH=. python src/load_dataset.py -d MRQANaturalQuestionsDev -w wikidata/entity_info.json.gz\n```\n\n### 4. Generate Substitutions\n```\nPYTHONPATH=. python src/generate_substitutions.py --inpath datasets/normalized/MRQANaturalQuestionsTrain.jsonl --outpath datasets/substitution-sets/MRQANaturalQuestionsTrain<substitution_type>.jsonl <substitution-command> -n 1 ...\nPYTHONPATH=. python src/generate_substitutions.py --inpath datasets/normalized/MRQANaturalQuestionsDev.jsonl --outpath datasets/substitution-sets/MRQANaturalQuestionsDev<substitution_type>.jsonl <substitution-command> -n 1 ...\n```\n\nSee descriptions of the substitution policies (substitution-commands) we provide [here](#our-substitution-functions).\nInspect the argparse and substitution-specific subparsers in `generate_substitutions.py` to see additional arguments.\n\n## Our Substitution Functions\n\nHere we define the the substitution functions we provide.\nThese functions ingests a QADataset, and modifies the context passage, according to defined rules, such that there is now a new answer to the question, according to the context.\nGreater detail is provided in our paper.\n\n* **Alias Substitution** (sub-command: `alias-substitution`) --- Here we replace an answer with one of it's wikidata aliases. \nSince the substituted answer is always semantically equivalent, answer type preservation is naturally maintained.\n* **Popularity Substitution** (sub-command: `popularity-substitution`) --- Here we replace answers with a WikiData answer of the same type, with a specified popularity bracket (according to monthly page views).\n* **Corpus Substitution** (sub-command: `corpus-substitution`) --- Here we replace answers with other answers of the same type, sampled from the same corpus.\n* **Type Swap Substitution** (sub-command: `type-swap-substitution`) --- Here we replace answers with other answers of different type, sampled from the same corpus.\n\n## How to Add Your own Dataset / Substitution Fn / NER Models\n\n### Use your own Dataset\n\nTo add your own dataset, create your own subclass of `QADataset` (in `src/classes/qadataset.py`).\n\n1. Overwrite the `read_original_dataset` function, to read your dataset, creating a List of `QAExample` objects.\n2. Add your class and the url/filepath to the `DATASETS` variable in `src/load_dataset.py`.\n\nSee `MRQANaturalQuetsionsDataset` in `src/classes/qadataset.py` as an example.\n\n### Use your own Substitution Function\n\nWe define 5 different substitution functions in `src/generate_substitutions.py`. These are described [here](#our-substitution-functions).\nInspect their docstrings and feel free to add your own, leveraging any of the wikidata, derived answer type, or other info we populate for examples and answers.\nHere are the steps to create your own:\n\n1. Add a subparser in `src/generate_substitutions.py` for your new function, with any relevant parameters. See `alias_sub_parser` as an example.\n2. Add your own substitution function to `src/substitution_fns.py`, ensuring the signature arguments match those specified in the subparser. See `alias_substitution_fn` as an example.\n3. Add a reference to your new function to `SUBSTITUTION_FNS` in `src/generate_substitutions.py`. Ensure the dictionary key matches the subparser name.\n\n### Use your own Named Entity Recognition and/or Entity Linking Model\n\nOur SpaCy NER model is trained and used mainly to categorize answer text into answer types. \nOnly substitutions that preserve answer type are likely to be coherent.\n\nThe functions which need to be changed are:\n1. `run_ner_linking` in `utils.py`, which loads the NER model and populates info for each answer (see function docstring).\n2. `Answer._select_answer_type()` in `src/classes/answer.py`, which uses the NER answer type label and wikidata type labels to cateogrize the answer into a type category.\n\n## Citation\n\nPlease cite the following if you found this resource or our paper useful.\n```\n@inproceedings{longpre-etal-2021-entity,\n    title = \"Entity-Based Knowledge Conflicts in Question Answering\",\n    author = \"Longpre, Shayne  and\n      Perisetla, Kartik  and\n      Chen, Anthony  and\n      Ramesh, Nikhil  and\n      DuBois, Chris  and\n      Singh, Sameer\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-main.565\",\n    pages = \"7052--7063\",\n}\n```\n\n## License\nThe Knowledge Conflicts repository, and entity-based substitution framework are licensed according to the [LICENSE](LICENSE) file.\n\n\n## Contact Us\nTo contact us feel free to email the authors in the paper or create an issue in this repository.\n"
 },
 {
  "repo": "apple/swift-sample-distributed-actors-transport",
  "language": "Swift",
  "readme_contents": "# swift-sample-distributed-actors-transport\n\nSample application and `ActorTransport`, associated with `distributed actor` language evolution proposal.\n\n## Running the sample app\n\n1. Download the latest toolchain from `main` branch, or a built one from a specific PR. \n\n3. Move it to `Library/Developer/Toolchains/` and point the TOOLCHAIN env variable at it:\n\n```\nexport TOOLCHAIN=/Library/Developer/Toolchains/swift-DEVELOPMENT-SNAPSHOT-2021-09-18-a.xctoolchain\n```\n\nTo run the sample app, use the following command:\n\n```\ncd SampleApp\nDYLD_LIBRARY_PATH=$TOOLCHAIN/usr/lib/swift/macosx $TOOLCHAIN/usr/bin/swift run FishyActorsDemo\n```\n\nall necessary flags to build this pre-release feature are already enabled as unsafe flags in `Package.swift`.\n\nIf you wanted to perform the invocation manually, it would look something like this:\n\n```\nexport TOOLCHAIN=/Library/Developer/Toolchains/swift-PR-39087-1109.xctoolchain\nDYLD_LIBRARY_PATH=$TOOLCHAIN/usr/lib/swift/macosx $TOOLCHAIN/usr/bin/swift run FishyActorsDemo\n```\n\nsetting the `DYLD_LIBRARY_PATH` is important, so don't forget it.\n\n### Sample output\n\nThe sample is a chat room application. It creates a few \"nodes\" and starts distributed actors on them. \n\nThere are two kinds of actors, a `ChatRoom` and `Chatter`s. A single node, representing a cloud component, hosts the chat room. And a few other nodes host chatters. Note that chatters can be on the same or on different nodes.\n\nAs the application runs, chatters join the remote `ChatRoom` and say hello there.\n\nThe chat room logs whenever a chatter joins the room, or sends a message:\n\n```\n// chat room logs\n[:8001/ChatRoom@130A3D1B-...] Chatter [:9003/Chatter@0A2F138C-...] joined this chat room about: 'Cute Capybaras'\n[:8001/ChatRoom@130A3D1B-...] Chatter [:9002/Chatter@8152F16D-...] joined this chat room about: 'Cute Capybaras'\n[:8001/ChatRoom@130A3D1B-...] Chatter [:9003/Chatter@4F3970DE-...] joined this chat room about: 'Cute Capybaras'\n[:8001/ChatRoom@130A3D1B-...] Forwarding message from [:9002/Chatter@8152F16D-...] to 2 other chatters...\n[:8001/ChatRoom@130A3D1B-...] Forwarding message from [:9003/Chatter@4F3970DE-...] to 2 other chatters...\n[:8001/ChatRoom@130A3D1B-...] Forwarding message from [:9003/Chatter@0A2F138C-...] to 2 other chatters...\n[:8001/ChatRoom@130A3D1B-...] Forwarding message from [:9002/Chatter@8152F16D-...] to 2 other chatters...\n[:8001/ChatRoom@130A3D1B-...] Forwarding message from [:9003/Chatter@4F3970DE-...] to 2 other chatters...\n```\n\nThe chat room sends a `\"Welcome ...\"` message to a joining chatter, and forwards all other chat messages sent to the room to the chatter itself.\nA chatters logs look like this: \n\n```\n// first chatter\n[:9002/Chatter@8152F16D-...] Welcome to the 'Cute Capybaras' chat room! (chatters: 2)\n[:9002/Chatter@8152F16D-...] Chatter [:9003/Chatter@0A2F138C-...] joined [:8001/ChatRoom@130A3D1B-...] (total known members in room 2 (including self))\n[:9002/Chatter@8152F16D-...]] :9003/Chatter@4F3970DE-... wrote: Welcome [:9003/Chatter@0A2F138C-...]!\n[:9002/Chatter@8152F16D-...]] :9003/Chatter@0A2F138C-... wrote: Long time no see [:9002/Chatter@8152F16D-...]!\n[:9002/Chatter@8152F16D-...] Chatter [:9003/Chatter@4F3970DE-...] joined [:8001/ChatRoom@130A3D1B-...] (total known members in room 3 (including self))\n[:9002/Chatter@8152F16D-...]] :9003/Chatter@4F3970DE-... wrote: Hi there,  [:9002/Chatter@8152F16D-...]!\n```\n\nNotice that the simplified ID printout contains the port number of the node the chatter is running on. In this example, the chatroom is running on port `8001` while the chatter is on `9002`. Other chatters may be on the same or on different \"nodes\" which are represented by actor transport instances. \n\nThis sample is a distributed application created from just a single process, but all the \"nodes\" communicate through networking with eachother.\nThe same application could be launched on different physical hosts (and then would have different IP addresses), this is what location transparency of distributed actors enables us to do.\n\n### Distributed Tracing\n\nAdditionally, the calls can also be traced using \"Swift Distributed Tracing\". To use tracing for the sample app, start the services in [`docker-compose.yaml`](SampleApp/docker-compose.yaml), then run the sample again.\n\n```sh\ndocker compose -p fishy-transport-sample up -d\n```\n\nAfterwards, open Jaeger [http://localhost:16686](http://localhost:16686) and you'll see traces similar to this:\n\n![Jaeger Example Trace](images/jaeger-trace.png)\n\n### Experimental flags\n\n> This project showcases **EXPERIMENTAL** language features, and in order to access them the `-enable-experimental-distributed` flag must be set.\n\nThe project is pre-configured with a few experimental flags that are necessary to enable distributed actors, these are configured in each target's `swiftSettings`:\n\n```swift\n      .target(\n          name: \"FishyActorTransport\",\n          dependencies: [\n            ...\n          ],\n          swiftSettings: [\n            .unsafeFlags([\n              \"-Xfrontend\", \"-enable-experimental-distributed\",\n              \"-Xfrontend\", \"-validate-tbd-against-ir=none\",\n              \"-Xfrontend\", \"-disable-availability-checking\", // availability does not matter since _Distributed is not part of the SDK at this point\n            ])\n          ]),\n```\n\n## SwiftPM Plugin\n\nDistributed actor transports are expected to ship with an associated SwiftPM plugin that takes care of source generating the necessary \"glue\" between distributed functions and the transport runtime.\n\nPlugins are run automatically when the project is build, and therefore add no hassle to working with distributed actors.\n\n### Verbose mode\n\nIt is possible to force the plugin to run in `--verbose` mode by setting the `VERBOSE` environment variable, like this:\n\n\n```\nVERBOSE=true DYLD_LIBRARY_PATH=$TOOLCHAIN/usr/lib/swift/macosx $TOOLCHAIN/usr/bin/swift run FishyActorsDemo\n\nAnalyze: /Users/ktoso/code/fishy-actor-transport/SampleApp/Sources/FishyActorsDemo/_PrettyDemoLogger.swift\nAnalyze: /Users/ktoso/code/fishy-actor-transport/SampleApp/Sources/FishyActorsDemo/Actors.swift\n  Detected distributed actor: ChatRoom\n    Detected distributed func: join\n    Detected distributed func: message\n    Detected distributed func: leave\n  Detected distributed actor: Chatter\n    Detected distributed func: join\n    Detected distributed func: chatterJoined\n    Detected distributed func: chatRoomMessage\nAnalyze: /Users/ktoso/code/fishy-actor-transport/SampleApp/Sources/FishyActorsDemo/main.swift\nGenerate extensions...\nWARNING: This is only a *mock* sample plugin implementation, real functions won't be generated!\n  Generate 'FishyActorTransport' extensions for 'distributed actor ChatRoom' -> file:///Users/ktoso/code/fishy-actor-transport/SampleApp/.build/plugins/outputs/sampleapp/FishyActorsDemo/FishyActorTransportPlugin/GeneratedFishyActors_1.swift\n  Generate 'FishyActorTransport' extensions for 'distributed actor Chatter' -> file:///Users/ktoso/code/fishy-actor-transport/SampleApp/.build/plugins/outputs/sampleapp/FishyActorsDemo/FishyActorTransportPlugin/GeneratedFishyActors_1.swift\n```\n"
 },
 {
  "repo": "apple/swift-numerics",
  "language": "Swift",
  "readme_contents": "# Swift Numerics\n  \n## Introduction\n\nSwift Numerics provides a set of modules that support numerical computing in Swift.\nThese modules fall broadly into two categories:\n\n- API that is too specialized to go into the standard library, but which is sufficiently general to be centralized in a single common package.\n- API that is under active development toward possible future inclusion in the standard library.\n\nThere is some overlap between these two categories, and an API that begins in the first category may migrate into the second as it matures and new uses are discovered.\n\nSwift Numerics modules are fine-grained.\nFor example, if you need support for Complex numbers, you can import ComplexModule[^1] as a standalone module:\n\n```swift\nimport ComplexModule\n\nlet z = Complex<Double>.i\n```\n\nThere is also a top-level `Numerics` module that re-exports the complete public interface of Swift Numerics:\n\n```swift\nimport Numerics\n\n// The entire Swift Numerics API is now available\n```\n\nSwift Numerics modules have minimal dependencies on other projects.\n\nThe current modules assume only the availability of the Swift and C standard libraries and the runtime support provided by compiler-rt.\n\nFuture expansion may assume the availability of other standard interfaces, such as [BLAS (Basic Linear Algebra Subprograms)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) and [LAPACK (Linear Algebra Package)](https://en.wikipedia.org/wiki/LAPACK), but modules with more specialized dependencies (or dependencies that are not available on all platforms supported by Swift) belong in a separate package.\n\nBecause we intend to make it possible to adopt Swift Numerics modules in the standard library at some future point, Swift Numerics uses the same license and contribution guidelines as the Swift project.\n\n## Using Swift Numerics in your project\n\nTo use Swift Numerics in a SwiftPM project:\n\n1. Add the following line to the dependencies in your `Package.swift` file:\n\n```swift\n.package(url: \"https://github.com/apple/swift-numerics\", from: \"1.0.0\"),\n```\n\n2. Add `Numerics` as a dependency for your target:\n\n```swift\n.target(name: \"MyTarget\", dependencies: [\n  .product(name: \"Numerics\", package: \"swift-numerics\"),\n  \"AnotherModule\"\n]),\n```\n\n3. Add `import Numerics` in your source code.\n\n## Source stability\n\nThe Swift Numerics package is source stable; version numbers follow [Semantic Versioning](https://semver.org).\nThe public API of the `swift-numerics` package consists of non-underscored declarations that are marked either `public` or `usableFromInline` in modules re-exported by the top-level `Numerics` module, *excepting any API that involves a conformance to Differentiable (because Differentiable itself is not stable in Swift)*.\nInterfaces that aren't part of the public API may continue to change in any release, including patch releases. \n\nNote that contents of the `_NumericsShims` and `_TestSupport` modules, as well as contents of the `Tests` directory, explicitly are not public API.\nThe definitions therein may therefore change at whim, and the entire module may be removed in any new release.\nIf you have a use case that requires underscored operations, please raise an issue to request that they be made public API.\n\nFuture minor versions of the package may introduce changes to these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate.\nAccordingly, from time to time, we expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release.\nRequiring a new Swift release will only require a minor version bump.\n\n## Contributing to Swift Numerics\n\nSwift Numerics is a standalone library that is separate from the core Swift project, but it will sometimes act as a staging ground for APIs that will later be incorporated into the Swift Standard Library.\nWhen that happens, such changes will be proposed to the Swift Standard Library using the established evolution process of the Swift project.\n\nSwift Numerics uses GitHub issues to track bugs and features. We use pull requests for development.\n\n### How to propose a new module\n\n1. Raise an issue with the [new module] tag.\n2. Raise a PR with an implementation sketch.\n3. Once you have some consensus, ask an admin to create a feature branch against which PRs can be raised.\n4. When the design has stabilized and is functional enough to be useful, raise a PR to merge the new module to master.\n\n### How to propose a new feature for an existing module\n\n1. Raise an issue with the [enhancement] tag.\n2. Raise a PR with your implementation, and discuss the implementation there.\n3. Once there is a consensus that the new feature is desirable and the design is suitable, it can be merged.\n\n### How to fix a bug, or make smaller improvements\n\n1. Raise a PR with your change. \n2. Make sure to add test coverage for whatever changes you are making.\n\n### Forums\n\nQuestions about how to use Swift Numerics modules, or issues that are not clearly bugs can be discussed in the [\"Swift Numerics\" section of the Swift forums](https://forums.swift.org/c/related-projects/swift-numerics).\n\n## Modules\n\n1. [`RealModule`](Sources/RealModule/README.md)\n2. [`ComplexModule`](Sources/ComplexModule/README.md)\n3. [`IntegerUtilities`](Sources/IntegerUtilties/README.md) (on main only, not yet present in a released tag)\n\n## Future expansion\n\n1. [Large Fixed-Width Integers](https://github.com/apple/swift-numerics/issues/4)\n2. [Arbitrary-Precision Integers](https://github.com/apple/swift-numerics/issues/5)\n3. [Shaped Arrays](https://github.com/apple/swift-numerics/issues/6)\n4. [Decimal Floating-point](https://github.com/apple/swift-numerics/issues/7)\n\n[^1]: The module is named `ComplexModule` instead of `Complex` because Swift is currently unable to use the fully-qualified name for types when a type and module have the same name (discussion here: https://forums.swift.org/t/pitch-fully-qualified-name-syntax/28482).\n    This would prevent users of Swift Numerics who don't need generic types from doing things such as:\n\n    ```swift\n    import Complex\n    // I know I only ever want Complex<Double>, so I shouldn't need the generic parameter.\n    typealias Complex = Complex.Complex<Double> // This doesn't work, because name lookup fails.\n    ```\n    \n    For this reason, modules that would have this ambiguity are suffixed with `Module` within Swift Numerics:\n    \n    ```swift\n    import ComplexModule\n    // I know I only ever want Complex<Double>, so I shouldn't need the generic parameter.\n    typealias Complex = ComplexModule.Complex<Double>\n    // But I can still refer to the generic type by qualifying the name if I need it occasionally:\n    let a = ComplexModule.Complex<Float>\n    ```\n\n    The `Real` module does not contain a `Real` type, but does contain a `Real` protocol.\n    Users may want to define their own `Real` type (and possibly re-export the `Real` module)--that is why the suffix is also applied there.\n    New modules have to evaluate this decision carefully, but can err on the side of adding the suffix.\n    It's expected that most users will simply `import Numerics`, so this isn't an issue for them.\n"
 },
 {
  "repo": "apple/cups",
  "language": "C",
  "readme_contents": "README - Apple CUPS v2.3.5 - 2021-09-13\n=======================================\n\n> Note: Apple CUPS is the version of CUPS that is shipped with macOS and iOS.\n> For the current version of CUPS that is used on other operating systems, see\n> <https://openprinting.github.io/cups> for details.\n\n\nINTRODUCTION\n------------\n\nCUPS is a standards-based, open source printing system developed by Apple Inc.\nfor macOS\u00ae and other UNIX\u00ae-like operating systems.  CUPS uses the Internet\nPrinting Protocol (\"IPP\") and provides System V and Berkeley command-line\ninterfaces, a web interface, and a C API to manage printers and print jobs.  It\nsupports printing to both local (parallel, serial, USB) and networked printers,\nand printers can be shared from one computer to another, even over the Internet!\n\nInternally, CUPS uses PostScript Printer Description (\"PPD\") files to describe\nprinter capabilities and features and a wide variety of generic and device-\nspecific programs to convert and print many types of files.  Sample drivers are\nincluded with CUPS to support many Dymo, EPSON, HP, Intellitech, OKIDATA, and\nZebra printers.  Many more drivers are available online and (in some cases) on\nthe driver CD-ROM that came with your printer.\n\nCUPS is licensed under the Apache License Version 2.0.  See the file\n\"LICENSE\" for more information.\n\n\nREADING THE DOCUMENTATION\n-------------------------\n\nInitial documentation to get you started is provided in the root directory of\nthe CUPS sources:\n\n- `CHANGES.md`: A list of changes in the current major release of CUPS.\n- `CONTRIBUTING.md`: Guidelines for contributing to the CUPS project.\n- `CREDITS.md`: A list of past contributors to the CUPS project.\n- `DEVELOPING.md`: Guidelines for developing code for the CUPS project.\n- `INSTALL.md`: Instructions for building and installing CUPS.\n- `LICENSE`: The CUPS license agreement (Apache 2.0).\n- `NOTICE`: Copyright notices and exceptions to the CUPS license agreement.\n- `README.md`: This file.\n\nOnce you have installed the software you can access the documentation (and a\nbunch of other stuff) online at <http://localhost:631/> and using the `man`\ncommand, for example `man cups`.\n\nIf you're having trouble getting that far, the documentation is located under\nthe `doc/help` and `man` directories.\n\nPlease read the documentation before asking questions.\n\n\nGETTING SUPPORT AND OTHER RESOURCES\n-----------------------------------\n\nIf you have problems, *read the documentation first!*  We also provide two\nmailing lists which are available at <https://lists.cups.org/mailman/listinfo>.\n\nSee the CUPS web site at <https://www.cups.org/> for other resources.\n\n\nSETTING UP PRINTER QUEUES USING YOUR WEB BROWSER\n------------------------------------------------\n\nCUPS includes a web-based administration tool that allows you to manage\nprinters, classes, and jobs on your server.  Open <http://localhost:631/admin/>\nin your browser to access the printer administration tools:\n\n*Do not* use the hostname for your machine - it will not work with the default\nCUPS configuration.  To enable administration access on other addresses, check\nthe `Allow Remote Administration` box and click on the `Change Settings` button.\n\nYou will be asked for the administration password (root or any other user in the\n\"sys\", \"system\", \"root\", \"admin\", or \"lpadmin\" group on your system) when\nperforming any administrative function.\n\n\nSETTING UP PRINTER QUEUES FROM THE COMMAND-LINE\n-----------------------------------------------\n\nCUPS currently uses PPD (PostScript Printer Description) files that describe\nprinter capabilities and driver programs needed for each printer.  The\n`everywhere` PPD is used for nearly all modern networks printers sold since\nabout 2009.  For example, the following command creates a print queue for a\nprinter at address \"11.22.33.44\":\n\n    lpadmin -p printername -E -v ipp://11.22.33.44/ipp/print -m everywhere\n\nCUPS also includes several sample PPD files you can use for \"legacy\" printers:\n\n   Driver                         | PPD Name\n   -----------------------------  | ------------------------------\n   Dymo Label Printers            | drv:///sample.drv/dymo.ppd\n   Intellitech Intellibar         | drv:///sample.drv/intelbar.ppd\n   EPSON 9-pin Series             | drv:///sample.drv/epson9.ppd\n   EPSON 24-pin Series            | drv:///sample.drv/epson24.ppd\n   Generic PCL Laser Printer      | drv:///sample.drv/generpcl.ppd\n   Generic PostScript Printer     | drv:///sample.drv/generic.ppd\n   HP DeskJet Series              | drv:///sample.drv/deskjet.ppd\n   HP LaserJet Series             | drv:///sample.drv/laserjet.ppd\n   OKIDATA 9-Pin Series           | drv:///sample.drv/okidata9.ppd\n   OKIDATA 24-Pin Series          | drv:///sample.drv/okidat24.ppd\n   Zebra CPCL Label Printer       | drv:///sample.drv/zebracpl.ppd\n   Zebra EPL1 Label Printer       | drv:///sample.drv/zebraep1.ppd\n   Zebra EPL2 Label Printer       | drv:///sample.drv/zebraep2.ppd\n   Zebra ZPL Label Printer        | drv:///sample.drv/zebra.ppd\n\nYou can run the `lpinfo -m` command to list all of the available drivers:\n\n    lpinfo -m\n\nRun the `lpinfo -v` command to list the available printers:\n\n    lpinfo -v\n\nThen use the correct URI to add the printer using the `lpadmin` command:\n\n    lpadmin -p printername -E -v device-uri -m ppd-name\n\nCurrent network printers typically use `ipp` or `ipps` URIS:\n\n    lpadmin -p printername -E -v ipp://11.22.33.44/ipp/print -m everywhere\n    lpadmin -p printername -E -v ipps://11.22.33.44/ipp/print -m everywhere\n\nOlder network printers typically use `socket` or `lpd` URIs:\n\n    lpadmin -p printername -E -v socket://11.22.33.44 -m ppd-name\n    lpadmin -p printername -E -v lpd://11.22.33.44/ -m ppd-name\n\nThe sample drivers provide basic printing capabilities, but generally do not\nexercise the full potential of the printers or CUPS.  Other drivers provide\ngreater printing capabilities.\n\n\nPRINTING FILES\n--------------\n\nCUPS provides both the System V `lp` and Berkeley `lpr` commands for printing:\n\n    lp filename\n    lpr filename\n\nBoth the `lp` and `lpr` commands support printing options for the driver:\n\n    lp -o media=A4 -o resolution=600dpi filename\n    lpr -o media=A4 -o resolution=600dpi filename\n\nCUPS recognizes many types of images files as well as PDF, PostScript, and text\nfiles, so you can print those files directly rather than through an application.\n\nIf you have an application that generates output specifically for your printer\nthen you need to use the `-oraw` or `-l` options:\n\n    lp -o raw filename\n    lpr -l filename\n\nThis will prevent the filters from misinterpreting your print file.\n\n\nLEGAL STUFF\n-----------\n\nCopyright \u00a9 2007-2021 by Apple Inc.\nCopyright \u00a9 1997-2007 by Easy Software Products.\n\nCUPS is provided under the terms of the Apache License, Version 2.0 with\nexceptions for GPL2/LGPL2 software.  A copy of this license can be found in the\nfile `LICENSE`.  Additional legal information is provided in the file `NOTICE`.\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations under the License.\n"
 },
 {
  "repo": "apple/swift-package-registry-compatibility-test-suite",
  "language": "Swift",
  "readme_contents": "# Swift Package Registry Compatibility Test Suite\n\nContains tools for building and testing Swift package registry server that implements\n[SE-0292](https://github.com/apple/swift-evolution/blob/main/proposals/0292-package-registry-service.md) and [SE-0321](https://github.com/apple/swift-evolution/blob/main/proposals/0321-package-registry-publish.md).\n\n### Compatibility Test Suite\n\nThe [`PackageRegistryCompatibilityTestSuite`](./Sources/PackageRegistryCompatibilityTestSuite) module provides the\n`package-registry-compatibility` command-line tool for testing all APIs defined in the \n[service specification](https://github.com/apple/swift-package-manager/blob/main/Documentation/Registry.md).\n\n### Registry Service Reference Implementation\n\nThe [`PackageRegistryExample`](./Sources/PackageRegistryExample) module is a demo server app that implements the\n[package registry service specification](https://github.com/apple/swift-package-manager/blob/main/Documentation/Registry.md) and can be\ndeployed locally using Docker.\n\n### Testing\n\nRunning the unit tests for the compatibility test suite requires a package registry server. The easiest way is to use docker:\n\n```\ndocker-compose -f docker/docker-compose.yml -f docker/docker-compose.2004.55.yml run test\n```\n"
 },
 {
  "repo": "apple/swift-se0288-is-power",
  "language": "Swift",
  "readme_contents": "# SE0288_IsPower\n\n\n**SE0288_IsPower** is a standalone library that implements the Swift Evolution proposal\n[SE-0288: Adding `isPower(of:)` to BinaryInteger][proposal].\nYou can use this package independently,\nor as part of the [standard library preview package][stdlib-preview].\n\n* Proposal: [SE-0288][proposal]\n* Author: [Ding Ye](https://github.com/dingobye)\n\n\n## Introduction\n\nThis package adds a public API `isPower(of:)`, as an extension method,\nto the `BinaryInteger` protocol.\nIt checks if an integer is a power of another.\nThat is, `a.isPower(of: b)` checks whether there exists any integer `n` such that `a == pow(b, n)` is true.\n\n```swift\nimport SE0288_IsPower\n\nlet x: Int = Int.random(in: 0000..<0288)\n1.isPower(of: x)      // 'true' since x^0 == 1\n\nlet y: UInt = 1000\ny.isPower(of: 10)     // 'true' since 10^3 == 1000\n\n(-1).isPower(of: 1)   // 'false'\n\n(-32).isPower(of: -2) // 'true' since (-2)^5 == -32\n```\n\n\n## Usage\n\nYou can add this library as a dependency to any Swift package. \nAdd this line to the `dependencies` parameter in your `Package.swift` file:\n\n```swift\n.package(url: \"https://github.com/apple/swift-se0288-is-power\", from: \"2.0.0\"),\n```\n\nNext, add the module as a dependency for your targets that will use the library:\n\n```swift\n.product(name: \"SE0288_IsPower\", package: \"swift-se0288-is-power\"),\n```\n\nYou can now use `import SE0288_IsPower` to make the library available in any Swift file.\n\n## Contributing\n\nContributions to this package and the standard library preview package are welcomed and encouraged!\n\n- For help using this package or the standard library preview package, please [visit the Swift forums][user-forums]. \n- For issues related to these packages, [file a bug at bugs.swift.org][bugs].\n- Changes or additions to the APIs are made through \n  the [Swift Evolution process][evolution-process].\n  Please see the [guide for Contributing to Swift][contributing] for information.\n\n\n[proposal]: https://github.com/apple/swift-evolution/blob/master/proposals/0288-binaryinteger-ispower.md\n[stdlib-preview]: https://github.com/apple/swift-standard-library-preview \n[user-forums]: https://forums.swift.org/c/swift-users/\n[bugs]: https://bugs.swift.org\n[evolution-process]: https://github.com/apple/swift-evolution/blob/master/process.md\n[contributing]: https://swift.org/contributing\n"
 },
 {
  "repo": "apple/swift-standard-library-preview",
  "language": "Swift",
  "readme_contents": "# Standard Library Preview\n\nThe Swift Standard Library Preview package provides access to new functionality \nthat has been accepted into the standard library through the Swift Evolution process,\nbut has not yet shipped as part of an official Swift release.\n\nEach approved standard library proposal is available as its own individual package, listed below.\nThe `StandardLibraryPreview` package in this repository acts as an umbrella library,\nre-exporting each of the individual packages.\n\n## Included Modules\n\n- [**`SE0270_RangeSet`**](https://github.com/apple/swift-se0270-range-set/): \n  Operations on noncontiguous subranges of collections, \n  such as `subranges(where:)` and `moveSubranges(_:to:)`, \n  as well as the supporting `RangeSet` type.\n- [**`SE0288_IsPower`**](https://github.com/apple/swift-se0288-is-power/):\n  Extends `BinaryInteger` with an `isPower(of:)` method that returns whether\n  an integer is a power of another.\n\n## Usage\n\nTo use the umbrella library in a Swift Package Manager project,\nadd the following to your `Package.swift` file's dependencies:\n\n```swift\n.package(url: \"https://github.com/apple/swift-standard-library-preview.git\", from: \"0.0.1\"),\n```\n\nNext, add the preview package as a dependency for your target:\n\n```swift\n.product(name: \"StandardLibraryPreview\", package: \"swift-standard-library-preview\"),\n```\n\n> **Important:** The Standard Library Preview package is under continuous development,\nand maintains a major version of `0` to indicate that it is not intended to be source stable.\nFor example, after a submodule's functionality has been included\nwith shipping versions of the Swift standard library for some time,\nit may be removed from the umbrella library.\n>\n> If you require source stability in your project,\nadd the individual submodules instead of the `swift-standard-library-preview` package.\n\n## Contributing\n\nContributions to the Standard Library Preview package are welcomed and encouraged!\n\n- For help using the standard library preview package or the related submodules,\n  please [visit the Swift forums][user-forums].\n- For issues related to these packages, [file a bug at bugs.swift.org][bugs].\n- Changes or additions to the APIs are made through\n  the [Swift Evolution process][evolution-process].\n  Please see the [guide for Contributing to Swift][contributing] for information.\n\n[user-forums]: https://forums.swift.org/c/swift-users/\n[bugs]: https://bugs.swift.org\n[evolution-process]: https://github.com/apple/swift-evolution/blob/master/process.md\n[contributing]: https://swift.org/contributing\n"
 },
 {
  "repo": "apple/swift-se0270-range-set",
  "language": "Swift",
  "readme_contents": "# SE0270_RangeSet\n\n**SE0270_RangeSet** is a standalone library that implements the Swift Evolution proposal\n[SE-0270: Add Collection Operations on Noncontiguous Elements][proposal]. \nYou can use this package independently, \nor as part of the [standard library preview package][stdlib-preview].\n\n## Functionality\n\n**SE0270_RangeSet** provides operations on noncontiguous subranges of collections, \nsuch as `subranges(where:)` and `moveSubranges(_:to:)`, \nas well as the supporting `RangeSet` type.\n\n```swift\nimport SE0270_RangeSet\n\nvar numbers = [10, 12, -5, 14, -3, -9, 15]\nlet negatives = numbers.subranges(where: { $0 < 0 })\n// numbers[negatives].count == 3\n\nnumbers.moveSubranges(negatives, to: 0)\n// numbers == [-5, -3, -9, 10, 12, 14, 15]\n```\n\n## Usage\n\nYou can add this library as a dependency to any Swift package. \nAdd this line to the `dependencies` parameter in your `Package.swift` file:\n\n```swift\n.package(\n    url: \"https://github.com/apple/swift-se0270-range-set\",\n    from: \"1.0.0\"),\n```\n\nNext, add the module as a dependency for your targets that will use the library:\n\n```swift\n.product(name: \"SE0270_RangeSet\", package: \"swift-se0270-range-set\"),\n```\n\nYou can now use `import SE0270_RangeSet` to make the library available in any Swift file.\n\n## Contributing\n\nContributions to this package and the standard library preview package are welcomed and encouraged!\n\n- For help using this package or the standard library preview package, please [visit the Swift forums][user-forums]. \n- For issues related to these packages, [file a bug at bugs.swift.org][bugs].\n- Changes or additions to the APIs are made through \n  the [Swift Evolution process][evolution-process].\n  Please see the [guide for Contributing to Swift][contributing] for information.\n\n\n[proposal]: https://github.com/apple/swift-evolution/blob/master/proposals/0270-rangeset-and-collection-operations.md\n[stdlib-preview]: https://github.com/apple/swift-standard-library-preview \n[user-forums]: https://forums.swift.org/c/swift-users/\n[bugs]: https://bugs.swift.org\n[evolution-process]: https://github.com/apple/swift-evolution/blob/master/process.md\n[contributing]: https://swift.org/contributing\n"
 },
 {
  "repo": "apple/ml-core",
  "language": "Python",
  "readme_contents": "# CoRe: Contrastive Recurrent State-Space Models\n\nThis code implements the CoRe model and reproduces experimental results found in<br>\n**Robust Robotic Control from Pixels using Contrastive Recurrent State-Space models**<br>\nNeurIPS Deep Reinforcement Learning Workshop 2021 <br>\nNitish Srivastava, Walter Talbott, Martin Bertran Lopez, Shuangfei Zhai & Joshua M. Susskind<br>\n[[paper](https://arxiv.org/abs/2112.01163)] <br>\n\n![cartpole](videos/medium-cartpole.gif)\n\n![cheetah](videos/medium-cheetah.gif)\n\n![walker](videos/hard-walker.gif)\n\n\n## Requirements and Installation\nClone this repository and then execute the following steps. See `setup.sh` for an example of how to run these steps on a Ubuntu 18.04 machine. \n\n* Install dependencies.\n    ```\n    apt install -y libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n            libosmesa6-dev software-properties-common net-tools unzip \\\n            virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf xvfb ffmpeg\n    ```\n* Download the [DAVIS 2017\n  dataset](https://davischallenge.org/davis2017/code.html). Make sure to select the 2017 TrainVal - Images and Annotations (480p). The training images will be used as distracting backgrounds. The `DAVIS` directory should be in the same directory as the code. Check that `ls ./DAVIS/JPEGImages/480p/...` shows 90 video directories.\n* Install MuJoCo 2.1.\n\t- Download [MuJoCo version 2.1](https://mujoco.org/download) binaries for Linux or macOS.\n\t- Unzip the downloaded `mujoco210` directory into `~/.mujoco/mujoco210`.\n* Install MuJoCo 2.0 (For robosuite experiments only).\n\t- Download [MuJoCo version 2.0](https://roboti.us/download.html) binaries for Linux or macOS.\n\t- Unzip the downloaded directory and move it into `~/.mujoco/`.\n    - Symlink `mujoco200_linux` (or `mujoco200_macos`) to `mujoco200`.\n    ```\n    ln -s ~/.mujoco/mujoco200_linux ~/.mujoco/mujoco200\n    ```\n    - Place the [license key](https://roboti.us/license.html) at `~/.mujoco/mjkey.txt`.\n    - Add the MuJoCo binaries to `LD_LIBRARY_PATH`.\n    ```\n    export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco200/bin:$LD_LIBRARY_PATH\n    ```\n* Setup EGL GPU rendering (if a GPU is available).\n    - To ensure that the GPU is prioritized over the CPU for EGL rendering\n    ```\n    cp 10_nvidia.json /usr/share/glvnd/egl_vendor.d/\n    ```\n    - Create a dummy nvidia directory so that mujoco_py builds the extensions needed for GPU rendering.\n    ```\n    mkdir -p /usr/lib/nvidia-000\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-000\n    ```\n* Create a conda environment.\n\n    For Distracting Control Suite\n    ```\n    conda env create -f conda_env.yml\n    ```\n    \n    For Robosuite\n    ```\n    conda env create -f conda_env_robosuite.yml\n    ```\n\n## Training\n\n* The CoRe model can be trained on the Distracting Control Suite as follows:\n\n  ```\n  conda activate core\n  MUJOCO_GL=egl CUDA_VISIBLE_DEVICES=0 python train.py --config configs/dcs/core.yaml \n  ```\nThe training artifacts, including tensorboard logs and videos of validation rollouts will be written in `./artifacts/`.\n\nTo change the distraction setting, modify the `difficulty` parameter in `configs/dcs/core.yaml`. Possible values are `['easy', 'medium', 'hard', 'none', 'hard_bg']`.\n\nTo change the domain, modify the `domain` parameter in `configs/dcs/core.yaml`. Possible values are `['ball_in_cup', 'cartpole', 'cheetah', 'finger', 'reacher', 'walker']`.\n\n* To train on Robosuite (Door Task, Franka Panda Arm)\n  \n  - Using RGB image and proprioceptive inputs.\n  ```\n  conda activate core_robosuite\n  MUJOCO_GL=egl CUDA_VISIBLE_DEVICES=0 python train.py --config configs/robosuite/core.yaml\n  ```\n  - Using RGB image inputs only.\n  ```\n  conda activate core_robosuite\n  MUJOCO_GL=egl CUDA_VISIBLE_DEVICES=0 python train.py --config configs/robosuite/core_imageonly.yaml\n  ```\n\n## Citation\n```\n@article{srivastava2021core,\n    title={Robust Robotic Control from Pixels using Contrastive Recurrent State-Space Models}, \n    author={Nitish Srivastava and Walter Talbott and Martin Bertran Lopez and Shuangfei Zhai and Josh Susskind},\n    journal={NeurIPS Deep Reinforcement Learning Workshop},\n    year={2021}\n}\n```\n\n## License\nThis code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/FHIRModels",
  "language": "Swift",
  "readme_contents": "FHIRModels\n==========\n\n[![0.3.2](https://img.shields.io/badge/Latest-0.3.2-blueviolet.svg?style=flat)](https://github.com/apple/FHIRModels/releases) [![\ud83d\udd25 FHIR DSTU2, STU3, R4, build](https://img.shields.io/badge/\ud83d\udd25_FHIR-DSTU2_\u2022%20STU3_\u2022%20R4_\u2022%20\u03b24.5-orange.svg?style=flat)][fhir] ![Works on macOS, iOS, watchOS, tvOS and Linux](https://img.shields.io/badge/Platform-macOS_\u2022%20iOS_\u2022%20watchOS_\u2022%20tvOS_\u2022%20Linux-blue.svg?style=flat) [![Swift Package Manager](https://img.shields.io/badge/SPM-compatible-brightgreen.svg?style=flat)][spm] [![License](https://img.shields.io/badge/License-APACHE_2.0-lightgrey.svg?style=flat)](LICENSE)\n\nFHIRModels is a Swift library for [FHIR\u00ae][fhir] resource data models.\n\n## Features\n\n- Native Swift representation of FHIR resources, elements and data types\n- Separate targets for DSTU2, STU3, R4 and latest build versions\n- Enforced non-nullability of mandatory parameters\n- Enums for most closed code systems\n- Enums to support value[x] types\n- Date/Time parsing, validation and conversion to and from NSDate\n- Swift Codable support\n\n## Requirements\n\nFHIRModels works with Swift 5.1 and newer. Running unit tests requires Swift 5.3 and newer.\n\n## Installation\n\n[Swift Package Manager][spm] is the recommended way to add FHIRModels to your project:\n\n1. In Xcode 11 and newer, go to \u201cFile\u201d \u00bb \u201cSwift Package Management\u201d \u00bb \u201cAdd Package Dependency...\u201d\n2. Search for \u201cFHIRModels\u201d in the panel that comes up or paste the full URL to the GitHub repository, then select the package\n3. Pick the versioning that best works for you (usually the current version and \u201cup to next major\u201d)\n4. In order to update the dependency, periodically select \u201cUpdate to Latest Package Versions\u201d from Xcode 11's \u201cFile\u201d \u00bb \u201cSwift Package Management\u201d menu\n\nAlternatively, you can add FHIRModels to your `Package.swift` file as a dependency:\n\n```swift\ndependencies: [\n    .package(url: \"https://github.com/apple/FHIRModels.git\",\n            .upToNextMajor(from: \"0.2.0\"))\n]\n```\n\nUsage\n=====\n\n## Instantiate FHIR Models\n\nAssuming you have data that you know represents a FHIR resource, you have two options of instantiating a model. We'll assume you have JSON data for the examples below.\n\n### 1. Use ResourceProxy\n\nThe `ResourceProxy` type inspects the `resourceType` in the data and instantiates the appropriate class for you:\n\n```swift\nimport ModelsR4\n\nlet data = <FHIR JSON data>\nlet decoder = JSONDecoder()\ndo {\n    let proxy = try decoder.decode(ResourceProxy.self, from: data)\n    \n    let resource = proxy.get()\n    // `resource` is a generic `Resource`\n    \n    let patient = proxy.get(if: Patient.self)\n    // `patient` is a nullable `Patient`\n    \n    if case .patient(let patient) = proxy {\n        // `patient` is a `Patient`\n    }\n    \n    switch proxy {\n    case .patient(let patient):\n        // `patient` is a `Patient`\n        break\n    default:\n        break\n    }\n} catch {\n    print(\"Failed to instantiate: \\(error)\")\n}\n```\n\n### 2. Decode to known Resource\n\nIf you think you know the correct resource type already:\n\n```swift\nimport ModelsR4\n\nlet data = <FHIR JSON data>\nlet decoder = JSONDecoder()\ndo {\n    let resource = try decoder.decode(Patient.self, from: data)\n} catch {\n    print(\"Failed to instantiate Patient: \\(error)\")\n}\n```\n\n### 3. Get resources from Bundle\n\nTo get certain resources from a `Bundle` you can do:\n\n```swift\nimport ModelsR4\n\nlet data = <FHIR JSON data>\nlet bundle = try JSONDecoder().decode(ModelsR4.Bundle.self, from: data)\nlet observations = bundle.entry?.compactMap {\n    $0.resource?.get(if: ModelsR4.Observation.self)\n}\n// observations is an array of `Observation` instances\n```\n\n## Model Properties\n\nFHIR Resource and Element types are represented as Swift classes,\nprimitive data types are represented as structs.\nAll primitives are defined as `FHIRPrimitive`,\ngenericized to the appropriate type such as `FHIRBool` or `DateTime`.\n\n### Patient with a name\n\nThe library makes extensive use of `ExpressibleBy{Type}Literal` so you can take shortcuts when instantiating primitives.\nFor example, `HumanName.given` is an Array of `FHIRPrimitive<FHIRString>` that can be instantiated as follows:\n\n```swift\nimport ModelsR4\n\nlet name = HumanName(family: \"Altick\", given: [\"Kelly\"])\nlet patient = Patient(name: [name])\n\nname.given?.first                   // FHIRPrimitive<FHIRString>?\nname.given?.first?.value            // FHIRString?\nname.given?.first?.value?.string    // String?\n```\n\n### Working with primitives\n\nYou may be tempted to get the Swift native types from primitive values and pass these around.\nTo get a String from the resource ID you would do:\n\n```swift\nlet id = resource.id?.value?.string\n```\n\nInstead, consider passing the element around in its full form, in this case as `FHIRPrimitive<FHIRString>`.\nThis means you will not lose extensions while you can still use some primitives in code as if they were native types.\nWith `FHIRPrimitive<FHIRString>` for example, you can actually do:\n\n```swift\nif resource.id == \"101\" {\n    \n}\n```\n\nConversely, you can also assign many `FHIRPrimitive` types with String, Bool or numeric literals, for example:\n\n```swift\nlet patient = Patient(...)\npatient.id = \"101\"\npatient.active = true\n```\n\nLastly, many Swift native types have been extended to offer an `asFHIR{type}Primitive()` method. URL and String for example offer:\n\n```swift\nlet url = URL(string: \"http://apple.com\")!.asFHIRURIPrimitive()\n// url is a `FHIRPrimitive<FHIRURI>`\nlet str = \"http://hl7.org/fhir\".asFHIRURIPrimitive()\n// str is a `FHIRPrimitive<FHIRURI>?`\n```\n\n### Date & Time\n\nFHIRModels handles date/time \u2194\ufe0e string conversion transparently and will discard invalid dates.\n\n```swift\ndo {\n    let dateTimeString = \"2020-03-12T12:33:54.6543-06:00\"\n    let dateTime = try DateTime(dateTimeString)\n    dateTime.date.year == 2020\n    dateTime.time.minute == 33\n    dateTime.timeZone == TimeZone(secondsFromGMT: -6 * 3600)\n} catch {\n    print(\"Failed to parse date time string: \\(error)\")\n}\n```\n\n---\n\nGetting Involved\n================\n\nPlease feel free to open [GitHub issues][issues] for questions, suggestions or issues you may have.\nBecause the majority of the codebase is generated we will not be able to accept Pull Requests at this time.\n\nLicense\n=======\n\nThis work is [APACHE 2.0 licensed](./LICENSE).\n\nFHIR\u00ae is the registered trademark of HL7 and is used with the permission of HL7. Use of the FHIR trademark does not constitute endorsement of this product by HL7.\n\n[fhir]: https://hl7.org/fhir\n[issues]: https://github.com/apple/FHIRModels/issues\n[spm]: https://github.com/apple/swift-package-manager\n"
 },
 {
  "repo": "apple/turicreate",
  "language": "C++",
  "readme_contents": "Quick Links: [Installation](#supported-platforms) | [Documentation](#documentation)\n\n[![Build Status](https://travis-ci.com/apple/turicreate.svg?branch=master)](#)\n[![PyPI Release](https://img.shields.io/pypi/v/turicreate.svg)](#)\n[![Python Versions](https://img.shields.io/pypi/pyversions/turicreate.svg)](#)\n\n[<img align=\"right\" src=\"https://docs-assets.developer.apple.com/turicreate/turi-dog.svg\" alt=\"Turi Create\" width=\"100\">](#)\n\n# Turi Create \n\nTuri Create simplifies the development of custom machine learning models. You\ndon't have to be a machine learning expert to add recommendations, object\ndetection, image classification, image similarity or activity classification to\nyour app.\n\n* **Easy-to-use:** Focus on tasks instead of algorithms\n* **Visual:** Built-in, streaming visualizations to explore your data\n* **Flexible:** Supports text, images, audio, video and sensor data\n* **Fast and Scalable:** Work with large datasets on a single machine\n* **Ready To Deploy:** Export models to Core ML for use in iOS, macOS, watchOS, and tvOS apps\n\nWith Turi Create, you can accomplish many common ML tasks:\n\n| ML Task                 | Description                      |\n|:------------------------:|:--------------------------------:|\n| [Recommender](https://apple.github.io/turicreate/docs/userguide/recommender/)             | Personalize choices for users    |\n| [Image Classification](https://apple.github.io/turicreate/docs/userguide/image_classifier/)    | Label images                     |\n| [Drawing Classification](https://apple.github.io/turicreate/docs/userguide/drawing_classifier)  | Recognize Pencil/Touch Drawings and Gestures                     |\n| [Sound Classification](https://apple.github.io/turicreate/docs/userguide/sound_classifier)  | Classify sounds                     |\n| [Object Detection](https://apple.github.io/turicreate/docs/userguide/object_detection/)        | Recognize objects within images  |\n| [One Shot Object Detection](https://apple.github.io/turicreate/docs/userguide/one_shot_object_detection/)    | Recognize 2D objects within images using a single example  |\n| [Style Transfer](https://apple.github.io/turicreate/docs/userguide/style_transfer/)        | Stylize images |\n| [Activity Classification](https://apple.github.io/turicreate/docs/userguide/activity_classifier/) | Detect an activity using sensors |\n| [Image Similarity](https://apple.github.io/turicreate/docs/userguide/image_similarity/)        | Find similar images              |\n| [Classifiers](https://apple.github.io/turicreate/docs/userguide/supervised-learning/classifier.html)             | Predict a label           |\n| [Regression](https://apple.github.io/turicreate/docs/userguide/supervised-learning/regression.html)              | Predict numeric values           |\n| [Clustering](https://apple.github.io/turicreate/docs/userguide/clustering/)              | Group similar datapoints together|\n| [Text Classifier](https://apple.github.io/turicreate/docs/userguide/text_classifier/)         | Analyze sentiment of messages    |\n\n\nExample: Image classifier with a few lines of code\n--------------------------------------------------\n\nIf you want your app to recognize specific objects in images, you can build your own model with just a few lines of code:\n\n```python\nimport turicreate as tc\n\n# Load data \ndata = tc.SFrame('photoLabel.sframe')\n\n# Create a model\nmodel = tc.image_classifier.create(data, target='photoLabel')\n\n# Make predictions\npredictions = model.predict(data)\n\n# Export to Core ML\nmodel.export_coreml('MyClassifier.mlmodel')\n```\n \nIt's easy to use the resulting model in an [iOS application](https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml):\n\n<p align=\"center\"><img src=\"https://docs-assets.developer.apple.com/published/a2c37bce1f/689f61a6-1087-4112-99d9-bbfb326e3138.png\" alt=\"Turi Create\" width=\"600\"></p>\n\nSupported Platforms\n-------------------\n\nTuri Create supports:\n\n* macOS 10.12+\n* Linux (with glibc 2.10+)\n* Windows 10 (via WSL)\n\nSystem Requirements\n-------------------\n\nTuri Create requires:\n\n* Python 2.7, 3.5, 3.6, 3.7, 3.8\n* x86\\_64 architecture\n* At least 4 GB of RAM\n\nInstallation\n------------\n\nFor detailed instructions for different varieties of Linux see [LINUX\\_INSTALL.md](LINUX_INSTALL.md).\nFor common installation issues see [INSTALL\\_ISSUES.md](INSTALL_ISSUES.md).\n\nWe recommend using virtualenv to use, install, or build Turi Create. \n\n```shell\npip install virtualenv\n```\n\nThe method for installing *Turi Create* follows the\n[standard python package installation steps](https://packaging.python.org/installing/).\nTo create and activate a Python virtual environment called `venv` follow these steps:\n\n```shell\n# Create a Python virtual environment\ncd ~\nvirtualenv venv\n\n# Activate your virtual environment\nsource ~/venv/bin/activate\n```\nAlternatively, if you are using [Anaconda](https://www.anaconda.com/what-is-anaconda/), you may use its virtual environment:\n```shell\nconda create -n virtual_environment_name anaconda\nconda activate virtual_environment_name\n```\n\nTo install `Turi Create` within your virtual environment:\n```shell\n(venv) pip install -U turicreate\n```\n\nDocumentation\n-------------\n\nThe package [User Guide](https://apple.github.io/turicreate/docs/userguide) and [API Docs](https://apple.github.io/turicreate/docs/api) contain\nmore details on how to use Turi Create.\n\nGPU Support\n-----------\n\nTuri Create **does not require a GPU**, but certain models can be accelerated 9-13x by utilizing a GPU.\n\n| Linux                     | macOS 10.13+         | macOS 10.14+ discrete GPUs, macOS 10.15+ integrated GPUs |\n| :-------------------------|:---------------------|:---------------------------------------------------------|\n| Activity Classification   | Image Classification | Activity Classification                                  |\n| Drawing Classification    | Image Similarity     | Object Detection                                         |\n| Image Classification      | Sound Classification | One Shot Object Detection                                |\n| Image Similarity          |                      | Style Transfer                                           |\n| Object Detection          |                      |                                                          |\n| One Shot Object Detection |                      |                                                          |\n| Sound Classification      |                      |                                                          |\n| Style Transfer            |                      |                                                          |\n\nmacOS GPU support is automatic. For Linux GPU support, see [LinuxGPU.md](LinuxGPU.md).\n\nBuilding From Source\n---------------------\n\nIf you want to build Turi Create from source, see [BUILD.md](BUILD.md).\n\nContributing\n------------\n\nPrior to contributing, please review [CONTRIBUTING.md](CONTRIBUTING.md) and do\nnot provide any contributions unless you agree with the terms and conditions\nset forth in [CONTRIBUTING.md](CONTRIBUTING.md).\n\nWe want the Turi Create community to be as welcoming and inclusive as possible, and have adopted a [Code of Conduct](CODE_OF_CONDUCT.md) that we expect all community members, including contributors, to read and observe.\n"
 },
 {
  "repo": "apple/swift-service-discovery",
  "language": "Swift",
  "readme_contents": "# Swift Service Discovery\n\nA Service Discovery API for Swift. \n\nService discovery is how services locate one another within a distributed system. This API library is designed to establish a standard that can be implemented by various service discovery backends such as DNS-based, key-value store like Zookeeper, etc. In other words, this library defines the API only, similar to [SwiftLog](https://github.com/apple/swift-log) and [SwiftMetrics](https://github.com/apple/swift-metrics); actual functionalities are provided by backend implementations. \n\nThis is the beginning of a community-driven open-source project actively seeking contributions, be it code, documentation, or ideas. Apart from contributing to SwiftServiceDiscovery itself, we need SwiftServiceDiscovery-compatible libraries which manage service registration and location information for querying. What SwiftServiceDiscovery provides today is covered in the [API docs][api-docs], but it will continue to evolve with community input.\n\n## Getting started\n\nIf you have a server-side Swift application and would like to locate other services within the same system for making HTTP requests or RPCs, then SwiftServiceDiscovery is the right library for the job. Below you will find all you need to know to get started.\n\n### Concepts\n\n- **Service Identity**: Each service must have a unique identity. `Service` denotes the identity type used in a backend implementation.\n- **Service Instance**: A service may have zero or more instances, each of which has an associated location (typically host-port). `Instance` denotes the service instance type used in a backend implementation. \n\n### Selecting a service discovery backend implementation (applications only)\n\n> Note: If you are building a library, you don't need to concern yourself with this section. It is the end users of your library (the applications) who will decide which service discovery backend to use. Libraries should never change the service discovery implementation as that is something owned by the application.\n\nSwiftServiceDiscovery only provides the service discovery API. As an application owner, you need to select a service discovery backend to make querying available.\n\nSelecting a backend is done by adding a dependency on the desired backend implementation and instantiating it at the beginning of the program. \n\nFor example, suppose you have chosen the hypothetical `DNSBasedServiceDiscovery` as the backend: \n\n```swift\n// 1) Import the service discovery backend package\nimport DNSBasedServiceDiscovery\n\n// 2) Create a concrete ServiceDiscovery object\nlet serviceDiscovery = DNSBasedServiceDiscovery()\n```\n\nAs the API has just launched, not many implementations exist yet. If you are interested in implementing one see the \"Implementing a service discovery backend\" section below explaining how to do so. List of existing SwiftServiceDiscovery API compatible libraries:\n\n- [tuplestream/swift-k8s-service-discovery](https://github.com/tuplestream/swift-k8s-service-discovery) - service discovery using the k8s APIs\n- Your library? Get in touch!\n\n### Obtaining a service's instances\n\nTo fetch the current list of instances (where `result` is `Result<[Instance], Error>`):\n\n```swift\nserviceDiscovery.lookup(service) { result in\n    ...\n}\n```\n\nTo fetch the current list of instances (where `result` is `Result<[Instance], Error>`) AND subscribe to future changes:\n\n```swift\nlet cancellationToken = serviceDiscovery.subscribe(\n    to: service, \n    onNext: { result in\n        // This closure gets invoked once at the beginning and subsequently each time a change occurs\n        ...\n    },\n    onComplete: { reason in\n        // This closure gets invoked when the subscription completes\n        ...\n    }\n)\n\n...\n\n// Cancel the `subscribe` request\ncancellationToken.cancel()\n```\n\n`subscribe` returns a `CancellationToken` that you can use to cancel the subscription later on. `onComplete` is a closure that\ngets invoked when the subscription ends (e.g., when the service discovery instance shuts down) or gets cancelled through the \n`CancellationToken`. `CompletionReason` can be used to distinguish what leads to the completion.\n\n### Combinators\n\nSwiftServiceDiscovery includes combinators for common requirements such as transforming and filtering instances. For example:\n\n```swift\n// Only include instances running on port 8080\nlet serviceDiscovery = InMemoryServiceDiscovery(configuration: configuration)\n    .filterInstance { [8080].contains($0.port) }\n```\n\n## Implementing a service discovery backend\n\n> Note: Unless you need to implement a custom service discovery backend, everything in this section is likely not relevant, so please feel free to skip.\n\n### Adding the dependency\n\nTo add a dependency on the API package, you need to declare it in your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/apple/swift-service-discovery.git\", from: \"0.1.0\"),\n```\n\nand to your library target, add \"ServiceDiscovery\" to your dependencies:\n\n```swift\n.target(\n    name: \"MyServiceDiscovery\", \n    dependencies: [\n        .product(name: \"ServiceDiscovery\", package: \"swift-service-discovery\"),\n    ]\n),\n```\n\nTo become a compatible service discovery backend that all SwiftServiceDiscovery consumers can use, you need to implement a type that conforms to the `ServiceDiscovery` protocol provided by SwiftServiceDiscovery. It includes two methods, `lookup` and `subscribe`.\n\n#### `lookup`\n\n```swift\n/// Performs a lookup for the given service's instances. The result will be sent to `callback`.\n///\n/// `defaultLookupTimeout` will be used to compute `deadline` in case one is not specified.\n///\n/// - Parameters:\n///   - service: The service to lookup\n///   - deadline: Lookup is considered to have timed out if it does not complete by this time\n///   - callback: The closure to receive lookup result\nfunc lookup(_ service: Service, deadline: DispatchTime?, callback: @escaping (Result<[Instance], Error>) -> Void)\n```\n\n`lookup` fetches the current list of instances for the given service and sends it to `callback`. If the service is unknown (e.g., registration is required but it has not been done for the service), then the result should be a `LookupError.unknownService` failure. \n\nThe backend implementation should impose a deadline on when the operation will complete. `deadline` should be respected if given, otherwise one should be computed using `defaultLookupTimeout`. \n\n#### `subscribe`\n\n```swift\n/// Subscribes to receive a service's instances whenever they change.\n///\n/// The service's current list of instances will be sent to `nextResultHandler` when this method is first called. Subsequently,\n/// `nextResultHandler` will only be invoked when the `service`'s instances change.\n///\n/// ### Threading\n///\n/// `nextResultHandler` and `completionHandler` may be invoked on arbitrary threads, as determined by implementation.\n///\n/// - Parameters:\n///   - service: The service to subscribe to\n///   - nextResultHandler: The closure to receive update result\n///   - completionHandler: The closure to invoke when the subscription completes (e.g., when the `ServiceDiscovery` instance exits, etc.),\n///                 including cancellation requested through `CancellationToken`.\n///\n/// -  Returns: A `CancellationToken` instance that can be used to cancel the subscription in the future.\nfunc subscribe(to service: Service, onNext nextResultHandler: @escaping (Result<[Instance], Error>) -> Void, onComplete completionHandler: @escaping (CompletionReason) -> Void) -> CancellationToken\n```\n\n`subscribe` \"pushes\" service instances to the `nextResultHandler`. The backend implementation is expected to call `nextResultHandler`:\n\n- When `subscribe` is first invoked, the caller should receive the current list of instances for the given service. This is essentially the `lookup` result.\n- Whenever the given service's list of instances changes. The backend implementation has full control over how and when its service records get updated, but it must notify `nextResultHandler` when the instances list becomes different from the previous result.\n\nA new `CancellationToken` must be created for each `subscribe` request. If the cancellation token's `isCancelled` is `true`, the subscription has been cancelled and the backend implementation should cease calling the corresponding `nextResultHandler`.\n\nThe backend implementation must also notify via `completionHandler` when the subscription ends for any reason (e.g., the service discovery instance is shutting down or cancellation is requested through `CancellationToken`), so that the subscriber can submit another `subscribe` request if needed.\n\n---\n\nDo not hesitate to get in touch, over on https://forums.swift.org/c/server.\n\n[api-docs]: https://apple.github.io/swift-service-discovery/\n"
 },
 {
  "repo": "apple/HomeKitADK",
  "language": "C",
  "readme_contents": "# HomeKit Accessory Development Kit (ADK)\n\nThe HomeKit ADK is used by silicon vendors and accessory manufacturers to build HomeKit compatible devices.\n\nThe HomeKit ADK implements key components of the HomeKit Accessory Protocol (HAP), which embodies the core principles Apple brings to smart home technology: security, privacy, and reliability.\n\nThe HomeKit Open Source ADK is an open-source version of the HomeKit Accessory Development Kit. It can be used by any developer to prototype non-commercial smart home accessories. For commercial accessories, accessory developers must continue to use the commercial version of the HomeKit ADK available through the MFi Program.\n\nGo to the [Apple Developer Site](https://developer.apple.com/homekit/) if you like to learn more about developing HomeKit-enabled accessories and apps.\n\n\n### Getting Started\nPlease go through [Getting Started Guide](./Documentation/getting_started.md#prerequisites) before using HomeKit ADK.\n\n### Documentation\nADK documentation is available as markdown files in [Documentation](./Documentation/) directory. However, a more user friendly\n`HTML` documentation can be generated from the markdown files by running the following command:\n\n```sh\nmake docs\n```\n\nThe command above will prompt to open the generated HTML webpage. After the command has finished, the webpage `./Documentation/api_docs/html/index.html` can also be opened in a browser.\n"
 },
 {
  "repo": "apple/GCGC",
  "language": "Jupyter Notebook",
  "readme_contents": "# GCGC :  Garbage Collection Graph Collector \n\n<img src=\"images/stw_pauses_log.jpg\" alt=\"Example scatter plot\" />\n<img src=\"images/heatmap.jpg\" alt=\"Example heat map plot\" />\n\n\n\nGCGC uses a Jupyter notebook interface to analyze GC log files.\n\n\nThere are 17 generated plots, which analyze latency, concurrent and STW events, heap information, allocation rates, frequencies of events, and event summaries, comparing any number of log files and external data sources. \nThe tool uses Jupyter notebook data visualization allows for easy customization of provided plots.\n\nThe analysis is built into a provided notebook, and generates plots and tables from collected GC information. The collected data for each log is parsed into a python pandas 'event log'. Then, using the event logs as a persistent database, the event information can be sorted, filtered, and grouped in both pre-set and customizable ways to display relevant trends and outliers.\n\n\n\nCurrently supports collectors in JDK11 & JDK 16.\n # Requirements\n\n- Python3 \n- The following Python3 packages\n    - numpy\n    - pandas\n    - matplotlib\n    - Jupyter notebook \n\nInstallation explained here: [docs/setup.md](./docs/setup.md)\n\n\n\n# How to run analysis\n\nFollow the instructions in [docs/how-to-run.md](./docs/how-to-run.md)\n\n--- \n\n\n## Known edge cases:\n\nNote: The following edge cases are known and not handled automatically:\n\n1) Shenandoah has two phases per garbage collection cycle reporting Heap allocation, will lead to two plotted heap occupancy metrics for each GC phase.\n2) ZGC in JDK16 Puts information in safepoints, does not automatically print these in log analysis as it currently stands. These safepoints have comparable metrics to pause times, but ZGC does not report them in the same fashion, so these must be manually enabled on plots.\n3) ZGC bytes reclaimed calculation (This may extend to Shenandoah) may be negative, if the rate of allocation exceeds the rate of gc collection. Information is correctly provided in logs, not properly analyzed here. Feature is being fixed in a later version, see issue #61\n4) Trying to plot a graph or plot with a returned matlpotlib.axes variable declared in another cell does not show up inline in Jupyter notebooks. \n\n--- \n\n## Generating a log file\n\nCreating a log file is quite easy, just add these logging flags to your appllication. \n\n    -Xlog:gc*:./filename.log\n\nMore detailed logging can be added, but the tool should be able to handle the gc log reported using the above Java runtime flags.\n\n\n"
 },
 {
  "repo": "apple/swift-community-hosted-continuous-integration",
  "language": null,
  "readme_contents": "# Swift Community-Hosted Continuous Integration\n\n[Swift Community-Hosted CI](https://ci-external.swift.org) is an extension of Swift CI that allows the community to add additional platforms. Community members can volunteer to host new platforms and they are responsible for maintaining the host nodes. The maintainer will provide a build preset which will be periodically built on the node. This allows the Swift community to see the impact of changes on a greater range of platforms.\n\n## Current List of Nodes\n   * Fedora 33\n   * Fedora Rawhide\n   * ARMv7 for Debian \"Stretch\"\n   * Ubuntu 16.04\n   * Ubuntu 18.04 for TensorFlow\n   * PPC64LE for Ubuntu 16.04\n   * AArch64 for Ubuntu 18.04\n   * AArch64 for Ubuntu 20.04\n   * Android\n   * macOS 10.13 for TensorFlow\n   * Debian 10\n   * wasm32 cross-compiled from Ubuntu 20.04\n\n\n## Add Nodes\n\n1. Create a pull request \n    * Add new JSON file under nodes directory -  nodes/<platform>_<os_version>.json\n\n```json\n{\n   \"contact\":{\n      \"name\":\"Full name\",\n      \"email\":\"Email address\",\n      \"company\":\"Company name (optional)\"\n   },\n   \"node\":{\n      \"platform\":\"Platform name\",\n      \"os_version\":\"Operating system version\"\n   },\n   \"jobs\":[\n      {\n         \"display_name\":\"Swift - <PLATFORM> (Tools <TOOLS_CONFIG>, Stdlib <STDLIB_CONFIG>) (<BRANCH>))\",\n         \"branch\":\"Swift branch\",\n         \"preset\":\"Build preset from utils/build-preset.ini\"\n      }\n   ]\n}\n```\n\nExample:\nFile name: macOS_10_13.json\n\n```json\n{\n   \"contact\":{\n      \"name\":\"Mishal Shah\",\n      \"email\":\"example@apple.com\",\n      \"company\":\"Apple Inc\"\n   },\n   \"node\":{\n      \"platform\":\"macOS\",\n      \"os_version\":\"macOS 10.13\"\n   },\n   \"jobs\":[\n      {\n         \"display_name\":\"Swift - macOS (Tools RA, Stdlib RD) (master)\",\n         \"branch\":\"master\",\n         \"preset\":\"buildbot_incremental,tools=RA,stdlib=RD,build\"\n      }\n   ]\n}\n```\n\n2. Verify preset builds on the server:\n    * Clone Swift \n      `git clone https://github.com/apple/swift.git`\n    * Clone all other repositories \n      `./swift/utils/update-checkout --scheme <BRANCH> --clone`\n    * Build + Test\n      `./swift/utils/build-script --preset=<PRESET>`\n\n3. Once the pull request has been merged, you will receive an email with a set of terms and conditions to agree to, and to exchange public key information for connecting to the CI system.\n\n4. You will be required to provide the following information in the email:\n    * Agreement to terms and conditions\n    * IP address\n    * Username\n\n## Maintaining Node\n\nThe node maintainer is responsible for updating the OS with security patches, keeping the host online, and installing any required packages to build the Swift compiler on the node. In the event that the host node becomes unaccessible or offline, the maintainer is responsible for bringing the node online within two weeks of being notified. Otherwise, an un-maintained node may be removed from the CI.\n"
 },
 {
  "repo": "apple/tensorflow_macos",
  "language": "Shell",
  "readme_contents": "## *You can now leverage Apple\u2019s tensorflow-metal PluggableDevice in TensorFlow v2.5 for accelerated training on Mac GPUs directly with Metal. Learn more [here](https://developer.apple.com/metal/tensorflow-plugin/).*\n\n\u2003\n\n\u2003\n\u2003\u2003\n\n\n## Mac-optimized TensorFlow and TensorFlow Addons\n\n### INTRODUCTION\n\nThis pre-release delivers hardware-accelerated TensorFlow and TensorFlow Addons for macOS 11.0+. Native hardware acceleration is supported on M1 Macs and Intel-based Macs through Apple\u2019s [ML Compute](https://developer.apple.com/documentation/mlcompute) framework.\n\n### CURRENT RELEASE\n\n- 0.1-alpha3\n\n### SUPPORTED VERSIONS\n\n- TensorFlow r2.4rc0\n- TensorFlow Addons 0.11.2\n\n### REQUIREMENTS\n\n- macOS 11.0+\n- Python 3.8 (required to be downloaded from [Xcode Command Line Tools](https://developer.apple.com/download/more/?=command%20line%20tools) for M1 Macs).\n\n### INSTALLATION\n\nAn archive containing Python packages and an installation script can be downloaded from the [releases](https://github.com/apple/tensorflow_macos/releases).\n\n- To quickly try this out, copy and paste the following into Terminal:\n\n  ```\n  % /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/apple/tensorflow_macos/master/scripts/download_and_install.sh)\"\n  ```\n\n  This will verify your system, ask you for confirmation, then create a [virtual environment](https://docs.python.org/3.8/tutorial/venv.html) with TensorFlow for macOS installed.\n\n- Alternatively, download the archive file from the [releases](https://github.com/apple/tensorflow_macos/releases). The archive contains an installation script, accelerated versions of TensorFlow, TensorFlow Addons, and needed dependencies.\n\n  ```\n  % curl -fLO https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha2/tensorflow_macos-${VERSION}.tar.gz\n  % tar xvzf tensorflow_macos-${VERSION}.tar\n  % cd tensorflow_macos\n  % ./install_venv.sh --prompt\n  ```\n\n#### Installation on Conda\n\nThis pre-release version supports installation and testing using the Python from Xcode Command Line Tools. See [#153](https://github.com/apple/tensorflow_macos/issues/153) for more information on installation in a Conda environment.\n\n#### Notes\n\nFor M1 Macs, the following packages are currently unavailable:\n\n- SciPy and dependent packages\n- Server/Client TensorBoard packages\n\nWhen installing pip packages in a virtual environment, you may need to specify `--target` as follows:\n\n```\n% pip install --upgrade -t \"${VIRTUAL_ENV}/lib/python3.8/site-packages/\" PACKAGE_NAME\n```\n\n### ISSUES AND FEEDBACK\n\nPlease submit feature requests or report issues via [GitHub Issues](https://github.com/apple/tensorflow_macos/issues).\n\n### ADDITIONAL INFORMATION\n\n#### Device Selection (Optional)\n\nIt is not necessary to make any changes to your existing TensorFlow scripts to use ML Compute as a backend for TensorFlow and TensorFlow Addons.\n\nThere is an optional `mlcompute.set_mlc_device(device_name='any')` API for ML Compute device selection. The default value for `device_name` is `'any'`, which means ML Compute will select the best available device on your system, including multiple GPUs on multi-GPU configurations. Other available options are `'cpu'` and `'gpu'`. Please note that in eager mode, ML Compute will use the CPU. For example, to choose the CPU device, you may do the following:\n\n  ```\n  # Import mlcompute module to use the optional set_mlc_device API for device selection with ML Compute.\n  from tensorflow.python.compiler.mlcompute import mlcompute\n\n  # Select CPU device.\n  mlcompute.set_mlc_device(device_name='cpu') # Available options are 'cpu', 'gpu', and 'any'.\n  ```\n\n#### Unsupported TensorFlow Features\n\nThe following TensorFlow features are currently not supported in this fork:\n\n- [tf.vectorized_map](https://www.tensorflow.org/api_docs/python/tf/vectorized_map)\n- [Higher-order gradients](https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients)\n- Jacobian-vector products (aka. [forwardprop](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator))\n\n\n#### Logs and Debugging\n\n##### Graph mode\n\nLogging provides more information about what happens when a TensorFlow model is optimized by ML Compute. Turn logging on by setting the environment variable `TF_MLC_LOGGING=1` when executing the model script. The following is the list of information that is logged in graph mode:\n\n- Device used by ML Compute.\n- Original TensorFlow graph without ML Compute.\n- TensorFlow graph after TensorFlow operations have been replaced with ML Compute.\n    - Look for MLCSubgraphOp nodes in this graph. Each of these nodes replaces a TensorFlow subgraph from the original graph, encapsulating all the operations in the subgraph. This, for example, can be used to determine which operations are being optimized by ML Compute.\n- Number of subgraphs using ML Compute and how many operations are included in each of these subgraphs.\n    - Having larger subgraphs that encapsulate big portions of the original graph usually results in better performance from ML Compute. Note that for training, there will usually be at least two MLCSubgraphOp nodes (representing forward and backward/gradient subgraphs).\n- TensorFlow subgraphs that correspond to each of the ML Compute graphs.\n\n\n##### Eager mode\n\nUnlike graph mode, logging in eager mode is controlled by `TF_CPP_MIN_VLOG_LEVEL`. The following is the list of information that is logged in eager mode:\n\n- The buffer pointer and shape of input/output tensor.\n- The key for associating the tensor\u2019s buffer to built the `MLCTraining` or `MLCInference` graph. This key is used to retrieve the graph and run a backward pass or an optimizer update.\n- The weight tensor format.\n- Caching statistics, such as insertions and deletions.\n\n\n##### Tips for debugging\n\n- Larger models being trained on the GPU may use more memory than is available, resulting in paging.  If this happens, try decreasing the batch size or the number of layers.\n- TensorFlow is multi-threaded, which means that different TensorFlow operations, such as` MLCSubgraphOp`, can execute concurrently. As a result, there may be overlapping logging information. To avoid this during the debugging process, set TensorFlow to execute operators sequentially by setting the number of threads to 1 (see [`tf.config.threading.set_inter_op_parallelism_threads`](https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads)).\n- In eager mode, you may disable the conversion of any operation to ML Compute by using `TF_DISABLE_MLC_EAGER=\u201c;Op1;Op2;...\u201d`. The gradient op may also need to be disabled by modifying  the file `$PYTHONHOME/site-packages/tensorflow/python/ops/_grad.py` (this avoids TensorFlow recompilation).\n- To initialize allocated memory with a specific value, use `TF_MLC_ALLOCATOR_INIT_VALUE=<init-value>`.\n- To disable ML Compute acceleration (e.g. for debugging or results verification), set the environment variable `TF_DISABLE_MLC=1`.\n\n"
 },
 {
  "repo": "apple/swift-nio-examples",
  "language": "Swift",
  "readme_contents": "## SwiftNIO Example Apps\n\nThe point of this repository is to be a collection of ready-to-use SwiftNIO example apps. The other `apple/swift-nio` repositories contain libraries which do sometimes contain example code but usually not whole applications.\n\nThe definition of app includes any sort of application, command line utilities, iOS Apps, macOS GUI applications or whatever you can think of.\n\n### Organisation\n\nEach example application should be fully contained in its own sub-directory together with a `README.md` explaining what the application demonstrates. Each application must be buildable through either `cd AppName && swift build` or `cd AppName && ./build.sh`.\n\nLike all other code in the SwiftNIO project, the license for all the code contained in this repository is the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0.html). See also `LICENSE.txt`.\n\n\n### Quality\n\nExample applications must go through pre-commit code review like all other code in the SwiftNIO project. It is however acceptable to publish demo applications that only work for a subset of the supported platforms if that limitation is clearly documented in the project's `README.md`.\n\n\n### NIO versions\n\nThe [`main`](https://github.com/apple/swift-nio-examples) branch contains the examples for the SwiftNIO 2 family. For the examples working with NIO 1, please use the [`nio-1`](https://github.com/apple/swift-nio-examples/tree/nio-1) branch.\n"
 },
 {
  "repo": "apple/learning-compressible-subspaces",
  "language": "Python",
  "readme_contents": "# Learning Compressible Subspaces\n\nThis is the official code release for our publication, [LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time](https://arxiv.org/abs/2110.04252). Our code is used to train and evaluate models that can be compressed in real-time after deployment, allowing for a fine-grained efficiency-accuracy trade-off.\n\nThis repository hosts code to train compressible subspaces for structured sparsity, unstructured sparsity, and quantization. We support three architectures: cPreResNet20, ResNet18, and VGG19. Training is performed on the CIFAR-10 and ImageNet datasets.\n\nDefault training configurations are provided in the `configs` folder. Note that they are automatically altered when different models and datasets are chosen through flags. See `training_params.py`. The following training parameter flags are available to all training regimes:\n\n- `--model`: Specifies the model to use. One of cpreresnet20, resnet18, or vgg19.\n- `--dataset`: Specifies the dataset to train on. One of cifar10 or imagenet.\n- `--imagenet_dir`: When using imagenet dataset, the directory to the dataset must be specified.\n- `--method`: Specifies the training method. For unstructured sparsity, one of target_topk, lcs_l, lcs_p. For structured sparsity, one of lec, ns, us, lcs_l, lcs_p. For quantized models, one of target_bit_width, lcs_l, lcs_p.\n- `--norm`: The normalization layers to use. One of IN (instance normalization), BN (batch normalization), or GN (group normalization).\n- `--epochs`: The number of epochs to train for.\n- `--learning_rate`: The optimizer learning rate.\n- `--batch_size`: Training and test batch sizes.\n- `--momentum`: The optimizer momentum.\n- `--weight_decay`: The L2 regularization weight.\n- `--warmup_budget`: The percentage of epochs to use for the training method warmup phase.\n- `--test_freq`: The number of training epochs to wait between test evaluation. Will also save models at this frequency.\n\nThe \"lcs_l\" training method refers to the \"LCS+L\" method in the paper. In this setting, we train a linear subspace where one end is optimized for efficiency, while the other end prioritizes accuracy. The \"lcs_p\" training method refers to the \"LCS+P\" in the paper and trains a degenerate subspace conditioned to perform at arbitrary sparsity rates in the unstructured and structured sparsity settings, or bit widths in the quantized setting.\n\n## Structured Sparsity\n\nIn the structured sparsity setting, we support five training methods:\n\n1. \"lcs_l\" -- This refers to the LCS+L method where one end of the linear subspace performs at high sparsity rates while the other performs at zero sparsity.\n2. \"lcs_p\" -- This refers to the LCS+P method where we train a degenerate subspace conditioned to perform at arbitrary sparsity rates.\n3. \"lec\" -- This refers to the method introduced in \"Learning Efficient Convolutional Networks through Network Slimming\" by Liu et al. (2017). We do not perform fine-tuning, as described in our paper.\n4. \"ns\" -- This refers to the method introduced in \"Slimmable Neural Networks\" by Yu et al. (2018). We use a single BatchNorm to allow for evaluation at arbitrary width factors, as decribed in our paper.\n5. \"us\" -- This refers to the method introduced in \"Universally Slimmable Networks and Improved Training Techniques\" by Yu & Huang (2019). We do not recalibrate BatchNorms (to facilitate on-device compression to arbitrary widths), as described in our paper.\n\nTraining a model in the structured sparsity setting can be accomplished by running the following command:\n\n> python train_structured.py\n\nBy default, the command above will train the cPreResNet20 architecture on CIFAR-10 using instance normalization layers with the LCS+L method. To specify the model, dataset, normalization, and training method, the flags `--model`, `--dataset`, `--norm`, `--method` can be used. The following command\n\n> python train_structured.py --model resnet18 --dataset imagenet --norm IN --method lcs_p --imagenet_dir <dir>\n\nwill train a ResNet18 point subspace (LCS+P) on ImageNet using instance normalization layers and the parameters from our paper.\n\nIn addition to the global flags above, the structured setting also has the following:\n\n- `--width_factors_list`: When training using the \"ns\" method, this sets the width factors at which the model will be trained.\n- `--width_factor_limits`: When training using the \"us\", \"lcs_l\", or \"lcs_p\" methods, sets the lower and upper width factor limits.\n- `--width_factor_samples`: When training using the \"us\", \"lcs_l\", or \"lcs_p\" methods, sets the number of samples to use for the sandwich rule. Two of these will be the samples from the width factor limits.\n- `--eval_width_factors`: Sets the width factors to evaluate the model for all training methods.\n\nThe command\n\n> python train_structured.py --model cpreresnet20 --dataset cifar10 --norm BN --method ns --width_factors_list 0.25,0.5,0.75,1.0\n\nwill train a cPreResNet20 architecture on CIFAR-10 via the NS method.\n\n## Unstructured Sparsity\n\nIn the unstructured sparsity setting, we support three training methods:\n\n1. \"lcs_l\" -- This refers to the LCS+L method where one end of the linear subspace performs at high sparsity rates while the other performs at zero sparsity.\n2. \"lcs_p\" -- This refers to the LCS+P method where we train a degenerate subspace conditioned to perform at arbitrary sparsity rates.\n3. \"target_topk\" -- this will train a network optimized to perform well at a specified TopK target.\n\nTraining a model in the unstructured sparsity setting can be accomplished by running the following command:\n\n> python train_unstructured.py\n\nBy default, the command above will train the cPreResNet20 architecture on CIFAR-10 using group normalization layers with the LCS+L method and the parameters used described in our paper. To specify the model, dataset, normalization, and training method, the flags `--model`, `--dataset`, `--norm`, `--method` can be used. The following command\n\n> python train_unstructured.py --model resnet18 --dataset imagenet --norm GN --method lcs_p --imagenet_dir <dir>\n\nwill train a ResNet18 point subspace (LCS+P) on ImageNet using group normalization layers again using the parameters from our paper.\n\nThe command\n\n> python train_unstructured.py --model resnet18 --dataset imagenet --method target_topk --topk 0.5 --imagenet_dir <dir>\n\nwill train a VGG19 architecture optimized to perform at a TopK value of 0.5.\n\nIn addition to the global flags above, the unstructured setting also has the following:\n\n- `--topk`: When training using the \"target_topk\" method, this sets the target TopK value.\n- `--eval_topk_grid`: Will evaluate the model at these TopK values.\n- '--topk_lower_bound': The lower bound TopK value (1-sparsity) to be used for training. For linear subspaces, one end of the line will be optimized for sparsity 1-topk_lower_bound which corresponds to the high accuracy endpoint. Note: If specified, eval_topk_grid must be specified as well.\n- '--topk_upper_bound': The upper bound TopK value (1-sparsity) to be used for training. For linear subspaces, one end of the line will be optimized for sparsity 1-topk_upper_bound which corresponds to the high efficiency endpoint. Note: If specified, eval_topk_grid must be specified as well.\n\nThe following command\n\n> python train_unstructured.py --model cpreresnet20 --dataset cifar10 --norm GN --method lcs_p --topk_lower_bound 0.005 --topk_upper_bound 0.05 --eval_topk_grid 0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05\n\nwill train a point subspace with high sparsity.\n\n## Quantization\n\nIn the quantized setting, we support three training methods:\n\n1. \"lcs_l\" -- This refers to the LCS+L method where one end of the linear subspace performs at a low bit width while the other performs at a hight  bit width.\n2. \"lcs_p\" -- This refers to the LCS+P method where we train a degenerate subspace conditioned to perform at arbitrary bit widths in a range.\n3. \"target_bit_width\" -- This trains a network optimized to perform at a specified bit width.\n\nTraining a model in the structured sparsity setting can be accomplished by running the following command:\n\n> python train_quantized.py\n\nBy default, the command above will train the cPreResNet20 architecture on CIFAR-10 using group normalization layers with the LCS+L method with a bit range [3,8]. To specify the model, dataset, normalization, and training method, the flags `--model`, `--dataset`, `--norm`, `--method` can be used. The following command\n\n> python train_quantized.py --model vgg19 --dataset imagenet --norm GN --method lcs_p --imagenet_dir <dir>\n\nwill train a ResNet18 point subspace (LCS+P) on ImageNet using group normalization layers.\n\nIn addition to the global flags above, the quantized setting also has the following:\n\n- `--bit_width`: When training using the \"target_bit_width\" method, this sets the target bit width.\n- `--eval_bit_widths`: Will evaluate models at these bit widths.\n- `--bit_width_limits`: This sets the upper and lower bit width bounds to use for training.\n\nThe following command\n\n> python train_quantized.py --model cpreresnet20 --dataset cifar10 --norm GN --method lcs_l --bit_width_limits 3,8 --eval_bit_widths 3,4,5,6,7,8\n\nwill train a linear subspace cPreResNet20 model with GN layers on the ImageNet dataset and will be optimized so that one end of the line performs at 3 bits, and the other at 8.\n"
 },
 {
  "repo": "apple/ccs-pykerberos",
  "language": "C",
  "readme_contents": "# PyKerberos Package\n\nThis Python package is a high-level wrapper for Kerberos (GSSAPI)\noperations.  The goal is to avoid having to build a module that wraps\nthe entire Kerberos.framework, and instead offer a limited set of\nfunctions that do what is needed for client/server Kerberos\nauthentication based on <http://www.ietf.org/rfc/rfc4559.txt>.\n\nMuch of the C-code here is adapted from Apache's mod_auth_kerb-5.0rc7.\n\n\n## Build\n\nIn this directory, run:\n\n```\npython setup.py build\n```\n\n## Testing\n\nTo run the tests in the tests folder, you must have a valid Kerberos setup on\nthe test machine. You can use the script .travis.sh as quick and easy way to\nsetup a Kerberos KDC and Apache web endpoint that can be used for the tests.\nOtherwise you can also run the following to run a self contained Docker\ncontainer\n\n```\ndocker run \\\n-v $(pwd):/app \\\n-w /app \\\n-e PYENV=2.7.13 \\\n-e KERBEROS_USERNAME=administrator \\\n-e KERBEROS_PASSWORD=Password01 \\\n-e KERBEROS_REALM=example.com \\\n-e KERBEROS_PORT=80 \\\nubuntu:16.04 \\\n/bin/bash .travis.sh\n```\n\nThe docker command needs to be run in the same directory as this library and\nyou can test it with different Python versions by changing the value of the\nPYENV environment value set in the command.\n\nPlease have a look at testing_notes.md for more information.\n\n\n## IMPORTANT\n\nThe checkPassword method provided by this library is meant only for testing purposes as it does\nnot offer any protection against possible KDC spoofing. That method should not be used in any\nproduction code.\n\n\n## Channel Bindings\n\nYou can use this library to authenticate with Channel Binding support. Channel\nBindings are tags that identify the particular data channel being used with the\nauthentication. You can use Channel bindings to offer more proof of a valid\nidentity. Some services like Microsoft's Extended Protection can enforce\nChannel Binding support on authorisation and you can use this library to meet\nthose requirements.\n\nMore details on Channel Bindings as set through the GSSAPI can be found here\n<https://docs.oracle.com/cd/E19455-01/806-3814/overview-52/index.html>. Using\nTLS as a example this is how you would add Channel Binding support to your\nauthentication mechanism. The following code snippet is based on RFC5929\n<https://tools.ietf.org/html/rfc5929> using the 'tls-server-endpoint-point'\ntype.\n\n```\nimport hashlib\n\ndef get_channel_bindings_application_data(socket):\n    # This is a highly simplified example, there are other use cases\n    # where you might need to use different hash types or get a socket\n    # object somehow.\n    server_certificate = socket.getpeercert(True)\n    certificate_hash = hashlib.sha256(server_certificate).hexdigest().upper()\n    certificate_digest = base64.b16decode(certificate_hash)\n    application_data = b'tls-server-end-point:%s' % certificate_digest\n\n    return application_data\n\ndef main():\n    # Code to setup a socket with the server\n    # A lot of code to setup the handshake and start the auth process\n    socket = getsocketsomehow()\n\n    # Connect to the host and start the auth process\n\n    # Build the channel bindings object\n    application_data = get_channel_bindings_application_data(socket)\n    channel_bindings = kerberos.channelBindings(application_data=application_data)\n\n    # More work to get responses from the server\n\n    result, context = kerberos.authGSSClientInit(kerb_spn, gssflags=gssflags, principal=principal)\n\n    # Pass through the channel_bindings object as created in the kerberos.channelBindings method\n    result = kerberos.authGSSClientStep(context, neg_resp_value, channel_bindings=channel_bindings)\n\n    # Repeat as necessary\n```\n\n## Python APIs\n\nSee kerberos.py.\n\n\n## Copyright and License\n\nCopyright (c) 2006-2021 Apple Inc.  All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0.  The\nApache License is a well-established open source license, enabling\ncollaborative open source software development.\n\nSee the \"LICENSE\" file for the full text of the license terms.\n"
 },
 {
  "repo": "apple/swift-internals",
  "language": "HTML",
  "readme_contents": "# Swift Internals\n\nThis is the source for the\n[Swift Internals](http://apple.github.io/swift-internals) website,\nwhich hosts internal documentation for the Swift compiler and\nstandard library, as well as the\n[Swift API Guidelines](https://swift.org/documentation/api-design-guidelines.html).,\nwhich are automatically regenerated from [this source](https://github.com/apple/swift-internals/tree/gh-pages/api-design-guidelines).\n\n## Local Testing and Development\n\n1. Have Ruby >= 2.0.0 installed.\n2. `gem install bundler`\u2014this command must normally be run with\n   sudo/root/admin privileges.\n3. `bundle install`\u2014run this command as a regular, unprivileged user.\n4. `LC_ALL=en_us.UTF-8 bundle exec jekyll serve --baseurl /swift-internals`\n5. Visit [http://localhost:4000/swift-internals/](http://localhost:4000/swift-internals/).\n6. Make edits to the source, refresh your browser, lather, rinse, repeat.\n\nNotes: \n\n* Changes to `_config.yml` require restarting the local server (step 4\n  above).\n* If you make changes to `_config.yml` specifically in order to serve\n  these pages from an address other than\n  http://apple.github.io/swift-internals, please make sure those\n  changes are not included in any pull requests, so we don't\n  inadvertently break the main site.\n"
 },
 {
  "repo": "apple/example-package-fisheryates",
  "language": "Swift",
  "readme_contents": "This example package will be cloned and built as a dependency if you build the `Dealer` example:\n\n    git clone https://github.com/apple/example-package-dealer.git\n    cd example-package-dealer\n    swift run Dealer\n\n# License\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n    1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n    2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n    3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n    4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n    5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n    6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n    7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n    8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n    9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n    END OF TERMS AND CONDITIONS\n\n    APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n    Copyright [yyyy] [name of copyright owner]\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n   \n   \n## Runtime Library Exception to the Apache 2.0 License: ##\n\n\n    As an exception, if you use this Software to compile your source code and\n    portions of this Software are embedded into the binary product as a result,\n    you may redistribute such product without providing attribution as would\n    otherwise be required by Sections 4(a), 4(b) and 4(d) of the License.\n"
 },
 {
  "repo": "apple/ml-shuffling-amplification",
  "language": "Python",
  "readme_contents": "## Hiding Among the Clones: Privacy Amplification by Shuffling\n\nThis software project accompanies the research paper, Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling (https://arxiv.org/abs/2012.12803).\n\nRecent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and Thakurta [EFMRTT19] demonstrates that random shuffling amplifies differential privacy guarantees of locally randomized data. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17] and has lead to significant interest in the shuffle model of privacy [CSUZZ19,EFMRTT19].  We show that random shuffling of n data records that are input to \u03b5_0-differentially private local randomizers results in an (O((1\u2212e^(\u03b5_0))sqrt(e^(\u03b5_0)log(1/\u03b4)/n)),\u03b4)-differentially private algorithm. This significantly improves over previous work and achieves the asymptotically optimal dependence in \u03b5_0. Our result is based on a new approach that is simpler than previous work and extends to approximate differential privacy with nearly the same guarantees. Our work also yields an empirical method to derive tighter bounds the resulting \u03b5 and we show that it gets to within a small constant factor of the optimal bound. As a direct corollary of our analysis, we derive a simple and asymptotically optimal algorithm for discrete distribution estimation in the shuffle model of privacy. We also observe that our result implies the first asymptotically optimal privacy analysis of noisy stochastic gradient descent that applies to sampling without replacement.\n\nThis repository contains a python implementation of our main amplification bound from shuffling purely differentially private local randomizers. \n\n## Documentation\n\nAppendix E of https://arxiv.org/abs/2012.12803 contains details of our implementation. Some function names differ, \"B\" in appendix is \"onestep\" in code and \"M_1\" in appendix is \"deltacomp\" in code.\n\n## Getting Started \n\nexample.py contains an example showing how to compute amplification bounds with this code. Given the parameters set as default in example.py, the output of this script should be the following two lines:\n\nShuffling 100000 4 -DP local randomizers results is (eps,  1e-06 )-DP in the shuffle model for eps between 0.1675385583317841 and 0.172790550755978\n\nAccording to our closed form analysis, shuffling 100000 4 -DP local randomizers results is (eps,  1e-06 )-DP in the shuffle model where eps is at most  0.5378040242374512"
 },
 {
  "repo": "apple/ml-gsn",
  "language": "Python",
  "readme_contents": "## Generative Scene Networks (GSN) - Official PyTorch Implementation\n**Unconstrained Scene Generation with Locally Conditioned Radiance Fields, ICCV 2021**<br>\nTerrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind<br>\n\n### [Project Page](https://apple.github.io/ml-gsn/) | [Paper](https://arxiv.org/abs/2104.00670) | [Data](#datasets)\n\n## Requirements\nThis code was tested with Python 3.6 and CUDA 11.1.1, and uses Pytorch Lightning. A suitable conda environment named `gsn` can be created and activated with:\n```\nconda env create -f environment.yaml python=3.6\nconda activate gsn\n```\nIf you do not already have CUDA installed, you can do so with:\n```\nwget https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run\nsh cuda_11.1.1_455.32.00_linux.run --toolkit --silent --override\nrm cuda_11.1.1_455.32.00_linux.run\n```\nCustom CUDA kernels may not work with older versions of CUDA. This code will revert to a native PyTorch implementation if the CUDA version is incompatible, although runtime may be ~25% slower.\n\n## Datasets\nWe provide camera trajectories for two datasets that we used to trained our model: Vizdoom and Replica. These datasets are composed of different sequences with corresponding rgb+depth frames and camera parameters (extrinsiscs and intrinsics).\n\nDataset | Size | Download Link\n--- | :---: | :---:\nVizdoom | 2.4 GB | [download](<https://docs-assets.developer.apple.com/ml-research/datasets/gsn/vizdoom.zip>)\nReplica | 11.0 GB | [download](<https://docs-assets.developer.apple.com/ml-research/datasets/gsn/replica.zip>)\n\nDatasets can be downloaded by running the following scripts:  \n**VizDoom**<br>\n```\npython scripts/download_vizdoom.py\n```\n**Replica**<br>\n```\npython scripts/download_replica.py\n```\n\n## Interactive exploration demo\nWe provide a [Jupyter notebook](notebooks/walkthrough_demo.ipynb) that allows for interactive exploration of scenes generated from a pre-trained model. Use the WASD keys to freely navigate through the scene! Once you are done, the notebook interpolates the camera path to render a continuous trajectory. Note: You need to download the Replica dataset before via this [script](scripts/download_replica.py) before running the notebook.\n\nExplore scene with WASD to set keypoints | Rendered trajectory\n:---: | :---:\n<img src=\"./assets/keyframes.gif\" width=256px> | <img src=\"./assets/camera_trajectory.gif\" width=256px>\n\n## Training models\nDownload the training dataset (if you have not done so already) and begin training with the following commands:  \n**VizDoom**<br>\n```\nbash scripts/launch_gsn_vizdoom_64x64.sh\n```\n\n**Replica**<br>\n```\nbash scripts/launch_gsn_replica_64x64.sh\n```\n\nTraining takes about 3 days to reach 500k iterations with a batch size of 32 on two A100 GPUs.\n\n## Pre-trained models\nWe provide pre-trained models for GSN to replicate our experimental results. In particular, we provide models for the Vizdoom dataset trained at 64x64 resolution, and for Replica dataset trained at 64x64 and 128x128. Note that either model can be rendered at higher resolutions than native resolution used durinig training by changing the intrinsic camera parameters during inference.\n\nDataset | Train Resolution | FID (5k) | Download Link\n--- | :---: | :---: | :---: \nVizdoom | 64x64 | 35.9 | [download](<https://docs-assets.developer.apple.com/ml-research/models/gsn/vizdoom_64x64.ckpt>)\nReplica | 64x64 | 41.5 | [download](<https://docs-assets.developer.apple.com/ml-research/models/gsn/replica_64x64.ckpt>)\nReplica | 128x128 | 43.4 | [download](<https://docs-assets.developer.apple.com/ml-research/models/gsn/replica_128x128.ckpt>)\n\n### Evaluating pre-trained models\nThe evaluation script requires the [training set](#datasets) to run. Download it first if you have not yet done so.\nDownload and run evaluation for pre-trained models with the following commands:  \n**VizDoom**<br>\n```\nbash scripts/eval_vizdoom_64x_64_pretrained.sh\n```\n**Replica**<br>\n```\nbash scripts/eval_replica_64x_64_pretrained.sh\n```\nRunning evaluation will compute the FID score and save sample sheets in the log directory.\n\n## Citation\n```\n@article{devries2021unconstrained,\n    title={Unconstrained Scene Generation with Locally Conditioned Radiance Fields},\n    author={Terrance DeVries and Miguel Angel Bautista and \n            Nitish Srivastava and Graham W. Taylor and \n            Joshua M. Susskind},\n    journal={arXiv},\n    year={2021}\n}\n```\n## License\nThis sample code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/ccs-caldavtester",
  "language": "Python",
  "readme_contents": "CALDAVTESTER\n============\n\n# INTRODUCTION\n\nCalDAVTester is a Python app that will run a series of scripted tests\nagainst a CalDAV server and verify the output, and optionally measure\nthe time taken to complete one or more repeated requests. The tests are\ndefined by XML files and ancillary HTTP request body files. A number of\ndifferent verification options are provided.\n\nMany tests are included in this package.\n\nCalDAVTester can be extended to run tests against any type of HTTP server\nprotocol by simply defining a new set of XML files.\n\n# INSTALL\nGet CalDAVTester from github, create a virtualenv to run it in and install\ninto the virtualenv:\n\n\tgit clone https://github.com/apple/ccs-caldavtester.git\n\tcd ccs-caldevtester\n\tvirtualenv venv\n\tsource venv/bin/activate\n\tpip install -r requirements.txt\n\n# COMMAND LINE OPTIONS\nCalDAVTester is run via the `testcaldav.py` script:\n\n\ttestcaldav.py\n\t\t[-s filename]\n\t\t[-x dirpath]\n\t\t[--basedir dirpath]\n\t\t[--ssl]\n\t\t[--all]\n\t\t[--random]\n\t\t[--random-seed SEED]\n\t\t[--stop]\n\t\t[--print-details-onfail]\n\t\t[--always-print-request]\n\t\t[--always-print-response]\n\t\t[--exclude filename]\n\t\t[--observer OBSERVER]\n\t\tfile1 file2 ...\n\n\t-s : filename specifies the file to use for server information\n\t(default is 'serverinfo.xml').\n\t\n\t-x : directory path for test scripts\n\t(default is 'scripts/tests').\n\t\n\t--basedir : directory path for serverinfo.xml, test/ and data/,\n\t\toverrides -s and -x values\n\t\n\t-p : filename specifies the file to use to populate the server with\n\tdata. Server data population only occurs when this option is\n\tpresent.\n\t\n\t-d : in conjunction with -p, if present specifies that the populated\n\tdata be removed after all tests have completed.\n\t\n\t--ssl : run tests using SSL/https connections to the server.\n\t\n\t--all : execute all tests found in the working directory. Each .xml\n\tfile in that directory is examined and those corresponding to the\n\tcaldavtest.dtd are executed.\n\t\n\t--random : randomize the order in which the tests are run.\n\t\n\t--random-seed SEED : a specific random seed to use.\n\t\n\t--stop : stop running all tests after one test file fails.\n\t\n\t--print-details-onfail : print HTTP request/response when a test fails.\n\t\n\t--always-print-request : always print HTTP request.\n\t\n\t--always-print-response : always print HTTP response.\n\t\n\t--exclude FILE : when running with --all, exclude the file from the test run. \n\t\n\t--observer OBSEREVER : specify one or more times to change which classes are\n\tused to process log and trace messages during a test. The OBSERVER name must\n\tbe the name of a module in the observers package. The default observer is the\n\t\"log\" observer. Available observers are:\n\t\n\t\t\"log\" - produces an output similar to Python unit tests.\n\t\t\"trace\" - produces an output similar to the original output format.\n\t\t\"loadfiles\" - prints each test file as it is loaded.\n\t\t\"jsondump\" - prints a JSON representation of the test results.\n\t\n\tfile1 file2 ...: a list of test files to execute tests from.\n\n# QUICKSTART\n\nEdit the serverinfo.xml file to run the test against your server setup.\n\nPrior to running, make sure you are in the virtualenv:\n\n\tsource venv/bin/activate\n\nRun `testcaldav.py --all` on the command line to run the tests. The app\nwill print its progress through the tests.\n\n# EXECUTION PROCESS\n\n1. Read in XML config.\n2. Execute &lt;start&gt; requests.\n3. For each &lt;test-suite&gt;, run each &lt;test&gt; the specified number of times,\n   executing each &lt;request&gt; in the test and verifying them.\n4. Delete any resources from requests marked with 'end-delete'.\n5. Execute &lt;end&gt; requests.\n\n# XML SCRIPT FILES\n\n## serverinfo.dtd\n\n\tDefines the XML DTD for the server information XML file:\n\t\n\tELEMENT <host>\n\t\thost name for server to test.\n\t\n\tELEMENT <nonsslport>\n\t\tport to use to connect to server (non-SSL).\n\t\n\tELEMENT <sslport>\n\t\tport to use to connect to server (SSL).\n\t\n\tELEMENT <authtype>\n\t\tHTTP authentication method to use.\n\t\n\tELEMENT <certdir>\n\t\tBase directory for TLS client certs.\n\t\n\tELEMENT <waitcount>\n\t\tFor requests that wait, defines how many iterations to wait for\n\t\t[Default: 120].\n\t\n\tELEMENT <waitdelay>\n\t\tFor requests that wait, defines how long between iterations to\n\t\twait for in seconds [Default: 0.25].\n\t\n\tELEMENT <waitsuccess>\n\t\tFor requests with the wait-for-success options, defines how many\n\t\tseconds to wait [Default: 10].\n\t\n\tELEMENT <features>\n\t\tlist of features for the server under test.\n\t\n\t\tELEMENT <feature>\n\t\t\tspecific feature supported by the server under test,\n\t\t\tused to do conditional testing.\n\t\n\tELEMENT <substitutions>\n\t\tused to encapsulate all variable substitutions.\n\t\n\t\tELEMENT <substitution>\n\t\t\ta variable substitution - the repeat attribute can\n\t\t\tbe used to repeat the substitution a set number of\n\t\t\ttimes whilst generating different substitutions.\n\t\n\t\t\tELEMENT <key>\n\t\t\t\tthe substitution key (usually '$xxx:').\n\t\n\t\t\tELEMENT <value>\n\t\t\t\tthe substitution value.\n\t\n\t\tELEMENT <repeat>\n\t\t\tallow repeating substitutions for the specified count.\n\n\n## caldavtest.dtd:\n\n\tDefines the XML DTD for test script files:\n\t\n\tATTRIBUTE ignore-all\n\t\tused on the top-level XML element to indicate whether this test\n\t\tis run when the --all command line switch for testcaldav.py is\n\t\tused. When set to 'no' the test is not run unless the file is\n\t\texplicitly specified on the command line.\n\t\n\tELEMENT <description>\n\t\ta description for this test script.\n\t\n\tELEMENT <require-feature>\n\t\tset of features.\n\t\n\t\tELEMENT <feature>\n\t\t\tfeature that server must support for this entire test\n\t\t\tscript to run.\n\t\n\tELEMENT <exclude-feature>\n\t\tset of features.\n\t\n\t\tELEMENT <feature>\n\t\t\tfeature that server must not support for this entire test\n\t\t\tscript to run.\n\t\n\tELEMENT <start>\n\t\tdefines a series of requests that are executed before testing\n\t\tstarts. This can be used to initialize a set of calendar\n\t\tresources on which tests can be run.\n\t\n\tELEMENT <end>\n\t\tdefines a series of requests that are executed after testing is\n\t\tcomplete. This can be used to clean-up the server after testing.\n\t\tNote that there are special mechanisms in place to allow\n\t\tresources created during testing to be automatically deleted\n\t\tafter testing, so there is no need to explicitly delete those\n\t\tresources here.\n\t\n\tELEMENT <test-suite>\n\t\tdefines a group of tests to be run. The suite is given a name\n\t\tand has an 'ignore' attribute that can be used to disable it.\n\t\n\t\tATTRIBUTE name\n\t\t\tname/description of test-suite.\n\t\tATTRIBUTE ignore\n\t\t\tif set to 'yes' then the entire test-suite will be skipped.\n\t\tATTRIBUTE only\n\t\t\tif set to 'yes' then all other test-suites (except others with\n\t\t\tthe same attribute value set) will be skipped.\n\t\n\t\tELEMENT <require-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must support for this test\n\t\t\t\tsuite to run.\n\t\n\t\tELEMENT <exclude-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must not support for this test\n\t\t\t\tsuite to run.\n\t\n\tELEMENT <test>\n\t\tdefines a single test within a test suite. A test has a name,\n\t\tdescription and one or more requests associated with it. There\n\t\tis also an 'ignore' attribute to disable the test. Tests can be\n\t\texecuted multiple times by setting the 'count' attribute to a\n\t\tvalue greater than 1. Timing information about the test can be\n\t\tprinted out by setting the 'stats' attribute to 'yes'.\n\t\n\t\tATTRIBUTE name\n\t\t\tname of test.\n\t\tATTRIBUTE count\n\t\t\tnumber of times to run the test. This allows tests to be\n\t\t\teasily repeated.\n\t\tATTRIBUTE stats\n\t\t\tif set to 'yes' then timing information for the test will be\n\t\t\tprinted.\n\t\tATTRIBUTE ignore\n\t\t\tif set to 'yes' then the entire test will be skipped.\n\t\n\t\tELEMENT <require-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must support for this test\n\t\t\t\tto run.\n\t\n\t\tELEMENT <exclude-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must not support for this test\n\t\t\t\tto run.\n\t\n\tELEMENT <description>\n\t\tdetailed description of the test.\n\t\n\tELEMENT <pause>\n\t\thalt tests and wait for user input. Useful for stopping tests to set a\n\t\tbreak point or examine server state, and then continue on.\n\t\n\tELEMENT <request>\n\t\tdefines an HTTP request to send to the server. Attributes on the\n\t\telement are:\n\t\n\t\tATTRIBUTE auth\n\t\t\tif 'yes', HTTP Basic authentication is done in the request.\n\t\tATTRIBUTE user\n\t\t\tif provided this value is used as the user id for HTTP Basic\n\t\t\tauthentication instead of the one in the serverinfo\n\t\t\tfile.\n\t\tATTRIBUTE pswd\n\t\t\tif provided this value is used as the password for HTTP\n\t\t\tBasic authentication instead of the one in the serverinfo\n\t\t\tfile.\n\t\tATTRIBUTE cert\n\t\t\tif provided this value is used as the file name for a TLS\n\t\t\tclient certificate to be used with the request.\n\t\tATTRIBUTE end-delete\n\t\t\tif set to 'yes', then the resource targeted by the request\n\t\t\tis deleted after testing is complete, but before the\n\t\t\trequests in the <end> element are run. This allows for quick\n\t\t\tclean-up of resources created during testing.\n\t\tATTRIBUTE print-response\n\t\t\tif set to 'yes' then the HTTP response (header and body) is\n\t\t\tprinted along with test results.\n\t\tATTRIBUTE wait-for-success\n\t\t\tif set to 'yes' then the HTTP request will repeat over and over\n\t\t\tfor a set amount of time waiting for the verifiers to pass. If\n\t\t\ttime expires without success then the overall request fails. The\n\t\t\tlength of time is controlled by the <waittime> element in the\n\t\t\tserverinfo file (defaults to 10 seconds).\n\t\n\t\tELEMENT <require-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must support for this request\n\t\t\t\tto run.\n\t\n\t\tELEMENT <exclude-feature>\n\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must not support for this request\n\t\t\t\tto run.\n\t\n\t\tELEMENT <method>\n\t\t\tthe HTTP method for this request. There are some 'special' methods that do some useful 'compound' operations:\n\t\t\t\t1) DELETEALL - deletes all resources within the collections specified by the <ruri> elements.\n\t\t\t\t2) DELAY - pause for the number of seconds specified by the <ruri> element.\n\t\t\t\t3) GETNEW - get the data from the newest resource in the collection specified by the <ruri> element and put its URI\n\t\t\t\t\t\t    into the $ variable for later use in an <ruri> element.\n\t\t\t\t4) WAITCOUNT N - wait until at least a certain number of resources \"N\" appear in a collection.\n\t\t\t\t5) WAITDELETEALL - wait until at least a certain number of resources appear in a collection, then delete all child\n\t\t\t\t\t\t\t\t   resources in that collection.\n\t\t\t\t6) GETCHANGED - the tool tracks the Etags on resources retrieved via GET. This special method will poll the specified\n\t\t\t\t\t\t\t\tresource until the Etag returned in the response is different from the one found in the most recent\n\t\t\t\t\t\t\t\ttest.  \n\t\t\t\t6) GETOTHER - the tool finds the newest sibling resource to the one specified in the <ruri> element.  \n\t\t\t\t6) GETCONTAINS XXX - the tool finds the child resource whose content contains the supplied text \"XXX\".  \n\t\n\t\tELEMENT <ruri>\n\t\t\tthe URI of the request. Multiple <ruri>'s are allowed with DELETEALL only.\n\t\t\tThe characters \"**\" may be used to cause a random uuid to be inserted where\n\t\t\tthose two characters appear. The characters \"##\" may be used to insert the\n\t\t\tcurrent test count iteration where those two characters occur.\n\t\n\t\tELEMENT <header>\n\t\t\tcan be used to specify additional headers in the request.\n\t\n\t\t\tELEMENT <name>\n\t\t\t\tthe header name.\n\t\n\t\t\tELEMENT <value>\n\t\t\t\tthe header value.\n\t\n\t\tELEMENT <data>\n\t\t\tused to specify the source and nature of data used in the\n\t\t\trequest body, if there is one.\n\t\t\t\n\t\t\tATTRIBUTE substitutions\n\t\t\t\tif set to 'yes' then '$xxx:' style variable substitutions\n\t\t\t\twill be performed on the data before it is sent in the request.\n\t\t\tATTRIBUTE generate\n\t\t\t\tif set to 'yes' then a basic calendar data \"fuzzing\" is done to\n\t\t\t\tthe source data to make it unique and up to date.\n\t\n\t\t\tELEMENT <content-type>\n\t\t\t\tthe MIME content type for the request body.\n\t\n\t\t\tELEMENT <filepath>\n\t\t\t\tthe relative path for the file containing the request body\n\t\t\t\tdata.\n\t\n\t\t\tELEMENT <generator>\n\t\t\t\ta callback and set of arguments used to generate the data.\n\t\t\t\t\n\t\t\t\tELEMENT <callback>\n\t\t\t\t\tthe name of the generator method to execute.\n\t\t\n\t\t\t\tELEMENT <arg>\n\t\t\t\t\targuments sent to the generator method.\n\t\t\n\t\t\t\t\tELEMENT <name>\n\t\t\t\t\t\tthe name of the argument.\n\t\t\n\t\t\t\t\tELEMENT <value>\n\t\t\t\t\t\tvalues for the argument.\n\t\n\t\t\tELEMENT <substitute>\n\t\t\t\ta set of substitution variables to use on this data only.\n\t\t\t\n\t\t\t\tELEMENT <name>\n\t\t\t\t\tthe variable name.\n\t\t\n\t\t\t\tELEMENT <value>\n\t\t\t\t\tthe variable value.\n\t\n\t\tELEMENT <verify>\n\t\t\tif present, used to specify a procedures for verifying that the\n\t\t\trequest executed as expected.\n\t\n\t\t\tELEMENT <require-feature>\n\t\t\t\tset of features.\n\t\t\n\t\t\t\tELEMENT <feature>\n\t\t\t\t\tfeature that server must support for this verification\n\t\t\t\t\tto be checked.\n\t\n\t\t\tELEMENT <exclude-feature>\n\t\t\t\tset of features.\n\t\n\t\t\tELEMENT <feature>\n\t\t\t\tfeature that server must not support for this verification\n\t\t\t\tto be checked.\n\t\t\t\n\t\t\tELEMENT <callback>\n\t\t\t\tthe name of the verification method to execute.\n\t\n\t\t\tELEMENT <arg>\n\t\t\t\targuments sent to the verification method.\n\t\n\t\t\t\tELEMENT <name>\n\t\t\t\t\tthe name of the argument.\n\t\n\t\t\t\tELEMENT <value>\n\t\t\t\t\tvalues for the argument.\n\t\n        ELEMENT <graburi>\n            if present, this stores the value of the actual request URI\n            used in a named variable which can be used in subsequent requests.\n            Useful for capturing URIs when the GETNEW method is used.\n            \n        ELEMENT <grabcount>\n            if present, this stores the number of child responses in a\n            {DAV:}multistatus response into the named variable which\n            can be used in subsequent requests. This is useful for\n            capturing the current count so that a change in the count\n            can be tested for later.\n            \n\t\tELEMENT <grabheader>\n\t\t\tif present, this stores the value of the specified header\n\t\t\treturned in the response in a named variable which can be used\n\t\t\tin subsequent requests.\n\t\t\t\n\t\tELEMENT <grabproperty>\n\t\t\tif present, this stores the value of the specified property\n\t\t\treturned in a PROPFIND response in a named variable which can\n\t\t\tbe used in subsequent requests.\n\t\t\t\n        ELEMENT <grabelement>\n            if present, this stores the text representation of an XML\n            element extracted from the response body in a named variable\n            which can be used in subsequent requests.\n            \n        ELEMENT <grabjson>\n            if present, this stores the text representation of a JSON\n            object extracted from the response body in a named variable\n            which can be used in subsequent requests.\n            \n\t\tELEMENT <grabcalproperty>\n\t\t\tif present, this stores a calendar property value in a named\n\t\t\tvariable which can be used in subsequent request. The syntax for\n\t\t\t<name> element is component/propname (e.g. \"VEVENT/SUMMARY\").\n\t\n\t\tELEMENT <grabcalparameter>\n\t\t\tif present, this stores a calendar parameter value in a named\n\t\t\tvariable which can be used in subsequent request. The syntax for\n\t\t\t<name> element is component/propname/paramname$propvalue where\n\t\t\tthe option $propvalue allows a specific property to be selected \n\t\t\t(e.g. \"VEVENT/DTSTART/TZID\", or\n\t\t\t\"VEVENT/ATTENDEE/PARTSTAT$mailto:user01@example.com\").\n\n\n# VERIFICATION Methods\n\n## acltems\nPerforms a check of multi-status response body and checks to see\nwhether the specified privileges are granted or denied on each\nresource in the response for the current user (i.e. tests the\nDAV:current-user-privilege-set).\n\n\tArgument: 'granted'\n\t\tA set of privileges that must be granted.\n\t\n\tArgument: 'denied'\n\t\tA set of privileges that must be denied denied.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>multistatusitems</callback>\n\t\t<arg>\n\t\t\t<name>granted</name>\n\t\t\t<value>DAV:read</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>denied</name>\n\t\t\t<value>DAV:write</value>\n\t\t\t<value>DAV:write-acl</value>\n\t\t</arg>\n\t</verify>\n\t\n## calandarDataMatch\nSimilar to data match but tries to \"normalize\" the calendar data so that e.g., different\nordering of properties is not significant.\n\n\tArgument: 'filepath'\n\t\tThe file path to a file containing data to match the response body to.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>dataMatch</callback>\n\t\t<arg>\n\t\t\t<name>filepath</name>\n\t\t\t<value>resources/put.ics</value>\n\t\t</arg>\n\t</verify>\n\t\n## dataMatch\nPerforms a check of response body and matches it against the data in the specified file.\n\n\tArgument: 'filepath'\n\t\tThe file path to a file containing data to match the response body to.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>dataMatch</callback>\n\t\t<arg>\n\t\t\t<name>filepath</name>\n\t\t\t<value>resources/put.ics</value>\n\t\t</arg>\n\t</verify>\n\t\n## dataString\nPerforms a check of response body tries to find occurrences of the specified strings or the\nabsence of specified strings.\n\n    Argument: 'equals'\n        One or more strings that must match exactly in the data (case-sensitive).\n    \n    Argument: 'contains'\n        One or more strings that must be contained in the data (case-sensitive).\n    \n\tArgument: 'notcontains'\n\t\tOne or more strings that must not be contained in the data (case-sensitive).\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>dataString</callback>\n\t\t<arg>\n\t\t\t<name>contains</name>\n\t\t\t<value>BEGIN:VEVENT</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>notcontains</name>\n\t\t\t<value>BEGIN:VTODO</value>\n\t\t</arg>\n\t</verify>\n\t\n## freeBusy\nPerforms a check of the response body to verify it contains an\niCalendar VFREEBUSY object with the specified busy periods and\ntypes.\n\n\tArgument: 'busy'\n\t\tA set of iCalendar PERIOD values for FBTYPE=BUSY periods\n\t\texpected in the response.\n\t\n\tArgument: 'tentative'\n\t\tA set of iCalendar PERIOD values for FBTYPE=BUSY-TENTATIVE\n\t\tperiods expected in the response.\n\t\n\tArgument: 'unavailable'\n\t\tA set of iCalendar PERIOD values for FBTYPE=BUSY-UNAVAILABLE\n\t\tperiods expected in the response.\n\t\n\tArgument: 'duration'\n\t\tIf present the period values being checked use duration rather then\n\t\tend time.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>freeBusy</callback>\n\t\t<arg>\n\t\t\t<name>busy</name>\n\t\t\t<value>20060107T010000Z/20060107T020000Z</value>\n\t\t\t<value>20060107T150000Z/20060107T163000Z</value>\n\t\t\t<value>20060108T150000Z/20060108T180000Z</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>unavailable</name>\n\t\t\t<value>20060108T130000Z/20060108T150000Z</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>tentative</name>\n\t\t\t<value>20060108T160000Z/20060108T170000Z</value>\n\t\t\t<value>20060108T210000Z/20060108T213000Z</value>\n\t\t</arg>\n\t</verify>\n\n## header\nPerforms a check of response header and value. This can be used to\ntest for the presence or absence of a header, or the presence of a\nheader with a specific value.\n\n\tArgument: 'header'\n\t\tThis can be specified in one of three forms:\n\t\t\n\t\t\t'headername' - will test for the presence of the response\n\t\t\theader named 'header name'.\n\t\n\t\t\t'headername$value' - will test for the presence of the\n\t\t\tresponse header named 'headername' and also check that its\n\t\t\tvalue matches 'value'.\n\t\n\t\t\t'!headername' - will test for the absence of a header named\n\t\t\t'headername' in the response header.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>header</callback>\n\t\t<arg>\n\t\t\t<name>header</name>\n\t\t\t<value>Content-type$text/plain</value>\n\t\t</arg>\n\t</verify>\n\n## jcalDataMatch\nLike calendarDataMatch except that comparison is done using jCal data.\n\n## jsonPointerMatch\nCompares the response with a JSON pointer and returns TRUE if there\nis a match, otherwise False.\nThe pointer is the absolute pointer from the root down. A JSON object's\nstring value can be checked by append \"~$\" and the string value to test\nto the JSON pointer value. To test for a null value append \"~~\". A single\n\".\" can be used as a reference-token in the JSON pointer to match against\nany member or array item at that position in the document.\n\t\n\tArgument: 'exists'\n\t\tJSON pointer for a JSON item to check the presence of\n\t\tin the response.\n\t\n\tArgument: 'notexists'\n\t\tJSON pointer for a JSON item to check the absence of\n\t\tin the response.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>jsonPointerMatch</callback>\n\t\t<arg>\n\t\t\t<name>exists</name>\n\t\t\t<value>/responses/response</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>notexists</name>\n\t\t\t<value>/responses/response/name~$ABC</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>exists</name>\n\t\t\t<value>/responses/./name~$XYZ</value>\n\t\t</arg>\n\t</verify>\n\t\n## multistatusItems\nPerforms a check of multi-status response body and checks to see\nwhat hrefs were returned and whether those had a good (2xx) or bad\n(non-2xx) response code. The overall response status must be 207.\n\n\tArgument: 'okhrefs'\n\t\tA set of hrefs for which a 2xx response status is required.\n\t\n\tArgument: 'badhrefs'\n\t\tA set of hrefs for which a non-2xx response status is required.\n\t\n\tArgument: 'prefix'\n\t\tA prefix that is appended to all of the specified okhrefs and\n\t\tbadhrefs values.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>multistatusitems</callback>\n\t\t<arg>\n\t\t\t<name>okhrefs</name>\n\t\t\t<value>/calendar/test/1.ics</value>\n\t\t\t<value>/calendar/test/2.ics</value>\n\t\t\t<value>/calendar/test/3.ics</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>badhrefs</name>\n\t\t\t<value>/calendar/test/4.ics</value>\n\t\t\t<value>/calendar/test/5.ics</value>\n\t\t\t<value>/calendar/test/6.ics</value>\n\t\t</arg>\n\t</verify>\n\t\n## postFreeBusy\nLooks for specific FREEBUSY periods for a particular ATTENDEE.\n\n\tArgument: 'attendee'\n\t\tCalendar user address for attendee to match.\n\t\n\tArgument: 'busy'\n\t\tPeriod for FBTYPE=BUSY to match.\n\t\n\tArgument: 'tentative'\n\t\tPeriod for FBTYPE=BUSY-TENTATIVE to match.\n\t\n\tArgument: 'unavailable'\n\t\tPeriod for FBTYPE=BUSY-UNAVAILABLE to match.\n\t\n\tArgument: 'events'\n\t\tNumber of VEVENTs (in extended free-busy request) to match.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>postFreeBusy</callback>\n\t\t<arg>\n\t\t\t<name>attendee</name>\n\t\t\t<value>$cuaddr1:</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>busy</name>\n\t\t\t<value>20060101T230000Z/20060102T000000Z</value>\n\t\t</arg>\n\t</verify>\n\t\n## prepostcondition\nPerforms a check of response body and status code to verify that a\nspecific pre-/post-condition error was returned. The response status\ncode has to be one of 403 or 409.\n\n\tArgument: 'error'\n\t\tThe expected XML element qualified-name to match.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>prepostcondition</callback>\n\t\t<arg>\n\t\t\t<name>error</name>\n\t\t\t<value>DAV:too-many-matches</value>\n\t\t</arg>\n\t</verify>\n\t\n## propfindItems\nPerforms a check of propfind multi-status response body and checks to see\nwhether the returned properties (and optionally their values) are good (2xx) or bad\n(non-2xx) response code. The overall response status must be 207.\n\n\tArgument: 'root-element'\n\t\tExpected root element for the XML response. Normally this is DAV:multistatus\n\t\tbut, e.g., MKCOL ext uses a different root, but mostly looks like multistatus\n\t\totherwise.\n\t\n\tArgument: 'okprops'\n\t\tA set of properties for which a 2xx response status is required. Two forms can be used:\n\t\t\n\t\t'propname' - will test for the presence of the property named\n\t\t'propname'. The element data must be a qualified XML element\n\t\tname.\n\t\n\t\t'propname$value' - will test for the presence of the property\n\t\tnamed 'propname' and check that its value matches the provided\n\t\t'value'. The element data must be a qualified XML element name.\n\t\tXML elements in the property value can be tested provided proper\n\t\tXML escaping is used (see example).\n\t\n\t\t'propname!value' - will test for the presence of the property\n\t\tnamed 'propname' and check that its value does not match the provided\n\t\t'value'. The element data must be a qualified XML element name.\n\t\tXML elements in the property value can be tested provided proper\n\t\tXML escaping is used (see example).\n\t\n\tArgument: 'badhrefs'\n\t\tA set of properties for which a non-2xx response status is\n\t\trequired. The same two forms as used for 'okprops' can be used\n\t\there.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>propfindItems</callback>\n\t\t<arg>\n\t\t\t<name>okprops</name>\n\t\t\t<value>{DAV:}getetag</value>\n\t\t\t<value>{DAV:}getcontenttype$text/plain</value>\n\t\t\t<value>{X:}getstate$&lt;X:ok/&gt;</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>badprops</name>\n\t\t\t<value>{X:}nostate</value>\n\t\t</arg>\n\t</verify>\n\t\n## propfindValues\nPerforms a regular expression match against property values. The overall\nresponse status must be 207.\n\n\tArgument: 'props'\n\t\tA set of properties for which a 2xx response status is required. Two forms can be used:\n\t\t\n\t\t'propname$value' - will test for property value match\n\t\t'propname!value' - will test for property value non-match\n\t\n\tArgument: 'ignore'\n\t\tOne or more href values for hrefs in the response which will be\n\t\tignored. e.g. when doing a PROPFIND Depth:1, you may want to\n\t\tignore the top-level resource when testing as only the\n\t\tproperties on the child resources may be of interest.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>propfindValues</callback>\n\t\t<arg>\n\t\t\t<name>props</name>\n\t\t\t<value>{DAV:}getcontenttype$text/.*</value>\n\t\t\t<value>{DAV:}getcontenttype!text/calendar</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>ignore</name>\n\t\t\t<value>/calendars/test/</value>\n\t\t</arg>\n\t</verify>\n\t\n## statusCode\nPerforms a simple test of the response status code and returns True\nif the code matches, otherwise False.\n\t\n\tArgument: 'status'\n\t\tIf the argument is not present, the any 2xx status code response\n\t\twill result in True. The status code value can be specified as\n\t\t'NNN' or 'Nxx' where 'N' is a digit and 'x' the letter x. In the\n\t\tlater case, the verifier will return True if the response status\n\t\tcode's 'major' digit matches the first digit.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>statusCode</callback>\n\t\t<arg>\n\t\t\t<name>status</name>\n\t\t\t<value>2xx</value>\n\t\t</arg>\n\t</verify>\n\t\n## xmlDataMatch\nCompares the response with an XML data file and returns TRUE if there\nis a match, otherwise False.\n\t\n\tArgument: 'filepath'\n\t\tThe file path to a file containing data to match the response body to.\n\t\n\tArgument: 'filter'\n\t\tAny specified XML elements will have their content removed from the\n\t\tresponse XML data before the comparison with the file data is done.\n\t\tThis can be used to ignore element values that change in each request,\n\t\te.g., a time stamp.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>xmlDataMatch</callback>\n\t\t<arg>\n\t\t\t<name>filepath</name>\n\t\t\t<value>resource/test.xml</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>filter</name>\n\t\t\t<value>{DAV:}getlastmodified</value>\n\t\t</arg>\n\t</verify>\n\t\n## xmlElementMatch\nCompares the response with an XML path and returns TRUE if there\nis a match, otherwise False.\nThe path is the absolute xpath from the root element down. Attribute, attribute-value\nand text contents tests of the matched element can be done using:\n\t\n\t[@attr] - \"attr\" is present as an attribute\n\t[@attr=value] - \"attr\" is present as an attribute with the value \"value\"\n\t[=text] - node text is \"text\".\n\t[!text] - node text is not \"text\".\n\t[*text] - node text contains \"text\".\n\t[$text] - node text does not contain \"text\".\n\t[+text] - node text starts with \"text\".\n\t[^tag] - node has child element \"tag\".\n\t[^tag=text] - node has child element \"tag\" with text \"text\".\n\t[|] - node is empty.\n\t[||] - node is not empty.\n\t[json] - node contains valid JSON data.\n\t[icalendar] - node contains valid iCalendare data.\n\t\n\tEach path segment can now have its own test and \"../\" can be used to move up to\n\tthe parent. This allows testing for an element matching specific content plus\n\tits sibling matching other specific content. e.g., \"/{D}A/{D}B[=b]/../{D}C[=c]\n\twhich checks for an element {D}A with two child elements {D}B and {D}C each\n\twith a specific value.\n\t\n\tArgument: 'parent'\n\t\tElementTree style path for an XML element to use as the root for any\n\t\tsubsequent \"exists\" or \"notexists\" tests. This is useful for targeting\n\t\ta specific resource in a Depth:1 multistatus response.\n\t\n\tArgument: 'exists'\n\t\tElementTree style path for an XML element to check the presence of\n\t\tin the response.\n\t\n\tArgument: 'notexists'\n\t\tElementTree style path for an XML element to check the absence of\n\t\tin the response.\n\n\tExample:\n\t\n\t<verify>\n\t\t<callback>xmlDataMatch</callback>\n\t\t<arg>\n\t\t\t<name>exists</name>\n\t\t\t<value>{DAV:}response/{DAV:}href</value>\n\t\t</arg>\n\t\t<arg>\n\t\t\t<name>notexists</name>\n\t\t\t<value>{DAV:}response/{DAV:}getetag</value>\n\t\t</arg>\n\t</verify>\n"
 },
 {
  "repo": "apple/swift-evolution-staging",
  "language": "Swift",
  "readme_contents": "# Swift Evolution Staging\n\nThis repository is the starting point for Swift Evolution proposal\nimplementations. See the [Swift Evolution Process][se-process] to learn about\nhow ideas are pitched, refined, and then proposed for inclusion in the Swift\nstandard library.\n\n[se-process]: https://github.com/apple/swift-evolution/blob/master/process.md\n\nComplete this checklist when preparing your implementation:\n  \n- In `Package.swift` and in the _Introduction_ section below, rename your module\n  to use a short, camel-cased name of your proposed feature (ex: `SE0000_MyFeature`).\n  \n- Rename the folders and files in the `Sources` and `Tests` directories to match\n  your new module name.\n  \n- Implement your proposed feature in the `Sources` directory, and add tests in\n  the `Tests` directory.\n  \n- Make sure the Swift project code header is at the beginning of every source\n  file.\n  \n- Finish editing the section below, and then remove this checklist and\n  everything else above the line. That's it!\n\n--------------------------------------------------------------------------------\n\n# Package Name\n\n> **Note:** This package is a part of a Swift Evolution proposal for\n  inclusion in the Swift standard library, and is not intended for use in\n  production code at this time.\n\n* Proposal: [SE-NNNN](https://github.com/apple/swift-evolution/proposals/NNNN-filename.md)\n* Author(s): [Author 1](https://github.com/author1), [Author 2](https://github.com/author1)\n\n\n## Introduction\n\nA short description of the proposed library. \nProvide examples and describe how they work.\n\n```swift\nimport SE0000_PackageName\n\nprint(Placeholder.message)\n// Prints(\"Hello, world!\")\n```\n\n\n## Usage\n\nTo use this library in a Swift Package Manager project,\nadd the following to your `Package.swift` file's dependencies:\n\n```swift\n.package(\n    url: \"https://github.com/apple/swift-evolution-staging.git\",\n    .branch(\"SE0000_PackageName\")),\n```\n\n\n"
 },
 {
  "repo": "apple/example-package-dealer",
  "language": "Swift",
  "readme_contents": "To build this example package:\n\n    git clone https://github.com/apple/example-package-dealer.git\n    cd example-package-dealer\n    swift run\n"
 },
 {
  "repo": "apple/ml-cvpr2019-swd",
  "language": "Python",
  "readme_contents": "## Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation\n\n[IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.](http://cvpr2019.thecvf.com)\n\n[[Paper]](https://arxiv.org/abs/1903.04064)\n\n## Introduction\nIn this work, we connect two distinct concepts for unsupervised domain adaptation: feature distribution alignment between domains by utilizing the task-specific decision boundary and the Wasserstein metric. The proposed method is designed to work with high-dimensional/structured outputs where the probability measures do not have significant overlap. For example image classification for high number of classes, semantic segmentation, and object detection.\n\nThe code here is a basic illustration of the implementation for the [intertwining\nmoons 2D dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html). \nFor the source samples, we generate an upper moon (blue points) and a lower moon (red points), labeled 0 and 1, respectively. The target samples are generated from the same distribution as the source samples but with added domain shifts by rotation and translation. The model consists of a 3-layered fully-connected network for a feature generator and 3-layered fully-connected networks for classifiers.\n\nThis is for demonstration purposes only. Please see the paper for more details.\n\n## Getting Started\nThis code is designed to work with Python 2.7.10.\nVirtualenv is recommended.\nUse pip to install dependencies\n```\npip install -r requirements.txt\n```\nTo run the demo in Python 3, please modify the requirements.txt with tensorflow==1.13.1 and matplotlib==3.1.0\n\n## Running the code\nTo run the demo with adaptation:\n```\npython swd.py -mode adapt_swd\n```\n\nTo run the demo without adaptation:\n```\npython swd.py -mode source_only\n```\n\n## Interpreting Outputs\nOutputs will be saved as png and gif files in the current folder for each mode.\nThe outputs show the source and target samples with the current decision boundary.\n&nbsp;<br />\n<img src=\"assets/output.gif\" height=\"200\">\n"
 },
 {
  "repo": "apple/example-package-deckofplayingcards",
  "language": "Swift",
  "readme_contents": "This example package will be cloned and built as a dependency if you build the `Dealer` example:\n\n    git clone https://github.com/apple/example-package-dealer.git\n    cd example-package-dealer\n    swift run Dealer\n"
 },
 {
  "repo": "apple/ml-covid-mobility",
  "language": "R",
  "readme_contents": "# It\u2019s complicated: characterizing the time-varying relationship between cell phone mobility and COVID-19 spread in the US\n\nThis code accompanies the research paper, [It\u2019s complicated: characterizing the time-varying relationship between cell phone mobility and COVID-19 spread in the US](https://www.medrxiv.org/content/10.1101/2021.04.24.21255827v1). In this paper, we propose an interpretable statistical model to identify spatiotemporal variation in the association between mobility and COVID-19 infection rates.\n\n## Introduction\n\nThis model is intended to help explain the relationship between human mobility (as captured by cell phones, and recorded by Google) and the spread of SARS-CoV2 / COVID-19 in the United States. In fall 2020, when we started this investigation, it was unclear to what extent these mobility variables were associated with the spread of the virus.  Although it was clear that unprecedented drops in mobility in March and April of 2020 preceded declining infection rates, there was limited evidence about whether this strong correlation held over time, and to what extent it varied by location. Our linear mixed effects model with a time-varying coefficient for the effect of mobility addresses this question. \n\nWe focus on the weekly log growth rate of infections, i.e. log( infections_t / infections_{t-1} ), where t denotes the week.  To estimate weekly infections, we utilize our previously developed methodology, described in full in [this preprint](https://www.medrxiv.org/content/10.1101/2020.10.16.20212753v1.full), in estimating infection incidence from reported case data.  \n\n## Getting Started \n\nThis code is designed to work with R version 4.0.2 along with the following packages:\n\n```\nargparse:2.0.3, bayesplot:1.7.2, brms:2.14.0, cmdstanr:0.1.3, covidcast:0.3.0, doParallel:1.0.15, ggdist:2.2.0, gridExtra:2.3, incidental:0.1, lubridate:1.7.9, mice:3.11.0, readxl:1.3.1, rstan:2.21.2, scales:1.1.1, splines2:0.4.1, tictoc:1.0, tidybayes:2.1.1, tidyverse:1.3.0, usmap:0.5.1.\n```\n\n### Running the Code \n\nWe provide tools to reproduce the main analyses in our paper. Specifically, we provide functionality to 1) download and preprocess publicly available COVID-19 and Google mobility data and 2) fit statistical models to explore the time-varying relationship between cell phone mobility and COVID-19 spread. The following code snippet can be used to download and preprocess data.\n```\nsource(\"utils_data_get.R\")\nsave_dir = \"./data/\"\nnum_cores = 4\nload_process_estimate_impute_save_data(save_dir, num_cores = num_cores)\n```\n\nWe provide the script `fit_model.R` to sample from the posterior distribution for a class of models that relate COVID-19 spread and mobility. For example, \n\n```\nRscript fit_model.R \n```\n\nfits the main model model in our paper and saves the model file to `./model_output`. The RMarkdown file `examples.md` shows different R2 disaggregations and produces the main figures from our paper. \n\n## Data Sources\n\nWe use [Google's Commmunity Mobility Reports](https://www.google.com/covid19/mobility/) as our data source for mobility. Their mobility is aggregated at the US County level each day into 6 coarse categories: retail & recreation, grocery & pharmacy, parks, transit stations, workplaces, and residential. See [here](https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927) for more information on the data.\n\nWe also obtain information on county-level temperature from [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data), county-level population estimates from [this repo](https://github.com/JieYingWu/COVID-19_US_County-level_Summaries) of COVID-19 US county-level summaries, COVID-19 case counts from the [New York Times](https://github.com/nytimes/covid-19-data) and [New York City Department of Health](https://github.com/nychealth/coronavirus-data), mask-wearing adherence from the [New York Times](https://github.com/nytimes/covid-19-data/tree/master/mask-use), [Pew Research](https://www.pewresearch.org/fact-tank/2020/06/23/most-americans-say-they-regularly-wore-a-mask-in-stores-in-the-past-month-fewer-see-others-doing-it/), and the [Delphi](https://github.com/cmu-delphi/delphi-epidata) API for epidemiological data. \n\n### Preprocessing\n\nWe use RIDE, the Robust Incidence Deconvolution Estimator, to estimate the unknown true number of infections each day per US county, using the [incidental R package](https://cran.r-project.org/web/packages/incidental/index.html). We use multiple imputation and the [mice R package](https://cran.r-project.org/web/packages/mice/index.html) to impute missing covariate values (i.e. mobility values) for each county at the weekly level; see the paper supplement and the `utils_data_get.R` script in this repo for additional details. \n\nWe exclude counties with less than 250 total COVID-19 cases as of the last date considered, February 20, 2021, which removes 176 counties. Next, we exclude counties with extreme growth patterns, where any weekly absolute growth rate exceeds 2 (removing 8 counties), or absolute growth rates exceeds 1.5 and the county has less than 50, 000 people (removing 8 counties). These restrictions remove outliers that arise from difficult to model events, such as prison outbreaks in sparsely populated counties.\n\n## References \n\nIt\u2019s complicated: characterizing the time-varying relationship between cell phone mobility and COVID-19 spread in the US.\nSean Jewell, Joseph Futoma, Lauren Hannah, Andrew C. Miller, Nicholas J. Foti, Emily B. Fox\nmedRxiv 2021.04.24.21255827; doi: https://doi.org/10.1101/2021.04.24.21255827\n\nStatistical deconvolution for inference of infection time series.\nAndrew C. Miller, Lauren Hannah, Joseph Futoma, Nicholas J. Foti, Emily B. Fox, Alexander D\u2019Amour, Mark Sandler, Rif A. Saurous, Joseph A. Lewnard \nmedRxiv 2020.10.16.20212753; doi: https://doi.org/10.1101/2020.10.16.20212753\n"
 },
 {
  "repo": "apple/swift-statsd-client",
  "language": "Swift",
  "readme_contents": "# SwiftStatsDClient\n\na metrics backend for [swift-metrics](https://github.com/apple/swift-metrics) that uses the [statsd](https://github.com/b/statsd_spec) protocol, and can be used to integrate applications with observability solutions that support `statsd` including:\n* [aws](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-custom-metrics-statsd.html)\n* [azure](https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform)\n* [google cloud](https://cloud.google.com/monitoring/agent/plugins/statsd)\n* [ibm cloud](https://cloud.ibm.com/catalog/services/ibm-cloud-monitoring-with-sysdig)\n* [grafana](https://grafana.com)\n* [graphite](https://graphiteapp.org)\n* many others\n\n## Getting started\n\ncreate an instance of the `StatsdClient` and boostrap the `MertricsSystem`  in your application's main:\n\n```swift\nlet statsdClient = try StatsdClient(host: host, port: port)\nMetricsSystem.bootstrap(statsdClient)\n```\n\nsee https://github.com/apple/swift-metrics#selecting-a-metrics-backend-implementation-applications-only\n\nremeber to also shutdown the client before you application terminates:\n\n```swift\nstatsdClient.shutdown()\n```\n\n## Architecture\n\nthe statsd client uses [swift-nio](https://github.com/apple/swift-nio) to establish a UDP connection to the statsd server\n\nmetrics types are mapped as follwoing:\n* Counter -> Counter\n* Gauge -> Gauge\n* Recorder -> Histogram\n* Timer -> Timer\n\n## Security\n\nPlease see [SECURITY.md](SECURITY.md) for details on the security process.\n\n## Getting involved\n\nDo not hesitate to get in touch as well, over on https://forums.swift.org/c/server\n"
 },
 {
  "repo": "apple/AudioUnitSDK",
  "language": "C++",
  "readme_contents": "# AudioUnitSDK\n\n## Overview\nThe AudioUnitSDK contains a set of base classes as well as utility sources required for Audio Unit development. These utility classes extend or wrap Core Audio API\u2019s providing developers with the essential scaffolding to create audio effects, instruments, and generators on Apple platforms. They provide an easier to implement C++ class model wrapping the C framework APIs.\n\n## Installing dependencies\n1. Install [Xcode][Xcode]\n\n[Xcode]: https://developer.apple.com/xcode/resources/\n\n## Building the project\n1. Open AudioUnitSDK.xcodeproj\n2. Build the AudioUnitSDK target\n3. Add headers from $(BUILT_PRODUCTS_DIR)/usr/local/include/AudioUnitSDK to your projects include path\n4. Link libAudioUnitSDK.a to your project\n\n\nAlternatively, you can add the AudioUnitSDK source directly to your project and build as part of your target. \n\n## Supported Deployment Targets\nmacOS (OS X) 10.9 / iOS 9.0 or later.\n\nCopyright (C) 2021 Apple Inc. All rights reserved."
 },
 {
  "repo": "apple/swift-nio-zlib-support",
  "language": "Swift",
  "readme_contents": "# swift-nio-zlib-support\n\n## Deprecated\n\nThis package used to support depending on zlib in the Swift Package Manager. It is no longer necessary for this use-case:\ninstead, you can use either `systemLibrary` targets or the SwiftPM `linkerSettings` flag. For examples of its use, see\n`CNIOExtrasZlib` in [`swift-nio-extras`](https://github.com/apple/swift-nio-extras).\n\n## Usage\n\nThis is a package just to be able to use zlib with SwiftPM.\nTo use this package, add this to your `Package.swift` in `dependencies`:\n\n    .package(url: \"https://github.com/apple/swift-nio-zlib-support.git\", from: \"1.0.0\"),\n\n"
 },
 {
  "repo": "apple/openjdk",
  "language": "Java",
  "readme_contents": "Xcode OpenJDK\n=============\n\nThis release of OpenJDK it to provide the community access to the source\ncode modifications to OpenJDK as shipped in Xcode.  Its use outside of\nXcode.app is not supported by Apple.\n\n\nWelcome to the JDK!\n===================\n\nFor information about building the JDK, including how to retrieve all\nof the source code, please see either of these files:\n\n  * doc/building.html   (html version)\n  * doc/building.md     (markdown version)\n\nSee http://openjdk.java.net/ for more information about the OpenJDK\nCommunity and the JDK.\n"
 },
 {
  "repo": "apple/ml-uwac",
  "language": "Python",
  "readme_contents": "# Uncertainty Weighted Actor Critic (UWAC)\n\nThis is the official implementation of Uncertainty Weighted Actor Critic (UWAC) that accompanies the research paper, Uncertainty Weighted Actor Critic for Offline Reinforcement Learning (ICML 2021).\n\nUncertainty Weighted Actor-Critic (UWAC), an algorithm detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. We adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms.\n\n## Getting Started\nThis codebase is built off of the official implementation of BEAR (https://github.com/rail-berkeley/d4rl_evaluations/tree/master/bear) and rlkit (https://github.com/vitchyr/rlkit/). In order to run UWAC, follow the installation instructions for rlkit as shown below, then install D4RL(https://github.com/rail-berkeley/d4rl).\n\nThen in order to run UWAC, an example command is:\n```\npython examples/UWAC_hdf5_d4rl.py --env='halfcheetah-medium-v0' --policy_lr=1e-4 --num_samples=100\n```\nwhere, `env` refers to a d4rl environment, `policy_lr` is the policy learning rate.\n\nTo run BEAR (baseline), an example command is:\n```\npython examples/bear_hdf5_d4rl.py --env='halfcheetah-medium-v0' --policy_lr=1e-4 --num_samples=100\n```\n\nSee readme from RLkit (https://github.com/vitchyr/rlkit) for troubleshooting with respect to RLkit problems.\n\n# License\nThis code is released under the [LICENSE](LICENSE) terms."
 },
 {
  "repo": "apple/ml-probabilistic-attention",
  "language": "Python",
  "readme_contents": "# Project Name\n\nThis software project accompanies the research paper [Probabilistic Attention for Interactive Segmentation](https://arxiv.org/abs/2106.15338) and its predecessor [Probabilistic Transformers](https://arxiv.org/abs/2010.15583).\n\nIt contains implementation of a Pytorch module for the probabilistic attention update proposed in the above paper(s). \n\n## Documentation\n\nRuns an update of the probabilistic version of attention based on a Mixture of Gaussians model.  \nIt accepts the following parameters during a forward pass:  \n* q: A tensor of queries with dims N, G, C, H  \n* zeta: A tensor of keys (query/key Gaussian means) with dims N, G, C, H  \n* alpha: A scalar (see special case above) or tensor of query/key Gaussian precisions with dims N, G, C, H  \n* mu: A tensor of value Gaussian means with dims N, G, Cv, H  \n* beta: A scalar (see special case above) or tensor of value Gaussian precisions with dims N, G, C, H  \n* pi: A tensor of mixture component priors with dims N, G, H, H  \n* v_init: A tensor of initial vals for the values with dims N, G, Cv, H (optional)  \n* v_fixed: A tensor of fixed vals for the values with dims N, G, (Cv+1), H (optional). The extra (last) channel is an indicator for the fixed val locations  \n* zeta_prior_precision: A tensor of precisions for the Gaussian prior over zeta with dims N, G, C, H (optional)  \n* mu_prior_precision: A tensor of precisions for the Gaussian prior over mu with dims N, G, Cv, H (optional)  \n* q_pos_emb: A tensor of query positional embeddings with dims C, H, H  \n* zeta_pos_emb: A tensor of key positional embeddings with dims C, H, H  \n* v_pos_emb: A tensor of value positional embeddings with dims Cv, H, H  \n* nonzero_wts_mask: A boolean indexing tensor for setting weight matrix values to zero (where mask value is false) with dims H, H  \n\nAnd returns the following output tensor:   \n* Updated values with dims N, G, Cv, H if no position embedding (v_pos_emb=None) else N, G, 2*Cv, H  \n\nNotably, this layer is equivalent to a standard dot product attention (without position embeddings) when:  \n* uniform_query_precision = True  \n* uniform_value_precision = True  \n* magnitude_priors = True  \n* alpha = 1/sqrt(C) (Could be a scalar to save some memory)  \n* beta = 0 (Could be a scalar to save some memory)  \n* v_init = None  \n* v_fixed = None  \n\n## Getting Started \n\nThe module is in the file probabilisticattention.py.\nIt can be imported as any other Pytorch layer.\n"
 },
 {
  "repo": "apple/ml-envmapnet",
  "language": "Python",
  "readme_contents": "# HDR Environment Map Estimation for Real-Time Augmented Reality\n\nThis software project contains implementations of metrics used in the paper, **HDR Environment Map Estimation for Real-Time Augmented Reality**, Gowri Somanath, Daniel Kurz, *Proc. IEEE/CVF CVPR 2021*, which can be found on [Machine Learning Research at Apple](https://machinelearning.apple.com/research/hdr-environment-map-estimation) and [arXiv](https://arxiv.org/abs/2011.10687). \n\n![](./images/figure-1-teaser.png)\nGiven a partial LDR environment map from a camera image (a), we estimate a visually coherent and completed HDR map that can\nbe used by graphics engines to create light probes. These enable rendering virtual objects with any material finish into the real environment\nin a perceptually pleasing and realistic way (b), more similar to ground truth (c) than state-of-the-art methods (d). Our mobile app renders\na virtual teapot using our estimated environment map in real-time (e).\n\n## Results\nWe performed quantitative benchmarking using the metrics provided, and results on the publicly available [Laval Indoor HDR Images dataset](http://indoor.hdrdb.com/).\nWe encourage related future research to use the same metrics and the provided implementations thereof to ensure comparability of experimental results.\nSee paper for details.\n\n| Method | FID | AngularError |\n| --- | --- | --- |\n| EnvMapNet (Ours) |  52.7 |  34.3 \u00b1 18.5| \n| Ours without ProjectionLoss|  77.7 | 39.2 \u00b1 29.9| \n| Ours without ClusterLoss|  203 | 75.1 \u00b1 25| \n| Gardner et al. | 197.4 | 65.3 \u00b1 24.5| \n| Artist IBL (Fixed) | - | 46.5 \u00b1 15.4| \n\n\n## Metrics Computation and Demo\nTo get started, feel free to run the provided demo application as follows:\n```\npython DemoEnvMapNetMetrics.py\n```\nThe demo uses metrics implemented in ```FIDCalculations.py``` and ```ParametricLights.py```.\nThe metrics are detailed in Section 4 (Metrics for benchmarking), and the results are presented in Table 1 and Section 5 of the paper.\n\nThis code has been tested with Python 3.6 and 3.7, and it has dependencies on the following packages.\n\nFor parametric lights and angular error:\n\n1. `pip3 install opencv-python`\n2. `pip3 install scikit-image`\n\nAdditional requirements for Frechet Inception Distance (FID):\n\n3. `pip3 install --upgrade keras==2.2.4`\n4. `pip3 install py360convert`\n\n\n"
 },
 {
  "repo": "apple/ml-cread",
  "language": "Python",
  "readme_contents": "# CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues \n\n[**Paper**](https://arxiv.org/abs/2105.09914) |\n[**Task**](#Task-Description) | [**Dataset**](#Dataset) | [**Run Code**](#Run-the-Code) |\n[**Citation**](#Citation) | [**License**](#License) | [**Contact**](#Contact-Us)\n\nThis is the source code of the paper [CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues](https://arxiv.org/abs/2105.09914).\nIn this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding.\nThe coreference resolution [MuDoCo](https://github.com/facebookresearch/mudoco) dataset augmented with our query rewrite annotation is released as well.\n\n## Task Description\nGiven an ongoing dialogue between a user and a dialogue assistant, for the user query, the model is required to predict both coreference links between the query and the dialogue context, and the self\\-contained rewritten user query that is independent to the dialogue context.\n\n## Dataset\nThe MuDoCo dataset is a public dataset that contains 7.5k task\\-oriented multi\\-turn dialogues across 6 domains (calling, messaging, music, news, reminders, weather). Each dialogue turn is annotated with coreference links (`links` field). Please refer to [MuDoCo](https://github.com/facebookresearch/mudoco) for more details.\n\nIn the **MuDoCo\\-QR\\-dataset** used in work, we annotate the query rewrite for each utterance, including both user and system turn. On top of the MudoCo data format, we add three fields `graded`, `rewrite_required` and `rewritten_utterance`. Most of the turns are with annotated with query rewrite (`graded` is true). Only 1.4% dialogue turns with incomplete dialogue context (e.g., missing turns) in MuDoCo are filtered out (`graded` is false). `rewrite_required` records whether the input utterance should be rewritten or not. `rewritten_utterance` is the rewritten query, same as the utterance if `rewrite_required` is false.\n\nThe resulting dataset is provided in the folder `MuDoCo-QR-dataset`.\n\n```json\n{\n    \"number\": 3,\n    \"utterance\": \"Show me a live version that he moonwalks on .\",\n    \"links\": [\n        [\n            {\n                \"turn_id\": 1,\n                \"text\": \"Michael Jackson\",\n                \"span\": {\n                    \"start\": 5,\n                    \"end\": 20\n                }\n            },\n            {\n                \"turn_id\": 3,\n                \"text\": \"he\",\n                \"span\": {\n                    \"start\": 28,\n                    \"end\": 30\n                }\n            }\n        ]\n    ],\n    \"graded\": true,\n    \"rewritten_utterance\": \"Show me a live version that Michael Jackson moonwalks on\",\n    \"rewrite_required\": true\n}\n```\n\n## Requirements\npython3.6 and the packages in `requirements.txt`, install them by running\n```console\n>>> pip install -r requirements.txt\n```\n\n## Run the Code\nEnter the `modeling` folder and follow the instruction below.\n\n```console\n>>> cd modeling\n```\n\n## Data Pre-processing\nFirst run the following command to prepare the data for training.\n\nThe processed data will be stored in the `proc_data/` directory.\n\n```console\n>>> python utils/process_data.py\n```\n\n\n## Training\nRun `train.sh` to train the model, which calls `main.py` with default hyper-parameters.\n\n```console\n>>> bash train.sh [job_name]\n```\n\nThe model checkpoint will be stored at `checkpoint/$job_name`, and training log file is at `log/$job_name.log`\n\nA reference training log (`log/trained-cread.log`) is provided.\n\n\n## Evaluation\nRun `decode.sh` to decode using a trained model. `job_name` is the same as specified in training.\n\n```console\n>>> bash decode.sh [job_name]\n```\n\nEvaluation result, with both generated rewritten utterances and model performance, is recorded in `deocde/$job_name.json`.\n\nA reference decoding file (`decode/trained-cread.json`) is provided.\n\n\n\n## Key Hyper-parameters in Main.py\n- task: which task to perform. The default value `qr-coref` specifies our complete joint learning model. Set to `qr` for the model variant `qr-only` model or `coref` for the model variant `coref-only` model.\n\n- coref\\_layer\\_idx: which gpt2 layers to use for coreference resolution, e.g., \"1,5,11\" uses three layers. n is between 0 to 11, if default gpt2\\-small is used.\n- n\\_coref\\_head: how many attention heads to use in each layer for coreference resolution. n is between 1 to 12.\n- use_coref\\_attn: whether to use coref2qr attention mechanism.\n- use\\_binary\\_cls: whether to use binary rewriting classifier.\n\nMore detailed explanation of other arguments can be found in `utils/utils.py`.\n\n## Citation\n```bibtex\n@inproceedings{tseng-etal-2021-cread,\n    title = \"{CREAD}: Combined Resolution of Ellipses and Anaphora in Dialogues\",\n    author = \"Tseng, Bo-Hsiang  and\n      Bhargava, Shruti  and\n      Lu, Jiarui  and\n      Moniz, Joel Ruben Antony  and\n      Piraviperumal, Dhivya  and\n      Li, Lin  and\n      Yu, Hong\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.naacl-main.265\",\n    pages = \"3390--3406\",\n}\n```\n\n## License\nThe code in this repository is licensed according to the [LICENSE](LICENSE) file.\n\n## Contact Us\nPlease contact bht26@cam.ac.uk or hong\\_yu@apple.com, or raise an issue in this repository.\n\n"
 },
 {
  "repo": "apple/swift-distributed-tracing-baggage-core",
  "language": "Swift",
  "readme_contents": "# \ud83e\uddf3 Distributed Tracing Baggage Core\n\n> \ud83d\udcd4 **NOTE**: It is very unlikely that you want to depend on _this_ package itself. \n>\n> Most libraries and projects should depend on and use the https://github.com/apple/swift-distributed-tracing-baggage package instead, \n> unless avoiding the `SwiftLog` dependency is necessary.\n\n`Baggage` is a minimal (zero-dependency) context propagation container, intended to \"carry\" baggage items\nfor purposes of cross-cutting tools to be built on top of it.\n\nIt is modeled after the concepts explained in [W3C Baggage](https://w3c.github.io/baggage/) and the \nin the spirit of [Tracing Plane](https://cs.brown.edu/~jcmace/papers/mace18universal.pdf) 's \"Baggage Context\" type,\nalthough by itself it does not define a specific serialization format. \n \nPlease refer to [Swift Distributed Tracing Baggage](https://github.com/apple/swift-distributed-tracing-baggage) \nand [Swift Distributed Tracing](https://github.com/apple/swift-distributed-tracing) for usage guides of this type.\n\n## Dependency\n\n In order to depend on this library you can use the Swift Package Manager, and add the following dependency to your `Package.swift`:\n\n```swift\ndependencies: [\n  .package(\n    name: \"swift-baggage-context-core\",\n    url: \"https://github.com/apple/swift-distributed-tracing-baggage-core.git\",\n    from: \"0.1.0\"\n  )\n]\n```\n\nand depend on the module in your target:\n\n```swift \ntargets: [\n    .target(\n        name: \"MyAwesomeApp\",\n        dependencies: [\n            \"CoreBaggage\",\n        ]\n    ),\n    // ... \n]\n```\n\n## Usage\n\nPlease refer to [Swift Distributed Tracing Baggage](https://github.com/apple/swift-distributed-tracing-baggage) for the intended usage,\nand detailed guidelines.\n\nAlternatively, please refer to the API documentation of the Baggage type.\n\n## Contributing\n\nPlease make sure to run the `./scripts/soundness.sh` script when contributing, it checks formatting and similar things.\n\nYou can make ensure it always is run and passes before you push by installing a pre-push hook with git:\n\n```\necho './scripts/soundness.sh' > .git/hooks/pre-push\n```\n"
 },
 {
  "repo": "apple/ml-stuttering-events-dataset",
  "language": "Python",
  "readme_contents": "\n# Stuttering Events in Podcasts Dataset (SEP-28k) \n\nThe SEP-28k dataset contains stuttering event annotations for approximately 28,000 3-second clips. In addition we include stutter event annotations for about 4,000 3-second clips from the FluencyBank dataset. Audio files are not part of this released dataset but may be downloaded using URLs provided in the `*_episodes.csv` files. Original copyright remains with the podcast owners. \n\n\n## Annotation Descriptions\n\nEach 3-second clip was annotated with the following labels by three annotators who were not clinicians but did have training on how to identify each type of stuttering event. Label files contain counts (out of three) corresponding to how many reviewers selected a given label. Multiple labels may be selected for a given clip. \n\n**Stuttering event labels**:\n* **Prolongation**: Elongated syllable (e.g., M[mmm]ommy)\n* **Block**: Gasps for air or stuttered pauses\n* **Sound Repetition**: Repeated syllables (e.g., I [pr-pr-pr-]prepared dinner)\n* **Word Repetition**: The same word or phrase is repeated (e.g., I made [made] dinner)\n* **No Stuttered Words**: Confirmation that none of the above is true.\n* **Interjection**: Common filler words such as \"um\" or \"uh\" or person-specific filler words that individuals use to cope with their stutter (e.g., some users frequently say \"you know\" as a filler).\n\n**Additional labels**:\n* **Unsure**: An annotator selects this if they are not confident in their labeling.\n* **Poor Audio Quality**: It is difficult to understand due to, for example, microphone quality.\n* **Difficult To Understand**: It is difficult to understand the speech.\n* **Natural Pause**: There is a pause in speech that is not considered a block or other disfluency. \n* **Music**: There is background music playing (only in SEP-28k)\n* **No Speech**: There is no speech in this clip. It is either silent or there is just background noise.\n\n**Data Files**:\n* **Links to media files:** Podcast/FluencyBank names, audio urls, and keycodes used with the annotation labels (`SEP-28k_episodes.csv` and `fluencybank_episodes.csv`)\n* **Annotations:** Clips used within each audio file with corresponding start/stop time and fluency labels (`SEP-28k_labels.csv` and `fluencybank_labels.csv`). \n\n\n# Downloading & Processing Scripts\n\n\nThere are two scripts used to download the raw audio files and extract into clips that correspond to the clip annotations. `[WAV_DIR]` refers to the folder where you are storing all of the raw audio data and `[CLIP_DIR]` refers to where you want to place the clips. These may be the same folder. \n\nTo download and extract clips from both datasets run the following from this directory\n\n* `python download_audio.py --episodes SEP-28k_episodes.csv --wavs [WAV_DIR]`\n* `python extract_clips.py --labels SEP-28k_labels.csv --wavs [DATA_DIR] --clips [CLIP_DIR]`\n* `python download_audio.py --episodes fluencybank_episodes.csv --wavs [WAV_DIR]`\n* `python extract_clips.py --labels fluencybank_labels.csv --wavs [DATA_DIR] --clips [CLIP_DIR]`\n\nThe raw SEP-28k wav files are 32 Gb and clipped SEP-28k wav files are 2.6 Gb. \n\n\n# Citation\n\nIf you find the SEP-28k dataset or this code useful in your research, please cite the following paper:\n```\n@misc{lea:2021,\n    author       = {Colin Lea AND Vikramjit Mitra AND Aparna Joshi AND Sachin Kajarekar AND Jeffrey P. Bigham},\n    title        = {{SEP-28k}: A Dataset for Stuttering Event Detection from Podcasts with People Who Stutter},\n    howpublished = {ICASSP 2021},\n}\n```\n\n# License\n\nThe code in this repository is licensed according to the [LICENSE](LICENSE) file.\n\nThe SEP-28k dataset is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0). To view a copy of this license, visit https://creativecommons.org/licenses/by-nc/4.0/."
 },
 {
  "repo": "apple/ml-multiple-futures-prediction",
  "language": "Python",
  "readme_contents": "# Multiple Futures Prediction\n## Paper\nThis software accompanies the paper [**Multiple Futures Prediction**](https://arxiv.org/abs/1911.00997). ([Poster](multiple_futures_prediction/assets/imgs/neurips_mfp_poster.pdf))<br>\n[Yichuan Charlie Tang](https://www.cs.toronto.edu/~tang) and Ruslan Salakhutdinov<br>\nNeural Information Processing Systems, 2019. (NeurIPS 2019)\n\n\nPlease cite our paper if you find our work useful for your research:\n```\n@article{tang2019mfp,\n  title={Multiple Futures Prediction},\n  author={Tang, Yichuan Charlie and Salakhutdinov, Ruslan},\n  booktitle={Advances in neural information processing systems},\n  year={2019}\n}\n```\n\n## Introduction\nMultiple Futures Prediction (MFP) is a framework for learning to forecast or predict future trajectories of agents, such as vehicles or pedestrians. A key feature of our framework is that it is able to learn multiple modes or multiple possible futures, by learning directly from trajectory data without annotations. Multi-agent interactions are also taken into account and the framework scales to an arbitrary number of agents in the scene by using a novel dynamic attention mechanism. It currently achieves state-of-the-art results on three vehicle forecasting datasets.\n\nThis research code is for demonstration purposes only. Please see the paper for more details.\n\n### Overall Architecture\n<p align=\"center\">\n<img src='multiple_futures_prediction/assets/imgs/mfp_comp_graph.png' width=\"700px\"/>\n\nThe Multiple Futures Prediction (MFP) architecture is shown above. For an arbitrary number of agents in the scene, we first use RNNs to encode their past trajectories into feature vectors. Dynamic attentional contextual encoding aggregates interactions and relational information. For each agent, a distribution over its latent modes are then predicted. You can think of the latent modes representing conservative/aggressive behaviors or directional (left vs. right turns) intentions. Given a distribution over the latent modes, decoding RNNs are then employed to decode or forecast future temporal trajectories. The MFP is a latent-variable graphical model and we use the EM algorithm to optimize the evidence lower-bound.\n\n\n## Getting Started\n\n### Prerequisites\nThis code is tested with Python 3.6, and PyTorch 1.1.0. Conda or Virtualenv is recommended.<br>\nUse pip (recent version, e.g. 20.1) to install dependencies, for example:\n```\npython3.6 -m venv .venv # Create new venv\nsource ./venv/bin/activate # Activate it\npip install -U pip # Update to latest version of pip\npip install -r requirements.txt # Install everything\n```\n\n### Datasets\n\n#### How to Obtain NGSIM Data:\n\n1. Obtain NGSIM Dataset here (US-101 and I-80):<br>\n(https://data.transportation.gov/Automobiles/Next-Generation-Simulation-NGSIM-Vehicle-Trajector/8ect-6jqj)\n```\nSpecifically you will need these files:\nUS-101:\n'0750am-0805am/trajectories-0750am-0805am.txt'\n'0805am-0820am/trajectories-0805am-0820am.txt'\n'0820am-0835am/trajectories-0820am-0835am.txt'\n\nI-80:\n'0400pm-0415pm/trajectories-0400-0415.txt'\n'0500pm-0515pm/trajectories-0500-0515.txt'\n'0515pm-0530pm/trajectories-0515-0530.txt'\n```\n2. Preprocess dataset with code from Nachiket Deo and Mohan M. Trivedi:<br> [Convolutional Social Pooling for Vehicle Trajectory Prediction.] (CVPRW, 2018)<br>\n(https://github.com/nachiket92/conv-social-pooling)<br>\n\n3. From the conv-social-pooling repo, run prepocess_data.m, this should obtain three files:<br>\nTrainSet.mat, ValSet.mat, and TestSet.mat. Copy them to the ngsim_data folder.\n\n### Usage\n\n#### Training\n```bash\ntrain_ngsim --config multiple_futures_prediction/configs/mfp2_ngsim.gin\n```\nor\n```bash\npython -m multiple_futures_prediction.cmd.train_ngsim_cmd --config multiple_futures_prediction/configs/mfp2_ngsim.gin\n```\nHyperparameters (e.g. specifying how many modes of MFP) can be specified in the .gin config files.\n\nExpected training outputs:\n```\nEpoch no: 0 update: 99 | Avg train loss: 57.3198  learning_rate:0.00100\nEpoch no: 0 update: 199 | Avg train loss: 4.7679  learning_rate:0.00100\nEpoch no: 0 update: 299 | Avg train loss: 4.3250  learning_rate:0.00100\nEpoch no: 0 update: 399 | Avg train loss: 4.0717  learning_rate:0.00100\nEpoch no: 0 update: 499 | Avg train loss: 3.9722  learning_rate:0.00100\nEpoch no: 0 update: 599 | Avg train loss: 3.8525  learning_rate:0.00100\nEpoch no: 0 update: 699 | Avg train loss: 3.5253  learning_rate:0.00100\nEpoch no: 0 update: 799 | Avg train loss: 3.6077  learning_rate:0.00100\nEpoch no: 0 update: 899 | Avg train loss: 3.4526  learning_rate:0.00100\nEpoch no: 0 update: 999 | Avg train loss: 3.5830  learning_rate:0.00100\nStarting eval\neval  val_dl nll\ntensor([-1.5164, -0.3173,  0.3902,  0.9374,  1.3751,  1.7362,  2.0362,  2.3008,\n         2.5510,  2.7974,  3.0370,  3.2702,  3.4920,  3.7007,  3.8979,  4.0836,\n         4.2569,  4.4173,  4.5682,  4.7082,  4.8378,  4.9581,  5.0716,  5.1855,\n         5.3239])\n```\nDepending on the CPU/GPU available, it can take from one to two days to complete \n300K training updates on the NGSIM dataset and match the results in Table 5 of the paper.\n\n## License\nThis code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/ml-qrecc",
  "language": "Python",
  "readme_contents": "# Open-Domain Question Answering Goes Conversational via Question Rewriting\n\n[**Tasks**](#task-description) | [**Dataset**](#dataset) | [**Evaluation**](#evaluation) |\n[**Paper**](https://arxiv.org/abs/2010.04898) |\n[**Citation**](#citation) | [**License**](#license)\n\nWe introduce QReCC (**Q**uestion **Re**writing in **C**onversational **C**ontext), an end-to-end open-domain question answering dataset comprising of 14K conversations with 81K question-answer pairs.\nThe goal of this dataset is to provide a challenging benchmark for end-to-end conversational question answering that includes the individual subtasks of question rewriting, passage retrieval and reading comprehension.\nPlease refer to our paper [Open-Domain Question Answering Goes Conversational via Question Rewriting](https://arxiv.org/abs/2010.04898) for details.\n\n## Task Description\n\nThe task in QReCC is to find answers to conversational questions within a collection of 10M web pages split into 54M passages.\nAnswers to questions in the same conversation may be distributed across several web pages.\n\n## Dataset\n\nQReCC contains 14K conversations with 81K question-answer pairs.\nWe build QReCC on questions from [TREC CAsT](https://github.com/daltonj/treccastweb/tree/master/2019/data), [QuAC](https://quac.ai) and [Google Natural Questions](https://github.com/google-research-datasets/natural-questions).\nWhile TREC CAsT and QuAC datasets contain multi-turn conversations, Natural Questions is not a conversational dataset.\nWe used questions in NQ dataset as prompts to create conversations explicitly balancing types of context-dependent questions, such as anaphora (co-references) and ellipsis.\n\nFor each query we collect query rewrites by resolving references, the resulting query rewrite is a context-independent version of the original (context-dependent) question.\nThe rewritten query is then used to with a search engine to answer the question. Each query is also annotated with answer, link to the web page that used to produce the answer.\n\nEach conversation in the dataset contains a unique `Conversation_no`, `Turn_no` unique within a conversation, the original `Question`, `Context`, `Rewrite`, `Answer` with `Answer_URL` and the `Conversation_source`.\n\n```json\n{\n  \"Context\": [\n    \"What are the pros and cons of electric cars?\",\n    \"Some pros are: They're easier on the environment. Electricity is cheaper than gasoline. Maintenance is less frequent and less expensive. They're very quiet. You'll get tax credits. They can shorten your commute time. Some cons are: Most EVs have pretty short ranges. Recharging can take a while.\"\n  ],\n  \"Question\": \"Tell me more about Tesla\",\n  \"Rewrite\": \"Tell me more about Tesla the car company.\",\n  \"Answer\": \"Tesla Inc. is an American automotive and energy company based in Palo Alto, California. The company specializes in electric car manufacturing and, through its SolarCity subsidiary, solar panel manufacturing.\",\n  \"Answer_URL\": \"https://en.wikipedia.org/wiki/Tesla,_Inc.\",\n  \"Conversation_no\": 74,\n  \"Turn_no\": 2,\n  \"Conversation_source\": \"trec\"\n}\n```\n\n## Evaluation\n\n### Evaluate performance on Retrieval Question Answering task\n\nTo evaluate retrieval QA, use [evaluate_retrieval.py](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_retrieval.py)\n\n### Evaluate performance on Extractive Question Answering task\n\nTo evaluate extractive QA, use [evaluate_qa.py](https://github.com/apple/ml-qrecc/blob/main/utils/evaluate_qa.py)\n\n## Citation\n\nPlease cite the following if you found QReCC dataset, our [paper](https://arxiv.org/abs/2010.04898), or these resources useful.\n\n```bibtex\n@article{qrecc,\n  title={Open-Domain Question Answering Goes Conversational via Question Rewriting},\n  author={Anantha, Raviteja and Vakulenko, Svitlana and Tu, Zhucheng and Longpre, Shayne and Pulman, Stephen and Chappidi, Srinivas},\n  journal={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  year={2021}\n}\n```\n\n## License\n\nThe code in this repository is licensed according to the [LICENSE](LICENSE) file.\n\nThe QReCC dataset is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/.\n\n## Contact Us\n\nTo contact us feel free to email the authors in the paper or create an issue in this repository.\n"
 },
 {
  "repo": "apple/apple_rules_lint",
  "language": "Starlark",
  "readme_contents": "# apple_rules_lint\nA linting framework for Bazel\n\n## Users\n\nYou must load and configure the linting framework before anything else.\nThis is because later rulesets that depend on the linting framework will\nattempt to ensure that linters are configured by registering no-op \nimplmentations of lint configs. You can do this by: \n\n```py\n# WORKSPACE\n\nload(\"@apple_rules_lint//lint:repositories.bzl\", \"lint_deps\")\n\nlint_deps()\n\nload(\"@apple_rules_lint//lint:setup.bzl\", \"lint_setup\")\n\nlint_setup({\n    \"java-checkstyle\": \"//your:checkstyle-config\",\n})\n```\n\nYou may override specific lint configurations on a per-package basis by:\n\n```py\n# BUILD.bazel\n\nload(\"@apple_rules_lint//lint:defs.bzl\", \"package_lint_config\")\n\npackage_lint_config({\n    \"java-checkstyle\": \":alternative-checkstyle-config\",\n})\n```\n\n### API Documentation\n\nCan be found in [the api docs](api.md)\n\n## Ruleset Authors\n\nTo add linter support to your repo, add this to...\n\n```py\n# repositories.bzl\nload(\"@apple_rules_lint//lint:repositories.bzl\", \"lint_deps\")\n\nlint_deps()\n```\n\nThen add this to...\n\n```py\n# setup.bzl\nload(\"@apple_rules_lint//lint:setup.bzl\", \"ruleset_lint_setup\")\n\nruleset_lint_setup()\n```\n\nTo obtain the currently configured config for a ruleset, use:\n\n```py\n# your_rules.bzl\n\nload(\"@apple_rules_lint//lint:defs.bzl\", \"get_lint_config\")\n\nconfig = get_lint_config(\"java-checkstyle\", tags)\nif config != None:\n    # set up lint targets\n    pass\n```\n\nWhere `tags` are the tags of the rule to check.\n\n\n# Integrating `apple_rules_lint` With Your Rulesets\n\nFor the sake of this example, we'll show how `apple_rules_lint` is\nintegrated with the Selenium project, but the same process can be\nfollowed for any linter:\n\n1. Wrap the linter with a `_test` rule, so you can run them with bazel\n   test. In Selenium, this is the\n   [spotbugs_test](https://github.com/SeleniumHQ/selenium/blob/selenium-4.0.0-beta-1/java/private/spotbugs.bzl)\n\n2. Create a config rule or a marker rule of some sort. For example,\n   [spotbugs_config](https://github.com/SeleniumHQ/selenium/blob/selenium-4.0.0-beta-1/java/private/spotbugs_config.bzl)\n\n3. Pick a \"well known\" name: `lang-tool` seems to work well (such as\n   `java-spotbugs`, but you might have `go-gofmt` or `py-black`)\n\n4. Create a macro that uses\n   [get_lint_config](./api.md#get_lint_config) to look up the config\n   for you. If that's present, create a new instance of your test\n   rule. You can see this in action\n   [here](https://github.com/SeleniumHQ/selenium/blob/selenium-4.0.0-beta-1/java/private/library.bzl).\n\n5. As you write code, make sure your macro is called. If you're a\n   ruleset author, this can be as lightweight as exporting the macro created\n   above as the default way to call your rules.\n\n6. ...\n\n7. Profit!\n\nUsers can then use the \"well known\" name to point to an instance of\nthe config rule in their `WORKSPACE` files:\n\n```starlark\nlint_setup({\n    \"java-spotbugs\": \"//java:spotbugs-config\",\n})\n```\n"
 },
 {
  "repo": "apple/vqg-multimodal-assistant",
  "language": "Python",
  "readme_contents": "# vqg-multimodal-assistant\n\nCode was tested on python3.6\nPlease see requirements.txt for packages to be installed\n\nTODO: Download data from X to Y folder\n\n# Training\npython run.py -model_dir ./model -c ./config/training-config.yaml\n\n# Inference\npython run.py -model_dir ./model -c ./config/inference-config.yaml"
 },
 {
  "repo": "apple/ccs-pycalendar",
  "language": "Python",
  "readme_contents": "Getting Started\n===============\n\nThis is a python library for parsing and generating iCalendar data.\n\n\nCopyright and License\n=====================\n\nCopyright (c) 2005-2017 Apple Inc.  All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0.  The\nApache License is a well-established open source license, enabling\ncollaborative open source software development.\n\nSee the \"LICENSE\" file for the full text of the license terms.\n"
 },
 {
  "repo": "apple/learning-subspaces",
  "language": "Python",
  "readme_contents": "# [Learning Neural Network Subspaces](https://arxiv.org/abs/2102.10472)\n\nWelcome to the codebase for [Learning Neural Network Subspaces](https://arxiv.org/abs/2102.10472) by Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, Mohammad Rastegari.\n\n<p align=\"center\">\n<img src=\"fig/Figure1.png\" alt=\"Figure1\" width=\"639\" height=\"416\"/>\n</p>\n\n## Abstract\n*Recent observations have advanced our understanding of the neural network optimization landscape, revealing the existence of (1) paths of\nhigh accuracy containing diverse solutions and\n(2) wider minima offering improved performance.\nPrevious methods observing diverse paths require\nmultiple training runs. In contrast we aim to leverage both property (1) and (2) with a single method\nand in a single training run. With a similar computational cost as training one model, we learn lines,\ncurves, and simplexes of high-accuracy neural networks. These neural network subspaces contain\ndiverse solutions that can be ensembled, approaching the ensemble performance of independently\ntrained networks without the training cost. Moreover, using the subspace midpoint boosts accuracy, calibration, and robustness to label noise,\noutperforming Stochastic Weight Averaging.*\n\n## Code Overview\n\nIn this repository we walk through learning neural network subspaces with [PyTorch](https://pytorch.org/).\nWe will ground the discussion with learning a line of neural networks.\nIn our code, a line is defined by endpoints `weight` and `weight1` and a point on the line is given by\n`w = (1 - alpha) * weight + alpha * weight1` for some `alpha` in `[0,1]`.\n\nAlgorithm 1 (see [paper](https://arxiv.org/abs/2102.10472)) works as follows:\n1. `weight` and `weight1` are initialized independently.\n2. For each batch `data, targets`, `alpha` is chosen uniformly from `[0,1]` and the weights \n`w = (1 - alpha) * weight + alpha * weight1` are used in the forward pass.\n3. The regularization term is computed (see [Eq. 3](https://arxiv.org/abs/2102.10472)).\n4. With `loss.backward()` and `optimizer.step()` the endpoints `weight` and `weight1` are updated.\n\nInstead of using a regular `nn.Conv2d` we instead use a `SubspaceConv` (found in `modes/modules.py`).\n\n```python\nclass SubspaceConv(nn.Conv2d):\n    def forward(self, x):\n        w = self.get_weight()\n        x = F.conv2d(\n            x,\n            w,\n            self.bias,\n            self.stride,\n            self.padding,\n            self.dilation,\n            self.groups,\n        )\n        return x\n```\n\nFor each subspace type (lines, curves, and simplexes) the function `get_weight` must be implemented. For lines we use:\n\n```python\nclass TwoParamConv(SubspaceConv):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.weight1 = nn.Parameter(torch.zeros_like(self.weight))\n\n    def initialize(self, initialize_fn):\n        initialize_fn(self.weight1)\n\nclass LinesConv(TwoParamConv):\n    def get_weight(self):\n        w = (1 - self.alpha) * self.weight + self.alpha * self.weight1\n        return w\n```\n\nNote that the other endpoint `weight` is instantiated and initialized by `nn.Conv2d`. Also note that there is an\nequivalent implementation for batch norm layers also found in `modes/modules.py`.\n\nNow we turn to the training logic\nwhich appears in `trainers/train_one_dim_subspaces.py`. In the snippet below we assume we are not training with\nthe layerwise variant (`args.layerwise = False`) and we are drawing only one sample from the subspace\n(`args.num_samples = 1`). \n\n\n```python\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data, target = data.to(args.device), target.to(args.device)\n\n    alpha = np.random.uniform(0, 1)\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d):\n            setattr(m, f\"alpha\", alpha)\n\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n```\nAll that's left is to compute the regularization term and call backward. For lines, this is given by the snippet below.\n\n```python\n    num = 0.0\n    norm = 0.0\n    norm1 = 0.0\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            num += (self.weight * self.weight1).sum()\n            norm += self.weight.pow(2).sum()\n            norm1 += self.weight1.pow(2).sum()\n    loss += args.beta * (num.pow(2) / (norm * norm1))\n\n    loss.backward()\n\n    optimizer.step()\n```\n\n## Training Lines, Curves, and Simplexes\n\nWe now walkthrough generating the plots in Figures 4 and 5 of the [paper](https://arxiv.org/abs/2102.10472).\nBefore running code please install [PyTorch](https://pytorch.org/) and \n[Tensorboard](https://www.tensorflow.org/tensorboard) (for making plots\nyou will also need `tex` on your computer).\nNote that this repository differs from that used to generate the figures in the paper, as the latter leveraged\nApple's internal tools. Accordingly there may be some bugs and we encourage you to submit an issue or send an\nemail if you run into any problems.\n\nIn this example walkthrough we consider TinyImageNet, which we download to `~/data` using a script such as\n[this](https://gist.github.com/moskomule/2e6a9a463f50447beca4e64ab4699ac4). To run standard\ntraining and ensemble the trained models, use the following command:\n```bash\npython experiment_configs/tinyimagenet/ensembles/train_ensemble_members.py\npython experiment_configs/tinyimagenet/ensembles/eval_ensembles.py\n```\nNote that if your data is not in `~/data` please change the paths in these experiment configs.\nLogs and checkpoints be saved in `learning-subspaces-results`, although this path can also be changed.\n\nFor one dimensional\nsubspaces, use the following command to train:\n```bash\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/train_lines.py\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/train_lines_layerwise.py\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/train_curves.py\n```\nTo evaluate (*i.e.* generate the data for Figure 4) use:\n```bash\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/eval_lines.py\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/eval_lines_layerwise.py\npython experiment_configs/tinyimagenet/one_dimensional_subspaces/eval_curves.py\n```\nWe recommend looking at the experiment config files before running, which can be modified to change\nthe type of model, number of random seeds. The default in these configs is 2 random seeds.\n\nAnalogously, to train simplexes use:\n```bash\npython experiment_configs/tinyimagenet/simplexes/train_simplexes.py\npython experiment_configs/tinyimagenet/simplexes/train_simplexes_layerwise.py\n```\nFor generating plots like those in Figure 4 and 5 use:\n```bash\npython analyze_results/tinyimagenet/one_dimensional_subspaces.py\npython analyze_results/tinyimagenet/simplexes.py\n```\nEquivalent configs exist for other datasets, and the configs can be modified to add label noise,\nexperiment with other models, and more. Also, if there is any functionality missing from this repository that you\nwould like please also submit an issue.\n\n## Bibtex\n\n```text\n@article{wortsman2021learning,\n  title={Learning Neural Network Subspaces},\n  author={Wortsman, Mitchell and Horton, Maxwell and Guestrin, Carlos and Farhadi, Ali and Rastegari, Mohammad},\n  journal={arXiv preprint arXiv:2102.10472},\n  year={2021}\n}\n```"
 },
 {
  "repo": "apple/darwin-libplatform",
  "language": "C",
  "readme_contents": ""
 },
 {
  "repo": "apple/ml-quant",
  "language": "Python",
  "readme_contents": "# Quant\n\n<img src=\"quant_logo.png\" width=\"48\">\n\nThis repository is a PyTorch implementation of [Least Squares Binary Quantization of Neural Networks](http://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Pouransari_Least_Squares_Binary_Quantization_of_Neural_Networks_CVPRW_2020_paper.pdf) and can be used to reproduce the results in the paper.\n\n**The code is written to use Python 3.6 or above.**\n\n## Installation\n\nTo install Quant you first need to clone our repository.\n\nWe suggest you first create a virtual environment and install dependencies in the virtual environment.\n\n```bash\n# Go to repo\ncd <path/to/quant>\n# Create virtual environment ...\npython -m venv .venv\n# ... and activate it\nsource .venv/bin/activate\n# Upgrade to the latest versions of pip and wheel\npip install -U pip wheel\npip install -r requirements.txt\n```\n\nThen install quant with these commands:\n\n```bash\npip install flit\nflit install -s\n```\n\n## Quick Start\n\nTo run MNIST training on the local machine, do this:\n\n```bash\npython examples/mnist/mnist.py --config examples/mnist/mnist_fp.yaml --experiment-name mnist-fp\n```\n\nOne can also resume an existing experiment.\nFor example, here we restore an experiment trained locally on local.\nThe `--restore-experiment` argument points to the path of a previous experiment,\nand `--skip-training` means for the resumed job we would like to only perform evaluation (i.e., no training).\n\n```bash\npython examples/mnist/mnist.py --restore-experiment experiments/mnist-fp --skip-training\n```\n\nFor CIFAR-100 and ImageNet, the CLI interface is the same.\nSimply use the configs in the `examples/{mnist,cifar100,imagenet}/` directories.\n\n[mnist_fp.yaml](./examples/mnist/mnist_fp.yaml), [cifar100_fp.yaml](./examples/cifar100/cifar100_fp.yaml) and [imagenet_fp.yaml](./examples/imagenet/imagenet_fp.yaml)\ninclude comments that list configuration choices for some important parameters with references to documentation sections that explain them in more detail.\n\nAll experiments store the configurations used, overall metrics, checkpoints, and copy\nof TensorBoard logs in a directory with the experiment name.\nThe experiment name can be optionally specified using `--experiment-name <name>`.\nIf it is not specified, the current datetime with config name is used.\n\nThe experiments artifacts directory looks like this:\n\n```bash\n$ ls experiments/my_experiment_name/\ncheckpoints  config.yaml  metrics  tensorboard\n```\n\n## Experiment Results\n\n### CIFAR-100\n\nWe can first train a teacher using:\n\n```bash\npython examples/cifar100/cifar100.py --config examples/cifar100/cifar100_fp.yaml --experiment-name cifar100-teacher\n```\n\nThen, we can train a quantized student model using a teacher checkpoint in the experiments artifacts directory.\nThe student config has paths that point to the teacher config / checkpoint.\nIf you used the command above, the paths in the default config files should refer to the checkpoint you just trained:\n\n```yaml\nkd_config:\n    teacher_config_path: examples/cifar100/cifar100_fp.yaml\n    teacher_checkpoint_path: experiments/cifar100-teacher/checkpoints/checkpoint_200.pt\n```\n\nThen we can train a quantized student model, for example with 2-bits activation:\n\n```bash\npython examples/cifar100/cifar100.py --config examples/cifar100/cifar100_ls1_weight_ls2_activation_kd.yaml --experiment-name cifar100-ls2\n```\n\nAll configs ending with `*_kd.yaml` use Knowledge Distillation (KD) and require a pre-trained teacher checkpoint.\nIf you want to train without knowledge distillation, just remove the `kd_config` section from the corresponding config file.\n`cifar100_fp.yaml` is a config that does not have this `kd_config` section, for example.\n\nHere are the results we obtained using the configs in the `examples/cifar100` directory.\n\n| Config                                                                                                       | `k^a`  | `k^w`  | top-1 accuracy | top-5 accuracy |\n| ------------------------------------------------------------------------------------------------------------ |:------:|:------:|:--------------:|:--------------:|\n| [cifar100_ls1_kd.yaml](./examples/cifar100/cifar100_ls1_kd.yaml)                                             | 1      | 1      | 71.5           | 92.0           |\n| [cifar100_ls1_weight_lsT_activation_kd.yaml](./examples/cifar100/cifar100_ls1_weight_lsT_activation_kd.yaml) | T      | 1      | 73.5           | 92.8           |\n| [cifar100_ls1_weight_gf2_activation_kd.yaml](./examples/cifar100/cifar100_ls1_weight_gf2_activation_kd.yaml) | 2      | 1      | 74.3           | 93.1           |\n| [cifar100_ls1_weight_ls2_activation_kd.yaml](./examples/cifar100/cifar100_ls1_weight_ls2_activation_kd.yaml) | 2      | 1      | 74.4           | 92.9           |\n| [cifar100_ls1_weight_fp_activation_kd.yaml](./examples/cifar100/cifar100_ls1_weight_fp_activation_kd.yaml)   | 32     | 1      | 76.2           | 93.7           |\n| [cifar100_fp.yaml](./examples/cifar100/cifar100_fp.yaml)                                                     | 32     | 32     | 77.8           | 93.9           |\n\n### ImageNet\n\nThe configs in this repo for ImageNet use 8 GPUs.\nPlease adapt this setting as needed for your setup.\n\nWe can first train a teacher using:\n\n```bash\npython examples/imagenet/imagenet.py --config examples/imagenet/imagenet_fp.yaml --experiment-name imagenet-teacher\n```\n\nThen, we can train a quantized student model using a teacher checkpoint in the experiments artifacts directory.\nThe student config has paths that point to the teacher config / checkpoint.\nIf you used the command above, the paths in the default config files should refer to the checkpoint you just trained:\n\n```yaml\nkd_config:\n    teacher_config_path: examples/imagenet/imagenet_fp.yaml\n    teacher_checkpoint_path: experiments/imagenet-teacher/checkpoints/checkpoint_100.pt\n```\n\nThen we can train a quantized student model, for example with 2-bits activation:\n\n```bash\npython examples/imagenet/imagenet.py --config examples/imagenet/imagenet_ls1_weight_ls2_activation_kd.yaml --experiment-name imagenet-ls2\n```\n\nAll configs ending with `*_kd.yaml` use Knowledge Distillation (KD) and require a pre-trained teacher checkpoint.\nIf you want to train without knowledge distillation, just remove the `kd_config` section from the corresponding config file.\n`imagenet_fp.yaml` is a config that does not have this `kd_config` section, for example.\n\nHere are the results we obtained using the configs in the `examples/imagenet` directory.\nThese configs can be used to reproduce the results in the paper.\nThe `ls-2` 240 epochs job can take around 9 days, while the `ls-1` 240 epochs job can take around 6 days on 8 x NVIDIA Tesla V100 GPUs.\n\n| Config                                                                                                       | `k^a`  | `k^w`  | top-1 accuracy | top-5 accuracy |\n| ------------------------------------------------------------------------------------------------------------ |:------:|:------:|:--------------:|:--------------:|\n| [imagenet_ls1_kd.yaml](./examples/imagenet/imagenet_ls1_kd.yaml)                                             | 1      | 1      | 58.9           | 81.4           |\n| [imagenet_ls1_weight_lsT_activation_kd.yaml](./examples/imagenet/imagenet_ls1_weight_lsT_activation_kd.yaml) | T      | 1      | 62.0           | 83.6           |\n| [imagenet_ls1_weight_gf2_activation_kd.yaml](./examples/imagenet/imagenet_ls1_weight_gf2_activation_kd.yaml) | 2      | 1      | 62.6           | 84.0           |\n| [imagenet_ls1_weight_ls2_activation_kd.yaml](./examples/imagenet/imagenet_ls1_weight_ls2_activation_kd.yaml) | 2      | 1      | 63.4           | 84.6           |\n| [imagenet_ls1_weight_fp_activation_kd.yaml](./examples/imagenet/imagenet_ls1_weight_fp_activation_kd.yaml)   | 32     | 1      | 66.1           | 86.5           |\n| [imagenet_fp.yaml](./examples/imagenet/imagenet_fp.yaml)                                                     | 32     | 32     | 69.8           | 89.3           |\n\n## TensorBoard\n\nThe config files in `examples/` all have the TensorBoard server turned on by default.\nWhile training is running, you can go to [http://localhost:6006](http://localhost:6006) to view TensorBoard.\nIf the `TENSORBOARD_PORT` environment variable is set, it overrides the default port.\n\nBy default, TensorBoard logs are saved under `runs/` (configured via `tensorboard_root` in config files).\nYou can also run your own `tensorboard` instance pointing to this log directory if you do not want TensorBoard to terminate after training finishes.\nThe logs are copied to the experiment directory when a run finishes.\n\n## Tests\n\nTo run the tests, make sure you have followed the installation instructions and then run\nthe `pytest` from the root directory of this package. This will run all our tests,\nstatic analysis, coverage analysis and style checks.\n\n## Documentation\n\nTo build the docs you only need to make a directory adjacent to this repo in the parent directory and run the `make html` command.\n\n```bash\nmkdir -p ../quant-docs-build\ncd doc\nmake html\n```\n\n## Contact\n\n* **Hadi Pouransari**: mpouransari@apple.com\n* **Michael Tu**: zhucheng_tu@apple.com\n\n## Citation\n\n```bibtex\n@InProceedings{Pouransari_2020_CVPR_Workshops,\n    author = {Pouransari, Hadi and Tu, Zhucheng and Tuzel, Oncel},\n    title = {Least Squares Binary Quantization of Neural Networks},\n    booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n    month = {June},\n    year = {2020}\n}\n```\n"
 },
 {
  "repo": "apple/ml-equivariant-neural-rendering",
  "language": "Jupyter Notebook",
  "readme_contents": "\nREADME - ENR Data \n\nThe datasets accompanying the ICML 2020 paper Equivariant neural rendering are licensed as follows:\n\n\n1) The cars, chairs, and mugs-hq images are rendered from ShapeNet models (http://www.shapenet.org/), they are licensed under the shapenet license - https://www.shapenet.org/terms\n\n2) The mountains images are licensed under under the CC-BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/legalcode).\nSatellite imagery \u00a9 2020 Maxar Technologies\n\n\nAdditionally we thank Bernhard Vogl (salzamt@dativ.at) for the environmental map used to render the mugs-hq images.  This environmental map and others can be found at http://dativ.at/lightprobes. \n\nIf you find this code useful in your research, consider citing with:\n\n\n@article{dupont2020equivariant,\n  title={Equivariant Neural Rendering},\n  author={Dupont, Emilien and Miguel Angel, Bautista and Colburn, Alex and Sankar, Aditya and Guestrin, Carlos and Susskind, Josh and Shan, Qi},\n  journal={arXiv preprint arXiv:2006.07630},\n  year={2020}\n}\n\n\n\nShapeNet Terms of Use\n\nAfter registering for a ShapeNet account, you will be considered for account approval and privilege elevation by an administrator. After approval, you (the \"Researcher\") receive permission to use the *ShapeNet database* (the \"Database\") *at Princeton University and Stanford University*. In exchange for being able to join the ShapeNet community and receive such permission, Researcher hereby agrees to the following terms and conditions:\n\n1. Researcher shall use the Database only for non-commercial research and educational purposes.\n2. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n3. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify Princeton University and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted 3D models that he or she may create from the Database.\n4. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n5. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n7. The law of the State of New Jersey shall apply to all disputes under this agreement.\n\n"
 },
 {
  "repo": "apple/swift-nio-nghttp2-support",
  "language": "Swift",
  "readme_contents": "# swift-nio-nghttp2-support\n\nThis is a package just to be able to use nghttp2 with SwiftPM.\nTo use this package, add this to your `Package.swift` in `dependencies`:\n\n    .package(url: \"git@github.com:apple/swift-nio-nghttp2-support.git\", from: \"1.0.0\"),\n\n"
 },
 {
  "repo": "apple/swift-nio-ssl-support",
  "language": "Swift",
  "readme_contents": "# swift-nio-ssl-support\n\nThis is a package just to be able to use openssl with SwiftPM.\nTo use this package, add this to your `Package.swift` in `dependencies`:\n\n    .package(url: \"https://github.com/apple/swift-nio-ssl-support.git\", from: \"1.0.0\"),\n\n"
 },
 {
  "repo": "apple/swift-cluster-membership",
  "language": "Swift",
  "readme_contents": "# Swift Cluster Membership\n\nThis library aims to help Swift make ground in a new space: clustered multi-node distributed systems. \n\nWith this library we provide reusable runtime agnostic membership protocol implementations which can be adopted in various clustering use-cases.\n\n## Background\n\nCluster membership protocols are a crucial building block for distributed systems, such as computation intensive clusters, schedulers, databases, key-value stores and more. With the announcement of this package, we aim to make building such systems simpler, as they no longer need to rely on external services to handle service membership for them. We would also like to invite the community to collaborate on and develop additional membership protocols.\n\nAt their core, membership protocols need to provide an answer for the question \"Who are my (live) peers?\". This seemingly simple task turns out to be not so simple at all in a distributed system where delayed or lost messages, network partitions, and unresponsive but still \"alive\" nodes are the daily bread and butter. Providing a predictable, reliable answer to this question is what cluster membership protocols do.\n\nThere are various trade-offs one can take while implementing a membership protocol, and it continues to be an interesting area of research and continued refinement. As such, the cluster-membership package intends to focus not on a single implementation, but serve as a collaboration space for various distributed algorithms in this space.\n\n## \ud83c\udfca\ud83c\udffe\u200d\u2640\ufe0f\ud83c\udfca\ud83c\udffb\u200d\u2640\ufe0f\ud83c\udfca\ud83c\udffe\u200d\u2642\ufe0f\ud83c\udfca\ud83c\udffc\u200d\u2642\ufe0f SWIMming with Swift\n\n### High-level Protocol Description\n\n> For a more in-depth discussion of the protocol and modifications in this implementation we suggest reading the [SWIM API Documentation](https://apple.github.io/swift-cluster-membership/docs/current/SWIM/Enums/SWIM.html), as well as the associated papers linked below.\n\nThe [*Scalable Weakly-consistent Infection-style process group Membership*](https://research.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf) algorithm (also known as \"SWIM\"), along with a few notable protocol extensions as documented in the 2018 [*Lifeguard: Local Health Awareness for More Accurate Failure Detection*](https://arxiv.org/abs/1707.00788) paper.\n\nSWIM is a [gossip protocol](https://en.wikipedia.org/wiki/Gossip_protocol) in which peers periodically exchange bits of information about their observations of other nodes\u2019 statuses, eventually spreading the information to all other members in a cluster. This category of distributed algorithms are very resilient against arbitrary message loss, network partitions and similar issues.\n\nAt a high level, SWIM works like this: \n\n* A member periodically pings a \"randomly\" selected peer it is aware of. It does so by sending a .ping message to that peer, expecting an [`.ack`](https://apple.github.io/swift-cluster-membership/docs/current/SWIM/Protocols/SWIMPingOriginPeer.html#/s:4SWIM18SWIMPingOriginPeerP3ack13acknowledging6target11incarnation7payloadys6UInt32V_AA8SWIMPeer_ps6UInt64VA2AO13GossipPayloadOtF) to be sent back. See how `A` probes `B` initially in the diagram below.\n    * The exchanged messages also carry a gossip `payload`, which is (partial) information about what other peers the sender of the message is aware of, along with their membership status (`.alive`, `.suspect`, etc.)\n* If it receives an `.ack`, the peer is considered still `.alive`. Otherwise, the target peer might have terminated/crashed or is unresponsive for other reasons. \n    * In order to double check if the peer really is dead, the origin asks a few other peers about the state of the unresponsive peer by sending `.pingRequest` messages to a configured number of other peers, which then issue direct pings to that peer (probing peer E in the diagram below).\n* If those pings fail, due to lack of .acks resulting in the peer being marked as `.suspect`,\n    * Our protocol implementation will also use additional `.nack` (\"negative acknowledgement\") messages in the situation to inform the ping request origin that the intermediary did receive those `.pingRequest` messages, however the target seems to not have responded. We use this information to adjust a Local Health Multiplier, which affects how timeouts are calculated. To learn more about this refer to the API docs and the Lifeguard paper.\n\n![SWIM: Messages Examples](images/ping_pingreq_cycle.svg)\n\nThe above mechanism, serves not only as a failure detection mechanism, but also as a gossip mechanism, which carries information about known members of the cluster. This way members eventually learn about the status of their peers, even without having them all listed upfront. It is worth pointing out however that this membership view is [weakly-consistent](https://en.wikipedia.org/wiki/Weak_consistency), which means there is no guarantee (or way to know, without additional information) if all members have the same exact view on the membership at any given point in time. However, it is an excellent building block for higher-level tools and systems to build their stronger guarantees on top.\n\nOnce the failure detection mechanism detects an unresponsive node, it eventually is marked as  .dead resulting in its irrevocable removal from the cluster. Our implementation offers an optional extension, adding an .unreachable state to the possible states, however most users will not find it necessary and it is disabled by default. For details and rules rules about legal status transitions refer to [SWIM.Status](https://github.com/apple/swift-cluster-membership/blob/main/Sources/SWIM/Status.swift#L18-L39) or the following diagram:\n\n![SWIM: Lifecycle Diagram](images/swim_lifecycle.svg)\n\nThe way Swift Cluster Membership implements protocols, is by offering \"`Instances`\" of them. For example, the SWIM implementation is encapsulated in the runtime agnostic [`SWIM.Instance`](https://github.com/apple/swift-cluster-membership/blob/main/Sources/SWIM/SWIMInstance.swift) which needs to be \u201cdriven\u201d or \u201cinterpreted\u201d by some glue code between a networking runtime and the instance itself. We call those glue pieces of an implementation \"`Shell`s\", and the library ships with a `SWIMNIOShell` implemented using [SwiftNIO](https://www.github.com/apple/swift-nio)\u2019s `DatagramChannel` that performs all messaging asynchronously over [UDP](https://searchnetworking.techtarget.com/definition/UDP-User-Datagram-Protocol). Alternative implementations can use completely different transports, or piggy back SWIM messages on some other existing gossip system etc.\n\nThe SWIM instance also has built-in support for emitting metrics (using [swift-metrics](https://github.com/apple/swift-metrics)) and can be configured to log details about internal details by passing a [swift-log](https://github.com/apple/swift-log) `Logger`.\n\n### Example: Reusing the SWIM protocol logic implementation\n\nThe primary purpose of this library is to share the `SWIM.Instance` implementation across various implementations which need some form of in-process membership service. Implementing a custom runtime is documented in depth in the project\u2019s README (https://github.com/apple/swift-cluster-membership/), so please have a look there if you are interested in implementing SWIM over some different transport.\n\nImplementing a new transport boils down a \u201cfill in the blanks\u201d exercise: \n\nFirst, one has to implement the Peer protocols (https://github.com/apple/swift-cluster-membership/blob/main/Sources/SWIM/Peer.swift) using one\u2019s target transport:\n\n```swift\n/// SWIM peer which can be initiated contact with, by sending ping or ping request messages.\npublic protocol SWIMPeer: SWIMAddressablePeer {\n    /// Perform a probe of this peer by sending a `ping` message.\n    /// \n    /// <... more docs here - please refer to the API docs for the latest version ...>\n    func ping(\n        payload: SWIM.GossipPayload,\n        from origin: SWIMPingOriginPeer,\n        timeout: DispatchTimeInterval,\n        sequenceNumber: SWIM.SequenceNumber,\n        onResponse: @escaping (Result<SWIM.PingResponse, Error>) -> Void\n    )\n    \n    // ... \n}\n```\n\nWhich usually means wrapping some connection, channel, or other identity with the ability to send messages and invoke the appropriate callbacks when applicable. \n\nThen, on the receiving end of a peer, one has to implement receiving those messages and invoke all the corresponding `on<SomeMessage>(...)` callbacks defined on the `SWIM.Instance` (grouped under [SWIMProtocol](https://github.com/apple/swift-cluster-membership/blob/main/Sources/SWIM/SWIMInstance.swift#L24-L85)).\n\nA piece of the SWIMProtocol is liste below to give you an idea about it:\n\n\n```swift\npublic protocol SWIMProtocol {\n\n    /// MUST be invoked periodically, in intervals of `self.swim.dynamicLHMProtocolInterval`.\n    ///\n    /// MUST NOT be scheduled using a \"repeated\" task/timer\", as the interval is dynamic and may change as the algorithm proceeds.\n    /// Implementations should schedule each next tick by handling the returned directive's `scheduleNextTick` case,\n    /// which includes the appropriate delay to use for the next protocol tick.\n    ///\n    /// This is the heart of the protocol, as each tick corresponds to a \"protocol period\" in which:\n    /// - suspect members are checked if they're overdue and should become `.unreachable` or `.dead`,\n    /// - decisions are made to `.ping` a random peer for fault detection,\n    /// - and some internal house keeping is performed.\n    ///\n    /// Note: This means that effectively all decisions are made in interval sof protocol periods.\n    /// It would be possible to have a secondary periodic or more ad-hoc interval to speed up\n    /// some operations, however this is currently not implemented and the protocol follows the fairly\n    /// standard mode of simply carrying payloads in periodic ping messages.\n    ///\n    /// - Returns: `SWIM.Instance.PeriodicPingTickDirective` which must be interpreted by a shell implementation\n    mutating func onPeriodicPingTick() -> [SWIM.Instance.PeriodicPingTickDirective]\n\n    mutating func onPing( ... ) -> [SWIM.Instance.PingDirective]\n\n    mutating func onPingRequest( ... ) -> [SWIM.Instance.PingRequestDirective]\n\n    mutating func onPingResponse( ... ) -> [SWIM.Instance.PingResponseDirective]\n\n    // ... \n}\n```\n\nThese calls perform all SWIM protocol specific tasks internally, and return directives which are simple to interpret \u201ccommands\u201d to an implementation about how it should react to the message. For example, upon receiving a `.pingRequest` message, the returned directive may instruct a shell to send a ping to some nodes. The directive prepares all apropriate target, timeout and additional information that makes it simpler to simply follow its instruction and implement the call correctly, e.g. like this:\n\n```swift\nself.swim.onPingRequest(\n    target: target,\n    pingRequestOrigin: pingRequestOrigin,            \n    payload: payload,\n    sequenceNumber: sequenceNumber\n).forEach { directive in\n    switch directive {\n    case .gossipProcessed(let gossipDirective):\n        self.handleGossipPayloadProcessedDirective(gossipDirective)\n\n    case .sendPing(let target, let payload, let pingRequestOriginPeer, let pingRequestSequenceNumber, let timeout, let sequenceNumber):\n        self.sendPing(\n            to: target,\n            payload: payload,\n            pingRequestOrigin: pingRequestOriginPeer,\n            pingRequestSequenceNumber: pingRequestSequenceNumber,\n            timeout: timeout,\n            sequenceNumber: sequenceNumber\n        )\n    }\n}\n```\n\nIn general this allows for all the tricky \"what to do when\" to be encapsulated within the protocol instance, and a Shell only has to follow instructions implementing them. The actual implementations will often need to perform some more involved concurrency and networking thasks, like awaiting for a sequence of responses, and handling them in a specific way etc, however the general outline of the protocol is orchestrated by the instance's directives.\n\nFor detailed documentation about each of the callbacks, when to invoke them, and how all this fits together, please refer to the [**API Documentation**](https://apple.github.io/swift-cluster-membership/docs/current/SWIM/index.html).\n\n### Example: SWIMming with Swift NIO\n\nThe repository contains an [end-to-end example](Samples/Sources/SWIMNIOSampleCluster) and an example implementation called [SWIMNIOExample](Sources/SWIMNIOExample) which makes use of the `SWIM.Instance` to enable a simple UDP based peer monitoring system. This allows peers to gossip and notify each other about node failures using the SWIM protocol by sending datagrams driven by SwiftNIO.\n\n> \ud83d\udcd8 The `SWIMNIOExample` implementation is offered only as an example, and has not been implemented with production use in mind, however with some amount of effort it could definitely do well for some use-cases. If you are interested in learning more about cluster membership algorithms, scalability benchmarking and using SwiftNIO itself, this is a great module to get your feet wet, and perhaps once the module is mature enough we could consider making it not only an example, but a reusable component for Swift NIO based clustered applications.\n\nIn it\u2019s simplest form, combining the provided SWIM instance and NIO shell to build a simple server, one can embedd the provided handlers like shown below, in a typical NIO channel pipeline:\n\n```swift\nlet bootstrap = DatagramBootstrap(group: group)\n    .channelOption(ChannelOptions.socketOption(.so_reuseaddr), value: 1)\n    .channelInitializer { channel in\n        channel.pipeline\n            // first install the SWIM handler, which contains the SWIMNIOShell:\n            .addHandler(SWIMNIOHandler(settings: settings)).flatMap {\n                // then install some user handler, it will receive SWIM events:\n                channel.pipeline.addHandler(SWIMNIOExampleHandler())\n        }\n    }\n\nbootstrap.bind(host: host, port: port)\n```\n\nThe example handler can then receive and handle SWIM cluster membership change events:\n\n```swift\nfinal class SWIMNIOExampleHandler: ChannelInboundHandler {\n    public typealias InboundIn = SWIM.MemberStatusChangedEvent\n    \n    let log = Logger(label: \"SWIMNIOExampleHandler\")\n    \n    public func channelRead(context: ChannelHandlerContext, data: NIOAny) {\n        let change: SWIM.MemberStatusChangedEvent = self.unwrapInboundIn(data)\n\n        self.log.info(\"Membership status changed: [\\(change.member.node)] is now [\\(change.status)]\", metadata: [    \n            \"swim/member\": \"\\(change.member.node)\",\n            \"swim/member/status\": \"\\(change.status)\",\n        ])\n    }\n}\n```\n\nIf you are interested in contributing and polishing up the SWIMNIO implementation please head over to the issues and pick up a task or propose an improvement yourself!\n\n## Additional Membership Protocol Implementations\n\nWe are generally interested in fostering discussions and implementations of additional membership implementations using a similar \"Instance\" style.\n\nIf you are interested in such algorithms, and have a favourite protocol that you'd like to see implemented, please do not hesitate to reach out heve via issues or the [Swift forums](https://forums.swift.org/c/server).\n\n## Contributing\n\nExperience reports, feedback, improvement ideas and contributions are greatly encouraged! \nWe look forward to hear from you.\n\nPlease refer to [CONTRIBUTING](CONTRIBUTING.md) guide to learn about the process of submitting pull requests,\nand refer to the [HANDBOOK](HANDBOOK.md) for terminology and other useful tips for working with this library.\n\n\n"
 },
 {
  "repo": "apple/ml-tree-dst",
  "language": "Python",
  "readme_contents": "# Conversational Semantic Parsing for Dialog State Tracking\n\nWe introduce TreeDST (**T**ree-based **D**ialog **S**tate **T**racking), a multi-turn, multi-domain task-oriented dialog dataset annotated with tree-based user dialog states and system dialog acts. The goal of this dataset is to provide a novel solution for end-to-end dialog state tracking as a conversational semantic parsing task. Please refer to our paper for [Conversational Semantic Parsing for Dialog State Tracking](https://arxiv.org/pdf/2010.12770.pdf) for details.\n\n## Task Description\n\nThe task in TreeDST is to predict the user dialog state for each turn of a conversation. The dialog state is a representation of the user's goal up to the current turn of the conversation.\n\n## Dataset Description\n\nThe dataset contains 27,280 conversations covering 10 domains with shared types of person, time and location. The dataset and schema can be accessed in the [dataset](dataset) folder. A tool for visualizing the data is in the \"dotted\" format is provided at [utils](utils) folder.\n\n\n## License\n\nThe code in this repository is licensed according to the [LICENSE](LICENSE) file.\n\nThe TreeDST dataset is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License.\nTo view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/.\n\n## Declaration\n\nThis dataset contains computer-generated synthetic conversation flows that has been human annotated to provide richer and more natural dialogue. **This dataset does not include any data from Siri production users**.\n"
 },
 {
  "repo": "apple/ml-collegial-ensembles",
  "language": "Python",
  "readme_contents": "# Collegial Ensembles\n\nThis software project accompanies the research paper, [Collegial Ensembles](https://arxiv.org/abs/2006.07678).\n\n\nCollegial Ensembles can be used to analytically derive optimal group convolution modules without having to train a single model.\n- Primal optimum: Optimal model with the same budget of parameters.\n- Dual optimum: Equivalent model with minimal number of parameters.\n\n\n## Reproducing Results on CIFAR\n\nTo reproduce the baselines on CIFAR-10:\n\n- `python train.py -d cifar10 --cardinality 1 --base-width 128 --num-gpus 8`\n\n- `python train.py -d cifar10 --cardinality 3 --base-width 64 --num-gpus 8`\n\nTo reproduce the primal optimum:\n\n- `python train.py -d cifar10 --cardinality 37 --base-width 10 --num-gpus 8`\n\nAnd the dual optimum:\n\n- `python train.py -d cifar10 --cardinality 10 --base-width 10 --num-gpus 8`\n\nReplace `cifar10` by `cifar100` to obtain results for CIFAR-100.\n\n\n## Reproducing Results on Downsampled ImageNet\n\nTo reproduce the baselines on ImageNet32x32:\n\n- `python train.py -d imagenet32 --train-path TRAIN_PATH --val-path VAL_PATH --cardinality 1 --base-width 128 --num-gpus 8`\n\n- `python train.py -d imagenet32 --train-path TRAIN_PATH --val-path VAL_PATH --cardinality 3 --base-width 64 --num-gpus 8`\n\nwhere `TRAIN_PATH` and `VAL_PATH` are the paths to the downsampled ImageNet zip files available at http://image-net.org/download-images.\n\nTo reproduce the primal optimum:\n\n- `python train.py -d imagenet32 --train-path TRAIN_PATH --val-path VAL_PATH --cardinality 37 --base-width 10 --num-gpus 8`\n\nAnd the dual optimum:\n\n- `python train.py -d imagenet32 --train-path TRAIN_PATH --val-path VAL_PATH --cardinality 10 --base-width 10 --num-gpus 8`\n\nReplace `imagenet32` by `imagenet64` to obtain results for ImageNet64x64\n\n## Reproducing Results on ImageNet\n\n\nTo reproduce the ResNet-50 baselines on ImageNet:\n\n- `python train.py -d imagenet --train-path TRAIN_PATH --val-path VAL_PATH --depth 50 --cardinality 1 --base-width 64 --num-gpus 8`\n\n- `python train.py -d imagenet --train-path TRAIN_PATH --val-path VAL_PATH --depth 50 --cardinality 32 --base-width 4 --num-gpus 8`\n\nTo reproduce the primal optimum:\n\n- `python train.py -d imagenet --train-path TRAIN_PATH --val-path VAL_PATH --depth 50 --cardinality 12 --base-width 10 --num-gpus 8`\n\nAnd the dual results:\n\n- `python train.py -d imagenet --train-path TRAIN_PATH --val-path VAL_PATH --depth 50 --cardinality 4 --base-width 16 --num-gpus 8`\n\nTo get results for ResNeXt-101, replace `--depth 50` with `--depth 101`.\n\n\n## Dependencies\n\n- python 3.6\n- pytorch 1.5.0\n- torchvision 0.6.0\n\n## Credit\n\nThe implementation of ResNeXt architecture is based on:\n\n- https://github.com/facebookresearch/ResNeXt\n- https://github.com/prlz77/ResNeXt.pytorch\n"
 },
 {
  "repo": "apple/swiftpm-on-llbuild2",
  "language": "Swift",
  "readme_contents": "# swiftpm-on-llbuild2\n\nThis repository will contain SwiftPM build rules written on top of the experimental [llbuild2](https://github.com/apple/swift-llbuild2) project.\n\n## License\n\nCopyright (c) 2020 Apple Inc. and the Swift project authors.\nLicensed under Apache License v2.0 with Runtime Library Exception.\n\nSee http://swift.org/LICENSE.txt for license information.\n\nSee http://swift.org/CONTRIBUTORS.txt for Swift project authors.\n"
 },
 {
  "repo": "apple/ml-transcript-translation-consistency-ratings",
  "language": null,
  "readme_contents": "# Human Ratings of Transcription/Translation Consistency\n\nThis repository accompanies the research paper [Consistent Transcription and Translation of Speech](https://arxiv.org/abs/2007.12741).\n\nIt contains transcript/translation consistency ratings for various system outputs as well as for the original references contained in [MuST-C](https://ict.fbk.eu/must-c/)'s English-German testset.\n\n## Documentation\n\n For each of the 2640 test set utterances, we consider consistency of the original reference transcript/translation, as well as between system outputs for 7 systems described in the paper. For each utterance/system combination, we collect 3 (in some cases 4) annotations, for a total of 63412 annotations. The data is spread among two CSV files, with `sys-consistency-ratings.csv` containing consistency ratings for system outputs, and `ref-consistency-ratings.csv` containing consistency ratings for the  reference transcripts/translations. \n\nThe CSV table contains the following fields:\n- `output_idx` is a unique identifier for each utterance/system combination.\n- `sent_idx` is the index of the utterance in the original MuST-C test set.\n- `model` takes one of 8 values: `casc`,`dirind`,`dirmu`,`dirsh`,`2st`,`tri`,`concat` indicates that this row contains the respective model outputs (see paper). [`sys-consistency-ratings.csv` only]\n- `sys_english` is the automatic English transcript [`sys-consistency-ratings.csv` only]\n- `sys_german` is the automatic German translation [`sys-consistency-ratings.csv` only]\n- `display_english_first`: The annotation UI displayed English transcript and German translation in random order. This field is set to `1` whenever the English was displayed first.\n- `rater_idx`: a unique, anonymous identifier for the human evaluator.\n- `rating`: the rating itself, an integer value between 1 and 4.\n- `rater_comment`: Raters were allowed to optionally explain their choice of rating in free text.\n- `adjusted_rating`: The rating, after running the random mixed effects model as described in the paper. This rating is thus adjusted for rater bias and utterance difficulty.\n\n## Annotation Process\n\nAnnotators were first asked to familiarize themselves with the annotation guidelines, which can be found under `guidelines.pdf`. These guidelines expand upon the definition presented in the paper and add several examples for each possible score. Annotators were then asked to `Assign a score that evaluates how \"CONSISTENT\" the above English and German sentences are. Please carefully read the guidelines and examples before answering.`\n\nFour choices were presented:\n- 4: OK (near-perfect consistency). Good match of meaning, both sides similarly grammatical or ungrammatical. Minor mismatch in grammaticality or fluency allowed.\n- 3: close (minor consistency problems). Meaning consistent, but some mismatch in style, spelling, or sentence structure.\n- 2: bad (less consistent). Major mismatch in *meaning* for parts of the sentences\n- 1: catastrophic (not at all consistent). Both sides have little in common\n\n## License\n\nThis work is licensed under a [Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license](https://creativecommons.org/licenses/by-nc/4.0/).\n\n## Citation\n\nIf you use this data in your project, please consider citing our paper:\n\n*Matthias Sperber, Hendra Setiawan, Christian Gollan, Udhyakumar Nallasamy, Matthias Paulik (2020).* [Consistent Transcription and Translation of Speech](https://arxiv.org/pdf/2007.12741.pdf). Transactions of the Association for Computational Linguistics (TACL).\n"
 },
 {
  "repo": "apple/ml-mkqa",
  "language": "Python",
  "readme_contents": "# MKQA: Multilingual Knowledge Questions & Answers\n\n[**Tasks**](#task-description) | [**Dataset**](#dataset) | [**Leaderboard**](#leaderboard) | [**Evaluation**](#evaluation) |\n[**Paper**](https://arxiv.org/abs/2007.15207) |\n[**Citation**](#citation) | [**License**](#license)\n\nWe introduce MKQA, an open-domain question answering evaluation set comprising 10k question-answer pairs \naligned across 26 typologically diverse languages (260k question-answer pairs in total).\nThe goal of this dataset is to provide a challenging benchmark for question answering quality across a wide set of\nlanguages. Please refer to our paper for details, [MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering](https://arxiv.org/abs/2007.15207)\n\n\n## Task Description\nGiven a question <code>q<sup>l</sup></code> in language `l`, the task is to produce a prediction <code>p<sup>l</sup></code> in {No Answer, Yes, No, Text Answer}, \nwhere a Text Answer is a span of tokens in the corresponding language. \n<code>p<sup>l</sup></code> can be obtained by any method, extracted from a document, generated, or derived from a knowledge graph.\nWherever possible, textual answers are accompanied by Wikidata QIDs, for entity linking and evaluating knowledge graph approaches. \nThese QIDs also enable automatic translations for most answers into any Wikipedia language through the Wikidata knowledge graph.\n\n\n## Dataset\nMKQA contains 10,000 queries sampled from the [Google Natural Questions dataset](https://github.com/google-research-datasets/natural-questions).  \n\nFor each query we collect new passage-independent answers. \nThese queries and answers are then human translated into 25 Non-English languages.\nMKQA data can be downloaded from [here](dataset/mkqa.jsonl.gz).\n\nEach example in the dataset contains the unique Natural Questions `example_id`, the original English `query`, and then `queries` and `answers` in 26 languages.\n\n```\n{\n 'example_id': 563260143484355911,\n 'queries': {\n  'en': \"who sings i hear you knocking but you can't come in\",\n  'ru': \"\u043a\u0442\u043e \u043f\u043e\u0435\u0442 i hear you knocking but you can't come in\",\n  'ja': '\u300c I hear you knocking\u300d\u306f\u8ab0\u304c\u6b4c\u3063\u3066\u3044\u307e\u3059\u304b',\n  'zh_cn': \"\u300ai hear you knocking but you can't come in\u300b\u662f\u8c01\u6f14\u5531\u7684\",\n  ...\n },\n 'query': \"who sings i hear you knocking but you can't come in\",\n 'answers': {'en': [{'type': 'entity',\n    'entity': 'Q545186',\n    'text': 'Dave Edmunds',\n    'aliases': []}],\n  'ru': [{'type': 'entity',\n    'entity': 'Q545186',\n    'text': '\u042d\u0434\u043c\u0443\u043d\u0434\u0441, \u0414\u044d\u0439\u0432',\n    'aliases': ['\u042d\u0434\u043c\u0443\u043d\u0434\u0441', '\u0414\u044d\u0439\u0432 \u042d\u0434\u043c\u0443\u043d\u0434\u0441', '\u042d\u0434\u043c\u0443\u043d\u0434\u0441 \u0414\u044d\u0439\u0432', 'Dave Edmunds']}],\n  'ja': [{'type': 'entity',\n    'entity': 'Q545186',\n    'text': '\u30c7\u30a4\u30f4\u30fb\u30a8\u30c9\u30e2\u30f3\u30ba',\n    'aliases': ['\u30c7\u30fc\u30d6\u30fb\u30a8\u30c9\u30e2\u30f3\u30ba', '\u30c7\u30a4\u30d6\u30fb\u30a8\u30c9\u30e2\u30f3\u30ba']}],\n  'zh_cn': [{'type': 'entity', 'text': '\u6234\u7ef4\u00b7\u57c3\u5fb7\u8499\u5179 ', 'entity': 'Q545186'}],\n  ...\n  },\n}\n```\nEach answer is labelled with an answer type. The breakdown is:\n\n| Answer Type | Occurrence |\n|---------------|---------------|\n| `entity`               | `4221`             |\n| `long_answer`          | `1815`             |\n| `unanswerable`         | `1427`             |\n| `date`                 | `1174`             |\n| `number`               | `485`              |\n| `number_with_unit`     | `394`              |\n| `short_phrase`         | `346`              |\n| `binary`               | `138`              |\n  \nFor each language, there can be more than one acceptable textual answer, in order to capture a variety of possible valid answers. \nAll the supported languages are:  \n\n| Language code | Language name |\n|---------------|---------------|\n| `ar`     | `Arabic`                    |\n| `da`     | `Danish`                    |\n| `de`     | `German`                    |\n| `en`     | `English`                   |\n| `es`     | `Spanish`                   |\n| `fi`     | `Finnish`                   |\n| `fr`     | `French`                    |\n| `he`     | `Hebrew`                    |\n| `hu`     | `Hungarian`                 |\n| `it`     | `Italian`                   |\n| `ja`     | `Japanese`                  |\n| `ko`     | `Korean`                    |\n| `km`     | `Khmer`                    |\n| `ms`     | `Malay`                     |\n| `nl`     | `Dutch`                     |\n| `no`     | `Norwegian`                 |\n| `pl`     | `Polish`                    |\n| `pt`     | `Portuguese`                |\n| `ru`     | `Russian`                   |\n| `sv`     | `Swedish`                   |\n| `th`     | `Thai`                      |\n| `tr`     | `Turkish`                   |\n| `vi`     | `Vietnamese`                |\n| `zh_cn`     | `Chinese (Simplified)`   |\n| `zh_hk`     | `Chinese (Hong kong)`    |\n| `zh_tw`     | `Chinese (Traditional)`  |\n\n\n## Leaderboard\n| Model name | Best Overall F1 | Best Answerable F1 | Best Unnswerable F1 | Link to paper |\n|---------------|---------------|---------------|---------------|---------------|\n| (Baseline) XLM-R Large Translate Train   |46.0        | 27.6        |84.5        |https://arxiv.org/pdf/1911.02116.pdf        |\n\n> Submit a pull request to this repository to add yourself to the MKQA leaderboard. Scores are ordered by Best Overall F1.\n\n## Evaluation\nThe official evaluation scripts provide two ways to evaluate performance on the MKQA dataset\n\nThe evaluation script expects a json lines (jsonl) prediction file with a specific format:\n```\n{\n  \"example_id\": -7449157003522518870,\n  \"prediction\": \"Haf\u00fe\u00f3r J\u00fal\u00edus `` Thor '' Bj\u00f6rnsson\",\n  \"binary_answer\": null,\n  \"no_answer_prob\": 0.23618\n}\n...\n```\n\n### Evaluate performance for single language \nTo evaluate prediscion for a single language, use `mkqa_eval.py` script and indicate prediction language\n```\npython mkqa_eval.py --annotation_file ./dataset/mkqa.jsonl.gz \\\n --predictions_file ./sample_predictions/en.jsonl \\\n --language en \\\n (--out_dir <optional output directory for saving metrics and pr curves>, --verbose) \n```\n\n### Evaluate performances for all languages\nTo evaluate predictions for all languages, use the following script. Save the prediction file for each language in the same directory and name each prediction file by its language code, such as `en.jsonl`\n```\npython mkqa_eval_all_languages.py --annotation_file ./dataset/mkqa.jsonl.gz \\\n --predictions_dir ./sample_predictions \\\n (--out_dir <optional output directory for saving metrics and pr curves>, --verbose)\n``` \n\nTo get our the zero shot multilingual bert baseline, use the provided prediction jsonl files [here][./sample_predictions] \nSample output:\n\n```\n+---------------+-----------+-----------+----------------------+----------------------+------------------------+---------------------+\n| language      |   best_em |   best_f1 |   best_answerable_em |   best_answerable_f1 |   best_unanswerable_em |   best_f1_threshold |\n|---------------+-----------+-----------+----------------------+----------------------+------------------------+---------------------|\n| en            |     45.39 |     51.97 |                26.65 |                36.38 |                  84.45 |               -5.95 |\n| es            |     39.73 |     43.83 |                16.69 |                22.76 |                  87.75 |               -2.52 |\n| zh_cn         |     32.42 |     32.43 |                 0    |                 0.01 |                  100   |              -10.53 |\n| Macro Average |     39.18 |     42.74 |                14.45 |                19.72 |                  90.73 |               -6.33 |\n+---------------+-----------+-----------+----------------------+----------------------+------------------------+---------------------+\n```  \n> Here it computes the macro average over the languages for which prediction files were supplied for.\nThe official macro-average requires all 26 prediction files to be included.\n\nTo run tests for the evaluation scripts\n```\npytest ./tests/test_mkqa_eval.py\n```\n\n\n## Citation\nPlease cite the following if you found MKQA, our [paper](https://arxiv.org/abs/2007.15207), or these resources useful.\n```\n@misc{mkqa,\n    title = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},\n    author = {Shayne Longpre and Yi Lu and Joachim Daiber},\n    year = {2020},\n    URL = {https://arxiv.org/pdf/2007.15207.pdf}\n}\n```\n\n## License\nThe code in this repository is licensed according to the [LICENSE](LICENSE) file.\n\n\nThe Multilingual Knowledge Questions and Answers dataset is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/  \n\n\n## Contact Us\nTo contact us feel free to email the authors in the paper or create an issue in this repository.\n"
 },
 {
  "repo": "apple/ml-dab",
  "language": "Python",
  "readme_contents": "# DAB: Differentiable Approximation Bridges\n\nA simplified example demonstrating a DAB network presented in [Improving Discrete Latent Representations With Differentiable Approximation Bridges](https://arxiv.org/abs/1905.03658).\n\n### Usage\n\nThe only dependency for this demo is [pytorch](https://pytorch.org/get-started/locally/).  \nTo run the 10-sort signum-dense problem described in section 4.1 of the [paper](https://arxiv.org/abs/1905.03658) simply run:\n\n```python\npython main.py\n```\n\nThis should result in the following which corroborates the paper\u2019s result of 94.2% :\n\n```bash\ntrain[Epoch 2168][1999872.0 samples][7.79 sec]: Loss: 79.2356   DABLoss: 7.9058 Accuracy: 95.5683\n\u2026\ntest[Epoch 2168][399360.0 samples][0.91 sec]: Loss: 79.2329     DABLoss: 7.9012 Accuracy: 94.6424\n```\n\n### Create a DAB for a custom non-differentiable function\n\n  1. Create a suitable approximation neural network.\n  2. Implement custom hard function similar to SignumWithMargin in models/dab.py .\n  3. Stack a DAB module in your neural network pipeline.\n  4. Add DAB loss to normal loss.\n\n\n### Cite\n\n```\n@article{\n  dabimprovingdiscreterepr2020,\n  title={Improving Discrete Latent Representations With Differentiable Approximation Bridges},\n  author={Ramapuram, Jason and Webb, Russ},\n  journal={IEEE WCCI},\n  year={2020}\n}\n"
 },
 {
  "repo": "apple/ml-capsules-inverted-attention-routing",
  "language": "Python",
  "readme_contents": "![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)  \n\n# Capsules with Inverted Dot-Product Attention Routing\n\n> Pytorch implementation for Capsules with Inverted Dot-Product Attention Routing.\n\n  \n ## Paper\n [**Capsules with Inverted Dot-Product Attention Routing**](https://openreview.net/pdf?id=HJe6uANtwH)<br>\n [Yao-Hung Hubert Tsai](https://yaohungt.github.io), Nitish Srivastava, Hanlin Goh, and [Ruslan Salakhutdinov](https://www.cs.cmu.edu/~rsalakhu/)<br>\nInternational Conference on Learning Representations (ICLR), 2020. \n\nPlease cite our paper if you find our work useful for your research:\n\n```tex\n@inproceedings{tsai2020Capsules,\n  title={Capsules with Inverted Dot-Product Attention Routing},\n  author={Tsai, Yao-Hung Hubert and Srivastava, Nitish and Goh, Hanlin and Salakhutdinov, Ruslan},\n  booktitle={International Conference on Learning Representations (ICLR)},\n  year={2020},\n}\n```\n\n## Overview\n\n### Overall Architecture\n<p align=\"center\">\n<img src='imgs/overall.png' width=\"1000px\"/>\n\nAn example of our proposed architecture is shown above. The backbone is a standard feed-forward convolutional neural network. The features extracted from this network are fed through another convolutional layer. At each spatial location, groups of 16 channels are made to create capsules (we assume a 16-dimensional pose in a capsule). LayerNorm is then applied across the 16 channels to obtain the primary capsules. This is followed by two convolutional capsule layers, and then by two fully-connected capsule layers. In the last capsule layer, each capsule corresponds to a class. These capsules are then used to compute logits that feed into a softmax to computed the classification probabilities. Inference in this network requires a feed-forward pass up to the primary capsules. After this, our proposed routing mechanism (discussed later) takes over.\n\n### Inverted Dot-Product Attention Routing\n<p align=\"center\">\n<img src='imgs/inverted_attention.png' width=\"1000px\"/>\n\nIn our method, the routing procedure resembles an inverted attention mechanism, where dot products are used to measure agreement. Specifically, the higher-level (parent) units compete for the attention of the lower-level (child) units, instead of the other way around, which is commonly used in attention models. Hence, the routing probability directly depends on the agreement between the parent\u2019s pose (from the previous iteration step) and the child\u2019s vote for the parent\u2019s pose (in the current iteration step). We (1) use Layer Normalization (Ba et al., 2016) as normalization, and we (2) perform inference of the latent capsule states and routing probabilities jointly across multiple capsule layers (instead of doing it layer-wise). \n\n### Concurrent Routing\n<p align=\"center\">\n<img src='imgs/concurrent.png' width=\"1000px\"/>\n\nThe concurrent routing is a parallel-in-time routing procedure for all capsules layers.\n\n## Usage\n\n### Prerequisites\n- Python 3.6/3.7\n- [Pytorch (>=1.2.0) and torchvision](https://pytorch.org/)\n- CUDA 10.0 or above\n\n### Datasets\n\nWe use [CIFAR10 and CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html).\n\n### Run the Code\n\n#### Arguments\n\n|     Args    |                        Value                        | help                                                                                                                                                                            |\n|:-----------:|:---------------------------------------------------:|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|    debug    |                          -                          | Enter into a debug mode, which means no models and results will be saved. True or False                                                                                                       |\n| num_routing |                          1                          | The number of routing iteration. The number should > 1. |\n|   dataset   |                       CIFAR10                       | Choice of the dataset. CIFAR10 or CIFAR100.                                                                                                                                                         |\n|   backbone  |                        resnet                       | Choice of the backbone. simple or resnet.                                                                                                                                                        |\n| config_path | ./configs/resnet_backbone_CIFAR10.json | Configurations for capsule layers.                                                                                                                                              |\n\n#### Running CIFAR-10 \n\n```bash\npython main_capsule.py --num_routing 2 --dataset CIFAR10 --backbone resnet --config_path ./configs/resnet_backbone_CIFAR10.json \n```\nWhen ```num_routing``` is ```1```, the average performance we obtained is _94.73%_.\n\nWhen ```num_routing``` is ```2```, the average performance we obtained is _94.85%_ and the best model we obtained is _95.14%_.\n\n\n#### Running CIFAR-100\n\n```bash\npython main_capsule.py --num_routing 2 --dataset CIFAR100 --backbone resnet --config_path ./configs/resnet_backbone_CIFAR100.json \n```\n\nWhen ```num_routing``` is ```1```, the average performance we obtained is _76.02%_.\n\nWhen ```num_routing``` is ```2```, the average performance we obtained is _76.27%_ and the best model we obtained is _78.02%_.\n\n\n## License\nThis code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/ccs-calendarserver",
  "language": "Python",
  "readme_contents": "==========================\nNotice of Archival\n==========================\n\nThe developers of the Calendar and Contacts Server have each moved on to other projects over the past few years, and given\nthe change in the Python language landscape, in addition to the care and feeding requried to maintain a secure set of\ndependecies, the time has come to officially archive the project.  Thank you to all the users and contributors; it was neat\nto hear about how this code was being deployed around the world, and we hope it remains useful for years to come.\n\n\n\n==========================\nGetting Started\n==========================\n\nThis is the core code base for the Calendar and Contacts Server, which is a CalDAV, CardDAV, WebDAV, and HTTP server.\n\nFor general information about the server, see https://apple.github.io/ccs-calendarserver.\n\n\n==========================\nCopyright and License\n==========================\n\nCopyright (c) 2005-2017 Apple Inc. All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0. The Apache License is a well-established open source license, enabling collaborative open source software development.\n\nSee the \"LICENSE\" file for the full text of the license terms.\n\n==========================\nQuickStart\n==========================\n\n**WARNING**: these instructions are for running a server from the source tree, which is useful for development. These are not the correct steps for running the server in deployment or as part of an OS install. You should not be using the run script in system startup files (eg. /etc/init.d); it does things (like download software) that you don't want to happen in that context.\n\nBegin by creating a directory to contain Calendar and Contacts Server and all its dependencies::\n\n mkdir ~/CalendarServer\n cd CalendarServer\n\nNext, check out the source code from the GIT repository. To check out the latest code::\n\n git clone https://github.com/apple/ccs-calendarserver.git\n\nNote: if you have two-factor authentication activated on GitHub, you'll need to use a personal access token instead of your password.  You can generate personal access tokens at https://github.com/settings/tokens\n\n`Pip <https://pip.pypa.io/en/stable/installing/>`_ is used to retrieve the python dependencies and stage them for use by virtualenv, however if your system does not have pip (and virtualenv), install it by running:\n\n    python -m ensurepip\n\nIf this yields a permission denied error, you are likely using a system-wide installation of Python, so either retry with a user installation of Python or prefix the command with 'sudo'.\n\nThe server requires various external libraries in order to operate. The bin/develop script in the sources will retrieve these dependencies and install them to the .develop directory. Note that this behavior is currently also a side-effect of bin/run, but that is likely to change in the future::\n\n    cd ccs-calendarserver\n    ./bin/develop\n    ____________________________________________________________\n\n    Using system version of libffi.\n\n    ____________________________________________________________\n\n    Using system version of OpenLDAP.\n\n    ____________________________________________________________\n\n    Using system version of SASL.\n\n    ____________________________________________________________\n    ...\n\nBefore you can run the server, you need to set up a configuration file for development. There is a provided test configuration that you can use to start with, conf/caldavd-test.plist, which can be copied to conf/caldavd-dev.plist (the default config file used by the bin/run script). If conf/caldavd-dev.plist is not present when the server starts, you will be prompted to create a new one from conf/caldavd-test.plist.\n\nYou will need to choose a directory service to use to populate your server's principals (users, groups, resources, and locations). A directory service provides the Calendar and Contacts Server with information about these principals. The directory services supported by Calendar and Contacts Server are:\n\n- XMLDirectoryService: this service is configurable via an XML file that contains principal information. The file conf/auth/accounts.xml provides an example principals configuration.\n- OpenDirectoryService: this service uses Apple's OpenDirectory client, the bulk of the configuration for which is handled external to Calendar and Contacts Server (e.g. System Preferences --> Users & Groups --> Login Options --> Network Account Server).\n- LdapDirectoryService: a highly flexible LDAP client that can leverage existing LDAP servers. See `twistedcaldav/stdconfig.py <https://github.com/apple/ccs-calendarserver/blob/master/twistedcaldav/stdconfig.py>`_ for the available LdapDirectoryService options and their defaults. \n\nThe caldavd-test.plist configuration uses XMLDirectoryService by default, set up to use conf/auth/accounts-test.xml. This is a generally useful configuration for development and testing.\n\nThis file contains a user principal, named admin, with password admin, which is set up (in caldavd-test.plist) to have administrative permissions on the server.\n\nStart the server using the bin/run script, and use the -n option to bypass dependency setup::\n\n    bin/run -n \n    Using /Users/andre/CalendarServer/ccs-calendarserver/.develop/roots/py_modules/bin/python as Python\n\n    Missing config file: /Users/andre/CalendarServer/ccs-calendarserver/conf/caldavd-dev.plist\n    You might want to start by copying the test configuration:\n\n      cp conf/caldavd-test.plist conf/caldavd-dev.plist\n\n    Would you like to copy the test configuration now? [y/n]y\n    Copying test cofiguration...\n\n    Starting server...\n\nThe server should then start up and bind to port 8008 for HTTP and 8443 for HTTPS. You should then be able to connect to the server using your web browser (eg. Safari, Firefox) or with a CalDAV client (eg. Calendar).\n"
 },
 {
  "repo": "apple/swift-lldb",
  "language": "C++",
  "readme_contents": "# Disclaimer\n\nThe swift-lldb repository is frozen and is preserved for historical purposes only.\nActive development is now happening in the following repository: https://github.com/apple/llvm-project\n\n# Swift Debugger and REPL\n\n**Welcome to the Swift Debugger and REPL!**\n\nSwift is a new, high performance systems programming language.  It has a clean\nand modern syntax, offers seamless access to existing C and Objective-C\ncode and frameworks, and is memory safe (by default).\n\nThis repository covers the Swift Debugger and REPL support, built on\ntop of the LLDB Debugger.\n\n# Building LLDB for Swift\n\nTo build LLDB for Swift, check out the swift repository and follow\nthe instruction listed there. You can build lldb passing the --lldb\nflag to it. Example invocation:\n\n```\nmkdir myswift\ncd myswift\ngit clone https://github.com/apple/swift.git swift\n./swift/utils/update-checkout\n./swift/utils/build-script -r --lldb\n```\n\n# Contribution Subtleties\n\nThe swift-lldb project enhances the core LLDB project developed under\nthe [LLVM Project][llvm]. Swift support in the debugger is added via\nthe existing source-level plugin infrastructure, isolated to files that\nare newly introduced in the lldb-swift repository.\n\nFiles that come from the [core LLDB project][lldb] can be readily\nidentified by their use of the LLVM comment header.  As no local\nchanges should be made to any of these files, follow the standard\n[guidance for upstream changes][upstream].\n\n[lldb]: http://lldb.llvm.org \"LLDB debugger\"\n[llvm]: http://llvm.org \"The LLVM Project\"\n[upstream]: http://swift.org/contributing/#llvm-and-swift \"Upstream LLVM changes\"\n"
 },
 {
  "repo": "apple/ml-cifar-10-faster",
  "language": "Python",
  "readme_contents": "# Training cifar-10 in under 11 seconds\n\nWe use 8 V100 Nvidia GPUs to train cifar-10 in under 11 seconds using mini-batches of 2048 elements,\nwith 256 elements per GPU.\nOur code modifies David C. Pages's \n[bag of tricks](https://github.com/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb) \nimplementation to take advantage of Nvidia's NCCL through pytorch's distributed training framework.\n\nIn our modifications we remove the exponential moving average model and extend the code to use multiple GPUs, \nbut otherwise maintain the rest of the original strategies. \n\nWe distribute the computation using data parallelism, that is: we maintain a copy of the full model in each GPU and process a subset of \n the mini-batch in each. After every backward pass we average the 8 resulting gradient estimates to produce the \n final gradient estimate. \n At the start of training we partition the dataset into 8 subsets and assign one subset to each worker. We maintain \n the same partition for all epochs. \n\n## Faster training with larger batches\n\nWe execute 50 runs and achieve an accuracy of 94% (or more) in 41 of them, with a maximum training time of \n10.31 (seconds) mean training time of 7.81 (seconds), minimum accuracy of 93.74% and median accuracy of 94.19%.\nA log of the output is available in [run_log.txt](run_log.txt)\n\n### Reproducing our results \n#### Hardware\nThese results were generated on a platform with:\n- 8 NVIDIA Tesla V100-SMX2 GPUs witn 32 GB of memory\n- An Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz with 20 cores\n\n#### Dependencies\nWe use python 3.6 the remaining dependencies are listed in requirements.txt\n\n#### To Run\n- Navigate to large_batch_cifar_10_fast \n- execute `bash ./train_cifar_parallel.sh` the log will be printed onto STDOUT and the file timing_log.tsv will contain \nthe per-epoch accuracies and run-times of the first replica as measured by worker 0\n\n## DAWNBench \n Note that DAWNBench timings do not include validation time, as in \n [this FAQ](https://github.com/stanford-futuredata/dawn-bench-entries), \n but do include initial preprocessing.\n \n## License\nThis sample code is released under the [LICENSE](LICENSE) terms.\n\n"
 },
 {
  "repo": "apple/ml-data-parameters",
  "language": "Python",
  "readme_contents": "# Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum\nThis repository accompanies the research paper, \n[Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum](\nhttps://papers.nips.cc/paper/9289-data-parameters-a-new-family-of-parameters-for-learning-a-differentiable-curriculum)\n(accepted at NeurIPS 2019). The online copy of the poster is available \n[here](./media/data_parametres_neurips19_poster.pdf).\n\n## Citation\nIf you find this code useful in your research then please cite:\n```\n@article{saxena2019data,\n  title={Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum},\n  author={Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},\n  booktitle={NeurIPS},\n  year={2019}\n}\n```\n## Data Parameters\nIn the paper cited above, we have introduced a new family of parameters termed \"data parameters\".\nSpecifically, we equip each class and training data point with a learnable parameter (data parameters), which governs \ntheir importance during different stages of training. Along with the model parameters, the data parameters are also \nlearnt with gradient descent, thereby yielding a curriculum which evolves during the course of training.\nMore importantly, post training, during inference, data parameters are not used, and hence do not alter the model's \ncomplexity or run-time at inference. \n\n![Training Overview](media/method_overview.png)\n\n\n## Setup and Requirements\nThis code was developed and tested on Nvidia V100 in the following environment.\n\n- Ubuntu 18.04\n- Python 3.6.9\n- Torch 1.2.0\n- Torchvision 0.4\n\n## Getting Started\nApart from the system requirements, you would also need to download ImageNet dataset locally.\nThis path needs to be provided to main_imagenet.py with --data argument. \n\n\n### Training model with data parameters\nWith very little modification, an existing DNN training pipeline can be modified to use data parameters.\n- The first modification is change in data loader. In contrast to standard data loaders which return (x_i, y_i) as \na tuple. We need to return the index of the sample. This is a one line change in \\__getitem\\__ function (see \n[cifar_dataset.py](dataset/cifar_dataset.py) or [imagenet_dataset.py](dataset/imagenet_dataset.py)). Note, this \nchange is required to implement instance level curriculum. Class level curriculum can be implemented without this \nmodification.\n\n- This brings us to the second change (crucial), change in optimizer. Standard optimizers in a deep learning framework like \nPyTorch are built for model parameters. They assume that at each iteration, all parameters are involved in the \ncomputational graph and receive a gradient. Therefore, at each iteration, all parameters undergo a weight decay \npenalty, along with an update to their corresponding momentum buffer. These assumptions and updates are valid for model\nparameters, but not for data parameters. At each iteration, only a subset of data parameters are part of the \ncomputational graph (corresponding to classes and instances in the minibatch). Using standard optimizers \nfrom PyTorch for data parameters will apply a weight decay penalty on all data parameters at each iteration, and \nwill therefore nullify the learnt curriculum. To circumvent this issue, we apply weight decay explicitly on the \nsubset of data parameters participating in the minibatch. Also, we have implemented a SparseSGD optimizer which performs \na sparse update of momentum buffer, updating buffer only for data parameters present in the computational graph.\nMore information can be found in file [sparse_sgd.py](optimizer/sparse_sgd.py).\n\n- Apart from these changes, the only change required is instantiation of data parameters and rescaling of logits with data\nparameters in the forward pass. Since data parameters interact with model at the last layer, in practice, there is \nnegligible overhead in training time. \n\n- The three things which can be tuned for data parameters is their: initialization, learning rate, and weight decay. \nIn practice, we have set initialization of data parameters to 1.0 (initializes training to use standard softmax loss).\nThis leaves us with two hyper-parameters whose value can be set by grid-search. In our experiments, we found data \nparameters to be robust to variations in these hyper-parameters. \n\nBelow we provide example commands along with the hyper-parameters to reproduce results on ImageNet and\nCIFAR100 noisy dataset from the paper. \n\n### ImageNet \n#### Baseline \n```\npython main_imagenet.py \\\n  --arch 'resnet18' \\ \n  --gpu 0 \\\n  --data 'path/to/imagenet' \\\n```\nThis command will train ResNet18 architecture on ImageNet dataset without data parameters. \nThis experiment can be used to obtain baseline performance without data parameters.\nRunning this script, you should obtain 70.2% accuracy on validation @ 100 epoch.\n\n\n#### Train with class level parameters \nTo train ResNet18 with class-level parameters you can use this command:\n```\npython main_imagenet.py \\\n  --arch 'resnet18' \\ \n  --data 'path/to/imagenet' \\\n  --init_class_param 1.0 \\\n  --lr_class_param 0.1 \\\n  --wd_class_param 1e-4 \\\n  --learn_class_paramters \\\n```\nNote, the learning rate, weight decay and initial value of class parameters can be specified \nusing --lr_class_param, --wd_class_param and --init_class_param respectively. \nRunning this script with the hyper-parameters specified above, you should obtain 70.5% accuracy on \nvalidation @ 100 epoch. You can run this script with different values of lr_class_param and wd_class_param \nto obtain more intuition about data parameters. \n\nTo facilitate introspection, the training script dumps the histogram, mean, highest and the lowest value of \ndata parameters in the tensorboard for visualization. For example, in the figure below, we can visualize the histogram\nof class-level parameters (x-axis) over the course of training (y-axis). The parameters of each class vary in the start \nof training, but towards the end of training, they all converge to similar value (indicating all classes were given \nclose to equal importance at convergence).\n![Histogram of class level parameters](./media/histogram_class_temperature_over_iterations.png)\n\n#### Joint training with class and instance level parameters\nAs mentioned in the paper, it is possible to train with class and instance level parameters in a joint manner. \nTo train ResNet18 with both parameters you can use this command:\n```\npython main_imagenet.py \\\n  --arch 'resnet18' \\ \n  --data 'path/to/imagenet' \\\n  --init_class_param 1.0 \\\n  --lr_class_param 0.1 \\\n  --wd_class_param 1e-4 \\\n  --init_inst_param 0.001 \\\n  --lr_inst_param 0.8 \\\n  --wd_inst_param 1e-8 \\\n  --learn_class_paramters \\\n  --learn_inst_parameters \n```\nFor joint training setup, class and instance level parameters are initialized to 1.0 and 0.001. This is to ensure that \ntheir initial sum is close to 1.0. We did not experiment with other initialization schemes, but data parameters can\nbe initialized with any arbitrary value (greater than 0). Running this script with the hyper-parameters specified \nabove, you should obtain 70.8% accuracy on validation @ 100 epoch.\n\n\n#### CIFAR100 Noisy Data\n[cifar_dataset.py](dataset/cifar_dataset.py) extends the standard CIFAR100 dataset from torchvision to allow corruption\nof a subset of data with uniform label swap.\n\n#### Baseline \n```\npython main_cifar.py \\\n  --rand_fraction 0.4 \\\n```\nThis command trains WideResNet28_10 architecture on CIFAR100 dataset (corruption rate=40%) without data parameters. \nThis experiment can be used to obtain baseline performance without data parameters.\nRunning this script, you should obtain 50.0% accuracy at convergence (see Table 2 in paper).\n\n#### Train with instance level parameters\nSince the noise present in the dataset is at instance level, we can train the DNN model with instance level parameters\nto learn instance specific curriculum. The curriculum should learn to ignore learning from corrupt samples in the \ndataset.\n```\npython main_cifar.py \\\n  --rand_fraction 0.4 \\\n  --init_inst_param 1.0 \\\n  --lr_inst_param 0.2 \\\n  --wd_inst_param 0.0 \\\n  --learn_inst_parameters \n```\nRunning this script, using instance level parameters, you should obtain 71% accuracy @ 84th epoch. \nFor results on noisy datasets, we always perform early stopping at 84th epoch (set by cross-validation).\nRunning with the same hyper-parameters for instance parameters, for 20% and 80% corruption rate, you should obtain 75%\n and 35% accuracy respectively. \n\n\n\n## License\nThis code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/ml-afv",
  "language": "Python",
  "readme_contents": "# Adversarial Fisher Vectors for Unsupervised Representation Learning \n\nThis software project accompanies the research paper, [Adversarial Fisher Vectors for Unsupervised Representation Learning](https://arxiv.org/abs/1910.13101).\n\nWe include sample code that can be used to train a GAN/EBM optionally with the MCMC inspired objective, and compute the Adversarial Fisher Vectors for linear classification.\n## Citation\n```\n@article{zhai2019adversarial,\n  title={Adversarial Fisher Vectors for Unsupervised Representation Learning},\n  author={Zhai, Shuangfei and Talbott, Walter and Guestrin, Carlos and Susskind, Joshua M},\n  booktitle={Advances in neural information processing systems},\n  year={2019}\n}\n```\n## Adversarial Fisher Vectors \nAdversarial Fisher Vectors (AVFs) provide a way of utilizing a trained GAN by extracting representations from it. AFVs achieve this by adopting an EBM view of a common GAN implementation, and represent an example with the derived Fisher Score, nomalized by the Fisher Information. In this repo, we demonstrate the use of AFVs by providing sample code for traing a GAN and linear classification on CIFAR10 with the induced representation. We also provide pretrained weights of a GAN on the combined CIFAR10 and CIFAR100 dataset, which yields state-of-the-art linear classification results on the two datasets.\n\n## Setup\nThis code is written in Pytorch with Python 2.7. It's tested on Ubuntu 16.04 and CUDA 8.0 (but later versions should work too), and requires one GPU card. Run the following command to install all the dependencies:\n```\npip install -r requirements.txt\n```\n\n## Getting Started \n### GAN Training (optional)\nIn order to compute the AFVs, the first step is to train a GAN (and interpret it as an EBM) on CIFAR10. A model with default setting can be trained by running:\n```\npython main.py\n```\nOne can also skip this step by using the pretrained model found under the checkpoints directory.\n### Linear Classifier\nAfter a model is trained, we can load the a checkpoint (e.g., the one from the last iteration) and use it for training a linear classifier. This can be done by running:\n```\npython classifiy_cifar10.py --netG [path-to-generator-ckpt] --netD [path-to-discriminator-ckpt]\n```\nIf using the pretrained model, this corresponds to:\n```\npython classifiy_cifar10.py --netG checkpoints/netG_pretrained.pth --netD checkpoints/netD_pretrained.pth\n```\nThis will train a linear SVM classifier with dropout on the induced AFVs. Using the pretrained weights, this should give you a test accuracy of ~0.89. Note that this step is pretty time consuming, as the AFVs are of very high dimensionality and need to be generated online. \n\n## License\nThis sample code is released under the [LICENSE](LICENSE) terms.\n"
 },
 {
  "repo": "apple/swift-llvm",
  "language": "LLVM",
  "readme_contents": "# Disclaimer\n\nThe swift-llvm repository is frozen and is preserved for historical purposes only.\nActive development is now happening in the following repository: https://github.com/apple/llvm-project.\n\nThe LLVM Compiler Infrastructure\n================================\n\nThis directory and its subdirectories contain source code for LLVM,\na toolkit for the construction of highly optimized compilers,\noptimizers, and runtime environments.\n\nLLVM is open source software. You may freely distribute it under the terms of\nthe license agreement found in LICENSE.txt.\n\nPlease see the documentation provided in docs/ for further\nassistance with LLVM, and in particular docs/GettingStarted.rst for getting\nstarted with LLVM and docs/README.txt for an overview of LLVM's\ndocumentation setup.\n\nIf you are writing a package for LLVM, see docs/Packaging.rst for our\nsuggestions.\n"
 },
 {
  "repo": "apple/swift-clang",
  "language": "C++",
  "readme_contents": "# Disclaimer\n\nThe swift-clang repository is frozen and is preserved for historical purposes only. Active development is now happening in the following repository: https://github.com/apple/llvm-project.\n\n//===----------------------------------------------------------------------===//\n// C Language Family Front-end\n//===----------------------------------------------------------------------===//\n\nWelcome to Clang.  This is a compiler front-end for the C family of languages\n(C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM\ncompiler infrastructure project.\n\nUnlike many other compiler frontends, Clang is useful for a number of things\nbeyond just compiling code: we intend for Clang to be host to a number of\ndifferent source-level tools.  One example of this is the Clang Static Analyzer.\n\nIf you're interested in more (including how to build Clang) it is best to read\nthe relevant web sites.  Here are some pointers:\n\nInformation on Clang:             http://clang.llvm.org/\nBuilding and using Clang:         http://clang.llvm.org/get_started.html\nClang Static Analyzer:            http://clang-analyzer.llvm.org/\nInformation on the LLVM project:  http://llvm.org/\n\nIf you have questions or comments about Clang, a great place to discuss them is\non the Clang development mailing list:\n  http://lists.llvm.org/mailman/listinfo/cfe-dev\n\nIf you find a bug in Clang, please file it in the LLVM bug tracker:\n  http://llvm.org/bugs/\n"
 },
 {
  "repo": "apple/swift-clang-tools-extra",
  "language": "C++",
  "readme_contents": "# Disclaimer\n\nThe swift-clang-tools-extra repository is frozen and is preserved for historical purposes only.\nActive development is now happening in the following repository: https://github.com/apple/llvm-project\n\n//===----------------------------------------------------------------------===//\n// Clang Tools repository\n//===----------------------------------------------------------------------===//\n\nWelcome to the repository of extra Clang Tools.  This repository holds tools\nthat are developed as part of the LLVM compiler infrastructure project and the\nClang frontend.  These tools are kept in a separate \"extra\" repository to\nallow lighter weight checkouts of the core Clang codebase.\n\nThis repository is only intended to be checked out inside of a full LLVM+Clang\ntree, and in the 'tools/extra' subdirectory of the Clang checkout.\n\nAll discussion regarding Clang, Clang-based tools, and code in this repository\nshould be held using the standard Clang mailing lists:\n  http://lists.llvm.org/mailman/listinfo/cfe-dev\n\nCode review for this tree should take place on the standard Clang patch and\ncommit lists:\n  http://lists.llvm.org/mailman/listinfo/cfe-commits\n\nIf you find a bug in these tools, please file it in the LLVM bug tracker:\n  http://llvm.org/bugs/\n"
 },
 {
  "repo": "apple/swift-libcxx",
  "language": "C++",
  "readme_contents": ""
 },
 {
  "repo": "apple/swift-compiler-rt",
  "language": "C",
  "readme_contents": "# Disclaimer\n\nThe swift-compiler-rt repository is frozen and is preserved for historical purposes only.\nActive development is now happening in the following repository: https://github.com/apple/llvm-project\n \nCompiler-RT\n================================\n\nThis directory and its subdirectories contain source code for the compiler\nsupport routines.\n\nCompiler-RT is open source software. You may freely distribute it under the\nterms of the license agreement found in LICENSE.txt.\n\n================================\n\n"
 },
 {
  "repo": "apple/llvm-project-v5-split",
  "language": "C++",
  "readme_contents": "//===----------------------------------------------------------------------===//\n// Clang Tools repository\n//===----------------------------------------------------------------------===//\n\nWelcome to the repository of extra Clang Tools.  This repository holds tools\nthat are developed as part of the LLVM compiler infrastructure project and the\nClang frontend.  These tools are kept in a separate \"extra\" repository to\nallow lighter weight checkouts of the core Clang codebase.\n\nThis repository is only intended to be checked out inside of a full LLVM+Clang\ntree, and in the 'tools/extra' subdirectory of the Clang checkout.\n\nAll discussion regarding Clang, Clang-based tools, and code in this repository\nshould be held using the standard Clang mailing lists:\n  http://lists.llvm.org/mailman/listinfo/cfe-dev\n\nCode review for this tree should take place on the standard Clang patch and\ncommit lists:\n  http://lists.llvm.org/mailman/listinfo/cfe-commits\n\nIf you find a bug in these tools, please file it in the LLVM bug tracker:\n  http://llvm.org/bugs/\n"
 },
 {
  "repo": "apple/llvm-project-v5",
  "language": "C++",
  "readme_contents": "# Disclaimer\n\nThe [llvm-monorepo-root](https://github.com/apple/llvm-monorepo-root),\n[llvm-project-v4](https://github.com/apple/llvm-project-v4),\n[llvm-project-v4-split](https://github.com/apple/llvm-project-v4-split),\n[llvm-project-v3](https://github.com/apple/llvm-project-v3),\n[llvm-project-v3-split](https://github.com/apple/llvm-project-v3-split),\n[llvm-project-v2](https://github.com/apple/llvm-project-v2), and\n[llvm-project-v2-split](https://github.com/apple/llvm-project-v2-split)\nrepositories are\nWIP repositories that are used for development and prototyping of an llvm-project monorepo\nthat will be used for Apple's compiler releases and for the open source Swift project.\nPlease see [this forum post](https://forums.swift.org/t/llvm-monorepo-transition-update/27079)\nfor more details.\n\nOriginal readme follows:\n\n# The LLVM Compiler Infrastructure\n\nThis directory and its subdirectories contain source code for LLVM,\na toolkit for the construction of highly optimized compilers,\noptimizers, and runtime environments.\n"
 },
 {
  "repo": "apple/llvm-monorepo-root",
  "language": null,
  "readme_contents": "# Disclaimer\n\nThe [llvm-monorepo-root](https://github.com/apple/llvm-monorepo-root),\n[llvm-project-v4](https://github.com/apple/llvm-project-v4),\n[llvm-project-v4-split](https://github.com/apple/llvm-project-v4-split),\n[llvm-project-v3](https://github.com/apple/llvm-project-v3),\n[llvm-project-v3-split](https://github.com/apple/llvm-project-v3-split),\n[llvm-project-v2](https://github.com/apple/llvm-project-v2), and\n[llvm-project-v2-split](https://github.com/apple/llvm-project-v2-split)\nrepositories are\nWIP repositories that are used for development and prototyping of an llvm-project monorepo\nthat will be used for Apple's compiler releases and for the open source Swift project.\nPlease see [this forum post](https://forums.swift.org/t/llvm-monorepo-transition-update/27079)\nfor more details.\n\nOriginal readme follows:\n\n# The LLVM Compiler Infrastructure\n\nThis directory and its subdirectories contain source code for LLVM,\na toolkit for the construction of highly optimized compilers,\noptimizers, and runtime environments.\n"
 },
 {
  "repo": "apple/ml-ncg",
  "language": "Python",
  "readme_contents": "# NCG PROJECT \nPublication Link : *[Nonlinear Conjugate Gradients For Scaling Synchronous Distributed DNN Training](https://arxiv.org/abs/1812.02886)*\n\nThis NCG repository consist of two components \n1) the **optimization** package\n2) the **examples** scripts\n\nUsage details both components are described in [ncg/README.md](ncg/README.md).\n\n## Getting Started\nFor sanity, work from with in a Python virtual environment of your choice.\n\n* Building the *optimization package* wheel\n```\n    # work from with in ncg directory\n    python setup.py bdist_wheel -d < dist_wheel target directory>\n```\n\n* Installing the wheel in your virtual environment\n```\n    pip install < dist_wheel target directory>/ncg-xyz.whl\n```\n\n* Alternatively, to install ncg package in develop mode\n```\n    pip install -e optimization\n    \n```\n\n\n\n\n"
 },
 {
  "repo": "apple/ccs-twistedextensions",
  "language": "Python",
  "readme_contents": "Introduction\n============\n\nThis is a python module consisting of extensions to the Twisted Framework\n(http://twistedmatrix.com/).\n\n\nCopyright and License\n=====================\n\nCopyright (c) 2005-2017 Apple Inc.  All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0.\n\nSee the file LICENSE_ for the full text of the license terms.\n\n.. _LICENSE: LICENSE.txt\n\n\nInstallation\n============\n\nPython version 2.7 is supported.\n\nThis library is can be built using the standard distutils mechanisms.\n\nIt is also registered with the Python Package Index (PyPI) as ``twextpy`` \n(the name ``twext`` is used by another module in PyPI) for use with ``pip`` and\n``easy_install``::\n\n  pip install twextpy\n\nThis will build and install the ``twext`` module along with its base\ndependencies.  This library has a number of optional features which must be\nspecified in order to download build and install their dependencies, for\nexample::\n\n  pip install twextpy[DAL,Postgres]\n\nThese features are:\n\nDAL\n  Enables use of the Database Abstraction Layer implemented in\n  ``twext.enterprise.dal``.\n\nLDAP\n  Enables support for the Lightweight Directory Access Protocol in\n  ``twext.who.ldap``.\n\nOpenDirectory\n  Enables support for the (Mac OS) OpenDirectory framework in\n  ``twext.who.opendirectory``.\n\nOracle\n  Enables support for Oracle database connectivity in ``twext.enterprise`` and\n  Oracle syntax in ``twext.enterprise.dal``.\n\nPostgres\n  Enables support for Postgres database connectivity in ``twext.enterprise``.\n\n\nDevelopment\n===========\n\nIf you are planning to work on this library, you can manage this with standard\ndistutils mechanisms.  There are, however, some tools in the ``bin`` directory\nwhich automate this management for you.\n\nTo use these tools, you must have ``pip`` on your system.\nIf you do not have ``pip``, instructions for installing it are at\nhttp://www.pip-installer.org/en/latest/installing.html.\nThe script ``install_pip`` (requires ``sudo`` access) automates this for you.\n\nThe shell script ``develop`` downloads and builds any dependancies and sets up a\ndevelopment environment.  ``develop`` handles non-Python as well as Python\ndependancies.\n\nThe tools ``python``, ``pyflakes``, ``trial``, and ``twistd`` are wrappers\naround the cooresponding commands that use ``develop`` to ensure that\ndependancies are available, ``PYTHONPATH`` is set up correctly, etc.\n\n``test`` runs all of the unit tests and does linting.  It should be run before\nchecking in any code.\n"
 },
 {
  "repo": "apple/ml-all-pairs",
  "language": "Python",
  "readme_contents": "<img alt=\"All-Pairs example\" src=\"examples/all_pairs_survey.png?raw=true\" width=\"400\">\n\n# all-pairs\n\nA data generator for studying learning with weak supervision.  The two primary components included in this repo are as follows:\n\n1. The data generator for the All-Pairs problem.  The purpose of this dataset is to explore the limits of learning with weak supervision.  Samples can be generated on the fly during training, an example is provided.\n1. The Type-Net model which can be more easily trained to solve the All-Pairs problem than conventional vision models.\n\nSee the accompanying paper on arXiv for details: [A New Benchmark and Progress Toward Improved Weakly Supervised Learning](https://arxiv.org/abs/1807.00126)\n\nCode is parameterized with argparse, so see individual files for details of configurations.  \n\nUse `python SCRIPT_NAME.py --help` to see help on each parameter.\n\n# Documentation\n### Data generator\n\nThe main generator is provided in `allpairs/grid_generator.py`. \nWe have included a pytorch dataset and dataloader in `code_pytorch/grid_loader.py`\n\n### Typenet\n\nA simple example of typenet is provided in `code_pytorch/typenet.py`\n\n# Getting Started\n\n### Requirements\n\nSee getting started below.  To get started you should have the following:\n\n- git\n- pip\n- virtualenv (pip install virtualenv)\n- Python\n\n### macOS\n\nAfter cloning or downloading this git repo, install the requirements:\n\n```\ncd ml-all-pairs\nvirtualenv env           (in python3: python3 -m venv env)\nsource env/bin/activate\npip install -r requirements.txt\n```\n\nTest the rendering.\n\n```\npython examples/make_survey_strip.py\n```\n\nValidate that the images produced are exactly as expected:\n\n```\npython test.py\n```\n\nGenerate samples for analysis; writes the png image files to the \"dest\" directory and save the ground-truth to a csv file:\n\n```\nmkdir samples\npython generate.py --pixels 72 --num-pairs 4 --num-classes 4 --num 1000 --dest samples --csv groundtruth.csv\n```\n\n### To Train Type-Net\n\n1. Install pytorch.\n\n1. Train the Type-Net model to solve the 4-4 All-Pairs problem.  \n```\npython train-pytorch-simple.py\n```\n1. To have more control over the parameters, you can use the following:\n```\npython train-pytorch.py --num-classes=4 --num-pairs=4\n```\n\n### Example Results\n\nTo see the results of training the 4-4 All-Pairs problem, run the commands below:\n\n```\npython train-pytorch-simple.py | tee examples/results.txt\ncd examples\npython plot_results.py\n```\nWe plot the maximum validation accuracy because the batch norm moving statistics (used in validation) are often wrong as the weights change.\n\n<img alt=\"All-Pairs example\" src=\"examples/training-4-4.png?raw=true\" width=\"400\">\n\n"
 },
 {
  "repo": "apple/swift-3-api-guidelines-review",
  "language": "Swift",
  "readme_contents": "# Swift 3 API Guidelines Review\n\nThis repository is part of the [Swift 3 API Design Guidelines effort](https://github.com/apple/swift-evolution/blob/master/README.md), which helps evaluate the effects of applying the [Swift API Design Guidelines](https://swift.org/documentation/api-design-guidelines/) to Objective-C APIs through [improvements to Swift's Clang importer](https://github.com/apple/swift-evolution/blob/master/proposals/0005-objective-c-name-translation.md).\n\nThis repository contains the Swift projections of Objective-C APIs for\nCocoa and Cocoa Touch across the four Apple platforms (iOS, OS X,\nwatchOS, tvOS) as well as sample projects that use those APIs. It is\nintended to allow anyone to explore the effects of Swift 3's changes\nto the Clang importer, both to the APIs themselves (are they closer to\nmatching the Swift API Design Guidelines?) and to sample projects that\nuse those APIs (is the code clearer and more \"Swifty\"?). This\nrepository contains two major branches:\n\n* `swift-2`: This branch provides the baseline Swift 2 projections of Objective-C APIs along with sample projects that compile in Swift 2. This branch will evolve only when the source inputs change, e.g., the addition of new sample code or updated SDKs when a new version of Xcode becomes available.\n\n* `swift-3`: This branch provides the Swift 3 versions of the APIs and code in Swift 2, branched from `swift-2`. The `swift-3` branch will evolve along with the implementation of the aforementioned improvements to Swift's Clang importer, and rebase from `swift-2` whenever `swift-2` changes.\n\nAside from documentation and scripts, all of the content in this\nrepository is drawn from other sources, either auto-generated from\nthose scripts and the Swift tools or imported from external sources.\n\nXcode version used to generate the current data: 7.2\n"
 },
 {
  "repo": "apple/ccs-pyopendirectory",
  "language": "C++",
  "readme_contents": ""
 },
 {
  "repo": "apple/swift-protobuf-plugin",
  "language": null,
  "readme_contents": "<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n\n# About Swift Protobuf\n\nThe Swift Protobuf project has been combined into a single Github repository at\n\nhttps://github.com/apple/swift-protobuf/\n\nPlease use that repository going forward.\n\nThank you for your patience while we continue to improve this project!\n"
 },
 {
  "repo": "apple/ccs-caldavclientlibrary",
  "language": "HTML",
  "readme_contents": "README for CalDAVClientLibrary\n\nINTRODUCTION\n\nCalDAVCLientLibrary is a Python library and tool for CalDAV. It is\ncomprised of five main modules in the top-level caldavclientlibrary\npackage:\n\nprotocol: this implements an HTTP/WebDAV/CalDAV protocol stack, using\nhttplib to communicate with the server.\n\nclient: this implements a CalDAV client session, with higher level\nfunctionality than the protocol module (e.g. 'get properties on resource\nX and return as a dict'). There is a higher level abstraction using an\nobject model to repesent a session, accounts, principals and calendars\nas objects.\n\nbrowser: this implements a shell-like browser that lets you interact\nwith the CalDAV server directly via protocol. You can 'cd' to different\nparts of the repository, 'ls' to list a collection, 'cat' to read\nresource data, 'props' to get properties. Then there are some higher\nlevel functions such as 'proxies' which let you manage (read and edit)\nthe proxy list for a principal, and 'acl' which lets you manage ACLs\ndirectly on resources. For those, the tool takes care of mapping from\nprincipal paths to principal URLs etc. Help is provided for each command\n(type '?'). It is easily extensible by adding new commands.\n\nui: a PyObjC application with a WebDAV browser GUI. This provides a file\nsystem like browser that allows properties and the data for a selected\nWebDAV resource do be displayed.\n\nadmin: a user account administration tool. Currently works only with the\nXML file directory account.\n\n*** NB This package requires Python 2.5. ***\n\nThe runshell.py script will launch the command line browser shell.\nThe runadmin.py script will run the XML directory admin tool.\n\n\nSHELL TOOL\n\n-- COMMAND LINE OPTIONS\n\n    Usage: runshell [OPTIONS]\n    \n    Options:\n    \n    -l              start with HTTP logging on.\n    \n    --server=HOST   url of the server include http/https scheme and\n                    port [REQUIRED].\n                    \n    --user=USER     user name to login as - will be prompted if not\n                    present [OPTIONAL].\n                    \n    --pswd=PSWD     password for user - will be prompted if not\n                    present [OPTIONAL].\n\n-- QUICKSTART - COMMANDLINE\n\nTo browse a calendar server on the local machine:\n\n    ./runshell.py --server http://localhost:8008\n    \nor, for SSL:\n\n    ./runshell.py --server https://localhost:8443\n\nThen type '?' followed by return to see the list of available commands.\n\n\nUI TOOL\n\n-- QUICKSTART - GUI\n\nBuild the GUI app using:\n\n    python setup.py py2app\n\nThe application will be placed in the 'dist' directory. Double-click\nthat to launch it. One it it running, click the 'Server' toolbar button\nand specify a server, user id and password.\n\nThe app will then display the top-level of the server resource hierarchy\nin the browser pane on the left. You can click and navigate through the\nresources via that pane (the 'Browser' toolbar buttons determine whether\nthe browser uses a column or list view).\n\nWhen a resource is selected in the browser pane, its properties or data\nare display in the right hand pane. You can toggle between viewing\nproperties or data by using the 'View' toolbar buttons.\n\n\nADMIN TOOL\n\n-- QUICKSTART - COMMANDLINE\n\nTo run the tool and see the list of available commands:\n\n    ./runadmin.py --help\n    \n\nTO DO\n\nLots of error handling and documentation.\n"
 },
 {
  "repo": "apple/ccs-pyosxframeworks",
  "language": "Python",
  "readme_contents": "Getting Started\n===============\n\nThis is a python library that wraps a number of useful OS X frameworks.\n\nDevelop\n=======\ncffi wrappers for various frameworks live in the osx.frameworks modules. Run\nthe osx._corefoundation_cffi_build module to create the osx._corefoundation.so\nmodule. That contains the cffi extension module for the wrappers. Each time\nchanges are made to the wrapper modules, re-run the cffi build.\n\nThe other modules in osx are Python objects that wrap various CoreFoundation\nobjects to provide implicit CFRetain/CFRelease behavior and also simple\ndata conversion APIs between the CF types and Python types. For example to\nturn a Python string \"s\" into a CFStringRef:\n\n\tcfstr = CFStringRef.fromString(s)\n\t\nThe base Python CFObjectRef class assumes ownership of the underlying CF object\nand will always release it. If the CF object being passed in has not already been\nretained, then set the \"owned\" argument to False.\n\nInstall\n=======\n\npython setup.py build\n\n\nCopyright and License\n=====================\n\nCopyright (c) 2015-2017 Apple Inc.  All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0.  The\nApache License is a well-established open source license, enabling\ncollaborative open source software development.\n\nSee the \"LICENSE\" file for the full text of the license terms.\n"
 },
 {
  "repo": "apple/ccs-pysecuretransport",
  "language": "Python",
  "readme_contents": "Getting Started\n===============\n\nOS X SecureTransport cffi based API that looks like pyOpenSSL.\n\nThe goal here is to provide the minimum API needed to support\nTLS in Twisted. All certificate handling and verification is\nmanaged via OS X (via the Keychain and its trust related settings).\n\n\nCopyright and License\n=====================\n\nCopyright (c) 2015-2017 Apple Inc.  All rights reserved.\n\nThis software is licensed under the Apache License, Version 2.0.  The\nApache License is a well-established open source license, enabling\ncollaborative open source software development.\n\nSee the \"LICENSE\" file for the full text of the license terms.\n"
 },
 {
  "repo": "apple/swift-protobuf-test-conformance",
  "language": null,
  "readme_contents": "<img src=\"https://swift.org/assets/images/swift.svg\" alt=\"Swift logo\" height=\"70\" >\n# Swift Protobuf Conformance Tester\n\n\nThe Swift Protobuf project has been combined into a single Github repository at\n\nhttps://github.com/apple/swift-protobuf/\n\nPlease use that repository going forward.\n\nThank you for your patience while we continue to improve this project!\n"
 },
 {
  "repo": "apple/darwin-libpthread",
  "language": "C",
  "readme_contents": ""
 }
]